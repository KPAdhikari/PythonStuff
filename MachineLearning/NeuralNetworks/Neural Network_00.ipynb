{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='GoTop'></a>\n",
    "## Contents\n",
    "* [First neural network for beginners explained (with code)](#neuralNetworkFromScratch1): Trying the basic neural network example (Perceptron) from a site.\n",
    "* [How to build a simple Neural Network from scratch with Python](#neuralNetworkFromScratch2): Trying something similar as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kpadhikari/Desktop/BigFls/CLAS12/GitProj/KPAdhikari/PythonStuff/MachineLearning/NeuralNetworks\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other References:\n",
    "* https://www.kdnuggets.com/2018/10/simple-neural-network-python.html\n",
    "* https://towardsdatascience.com/how-to-build-a-simple-neural-network-from-scratch-with-python-9f011896d2f3 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GoTop](#GoTop) <a id='neuralNetworkFromScratch2'></a>\n",
    "## First neural network for beginners explained (with code)\n",
    "Code and ideas copied from https://towardsdatascience.com/first-neural-network-for-beginners-explained-with-code-4cfd37e06eaf \n",
    "\n",
    "### Understand and create a Perceptron\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy, random, os\n",
    "lr = 1 #learning rate\n",
    "bias = 1 #value of bias\n",
    "weights = [random.random(),random.random(),random.random()] #weights generated in a list (3 weights in total for 2 neurons and the bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The beginning of the program just defines libraries and the values of the parameters, and creates a list which contains the values of the weights that will be modified (those are generated randomly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Perceptron(input1, input2, output) :\n",
    "  outputP = input1*weights[0]+input2*weights[1]+bias*weights[2]\n",
    "  if outputP > 0 : #activation function (here Heaviside)\n",
    "     outputP = 1\n",
    "  else :\n",
    "     outputP = 0\n",
    "  error = output - outputP\n",
    "  weights[0] += error * input1 * lr\n",
    "  weights[1] += error * input2 * lr\n",
    "  weights[2] += error * bias * lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a function which defines the work of the output neuron. It takes 3 parameters (the 2 values of the neurons and the expected output). “outputP” is the variable corresponding to the output given by the Perceptron. Then we calculate the error, used to modify the weights of every connections to the output neuron right after."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kp: Now the Training or Learning\n",
    "We create a loop that makes the neural network repeat every situation several times. This part is the learning phase. The number of iteration is chosen according to the precision we want. However, be aware that too much iterations could lead the network to over-fitting, which causes it to focus too much on the treated examples, so it couldn’t get a right output on case it didn’t see during its learning phase.\n",
    "\n",
    "However, our case here is a bit special, since there are only 4 possibilities, and we give the neural network all of them during its learning phase. A Perceptron is supposed to give a correct output without having ever seen the case it is treating.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(50) :\n",
    "  Perceptron(1,1,1) #True or true\n",
    "  Perceptron(1,0,1) #True or false\n",
    "  Perceptron(0,1,1) #False or true\n",
    "  Perceptron(0,0,0) #False or false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kp: The training/learning has determined the appropriate values of the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0676148383930464, 1.1732051684671074, -0.8658131029393434]\n"
     ]
    }
   ],
   "source": [
    "print(weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kp: Now testing it\n",
    "Execute the cell, and enter 2 numbers (one at a time) as command line inputs to the code.\n",
    "\n",
    "Entering two numbers separated by space didn't work (unlike in C/C++ code that I was used to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: '2 2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-0f023c9a7f71>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#x = int(input())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#y = int(input())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mxf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0myf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: '2 2'"
     ]
    }
   ],
   "source": [
    "#def TestPerceptron() #kp:\n",
    "# https://www.codesdope.com/python-readreadread/ #kp: (Read more about input() which takes everything as string)\n",
    "#With the int(input()), we can only enter integers, and it gives error if we enter otherwise\n",
    "#x = int(input())\n",
    "#y = int(input())\n",
    "xf = float(input())\n",
    "yf = float(input())\n",
    "x = int(xf)\n",
    "y = int(yf)\n",
    "outputP = x*weights[0] + y*weights[1] + bias*weights[2]\n",
    "if outputP > 0 : #activation function\n",
    "  outputP = 1\n",
    "else :\n",
    "  outputP = 0\n",
    "print(x, \"or\", y, \"is : \", outputP)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kp: Alternatively, let's make a test Function and call it as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TestPerceptron(input1=1.0, input2=1.0):\n",
    "    x = int(input1)\n",
    "    y = int(input2)\n",
    "    outputP = x*weights[0] + y*weights[1] + bias*weights[2]\n",
    "    if outputP > 0 : #activation function\n",
    "      outputP = 1\n",
    "    else :\n",
    "      outputP = 0\n",
    "    print(x, \"or\", y, \"is : \", outputP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 or 3 is :  1\n",
      "2 or 0 is :  1\n",
      "0 or 30 is :  1\n",
      "0 or 0 is :  0\n"
     ]
    }
   ],
   "source": [
    "TestPerceptron(2.0, 3.0)\n",
    "TestPerceptron(2.0, 0)\n",
    "TestPerceptron(0.0, 30.0)\n",
    "TestPerceptron(0.3, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GoTop](#GoTop) <a id='neuralNetworkFromScratch2'></a>\n",
    "## How to build a simple Neural Network from scratch with Python\n",
    "\n",
    "A quick guide to set up a Neural Network without using a framework. (Konstantinos Kitsios\n",
    "Follow\n",
    "Mar 18, 2019 · 6 min read)\n",
    "\n",
    "Ref: https://towardsdatascience.com/how-to-build-a-simple-neural-network-from-scratch-with-python-9f011896d2f3\n",
    "\n",
    "Neural Networks are becoming more and more popular every day and as a core field of Machine Learning and Artificial Intelligence they are going to play a major role in the technology, science and industry over the next years. This high popularity has given rise to many frameworks that allow you to implement Neural Networks very easily without knowing the complete theory behind them. On the other side, the strict theoretical proof of the Neural Networks algorithm demands a very high level of mathematics.\n",
    "\n",
    "In this post we will do something between. Specifically, in order to get a more solid understanding of the Neural Networks we will go through the actual implementation of a NN from scratch without using any framework. This may be a bit more difficult than using a framework but you will gain a much better understanding of the mechanism behind Neural Networks. Of course in large scale projects a framework implementation is preferred as it is easier and faster to set up. Also we will not give mathematical proofs of why the code below works as we suppose a basic understanding of the theory behind them.\n",
    "\n",
    "The tools used in this tutorial are just Python with numpy library(Scientific library for Linear Algebra operations). Supposing that you have python and pip installed, you can install numpy by running this command:\n",
    "```\n",
    "pip install numpy\n",
    "```\n",
    "A **Neural Network is actually a function of many variables:** It takes an input, makes computations and produces an output. We like to **visualise it as neurons in different layers, with each neuron in a layer connected with all neurons in the previous and the next layer.** All the computations take place inside those neurons and depend on the weights that connect the neurons with each other. **So all we have to do is learn the right weights in order to get the desired output**.\n",
    "\n",
    "Their structure is generally very complex including a lot of layers and even more than a million neurons in order to be able to handle the large datasets of our era. However, in order to understand how large deep neural networks work one should start with the simplest ones.\n",
    "\n",
    "Consequently, below we will implement a very simple network with 2 layers. In order to do that we need a very simple dataset as well, so we will use the **XOR dataset** in our example, as shown below. A and B are the 2 inputs of the NN and (A **XOR** B) is the output. We will try to make our NN **learn weights** such that whatever pair of A and B it takes as input, it will return the corresponding result.\n",
    "\n",
    "A | B | A **XOR** B\n",
    "--- | --- | ---\n",
    "0 | 0 | 0\n",
    "0 | 1 | 1\n",
    "1 | 0 | 1\n",
    "1 | 1 | 0\n",
    "The XOR truth table\n",
    "\n",
    "So, let’s start!\n",
    "\n",
    "First we need to define the structure of our Neural Network. Because our dataset is relatively simple, a network with just a hidden layer will do fine. So we will have an input layer, a hidden layer and an output layer. Next we need an activation function. The **sigmoid function** is a **good choice for the last layer** because it outputs values between 0 and 1 while **tanh(hyperbolic tangent)** works **better in the hidden layer**, but **every other commonly used function will work(e.g. ReLU)**. So the structure of our Neural Network will look like this:\n",
    "\n",
    "![Structure Of Neural Network](Images/structureOfNeuralNetwork.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the parameters to be learned are the weights W1, W2 and the biases b1, b2. As you can see W1 and b1 connect the input layer with the hidden layer while W2, b2 connect the hidden layer with the output layer. From the basic theory of NNs we know that the activations A1 and A2 are calculated as following:\n",
    "```\n",
    "A1 = h(W1*X + b1)\n",
    "A2 = g(W2*A1 + b2)\n",
    "```\n",
    "where g and h are the two activation functions we chose (for us sigmoid and tanh) and W1, W1, b1, b2 are generally Matrices.\n",
    "\n",
    "Now let’s jump into the actual code. The code style follows the guidelines proposed by prof. Andrew Ng at this [course](https://www.coursera.org/learn/machine-learning) in general.\n",
    "\n",
    "Note: You can find the fully working code in my repository [here](https://gitlab.com/kitsiosk/xor-neural-net)\n",
    "\n",
    "First we will implement our sigmoid activation function defined as follows: **g(z) = 1/(1+e^(-z))** where z will be a matrix in general. Luckily numpy supports calculations with matrices so the code is relatively simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "  return 1/(1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we have to initialize our parameters. Weight matrices W1 and W2 will be randomly initialized from a normal distribution while biases b1 and b2 will be initialized to zero.The function initialize_parameters(n_x, n_h, n_y) takes as input the number of units in each of the 3 layers and initializes the parameters properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y):\n",
    "  W1 = np.random.randn(n_h, n_x)\n",
    "  b1 = np.zeros((n_h, 1))\n",
    "  W2 = np.random.randn(n_y, n_h)\n",
    "  b2 = np.zeros((n_y, 1))\n",
    "\n",
    "  parameters = {\n",
    "    \"W1\": W1,\n",
    "    \"b1\" : b1,\n",
    "    \"W2\": W2,\n",
    "    \"b2\" : b2\n",
    "  }\n",
    "  return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#kp cell just for a quick check\n",
    "parametersT = {\n",
    "    \"W1\": 0.23,\n",
    "    \"b1\" : 'Krishna',\n",
    "    \"W2\": 2,\n",
    "    \"b2\" : \"Feb 2, 2020\"\n",
    "  }\n",
    "type(parametersT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to implement the Forward Propagation. The function forward_prop(X, parameters) takes as input the neural network input matrix X and the parameters dictionary and returns the output of the NN A2 with a cache dictionary that will be used later in back propagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, parameters):\n",
    "  W1 = parameters[\"W1\"]\n",
    "  b1 = parameters[\"b1\"]\n",
    "  W2 = parameters[\"W2\"]\n",
    "  b2 = parameters[\"b2\"]\n",
    "\n",
    "  Z1 = np.dot(W1, X) + b1\n",
    "  A1 = np.tanh(Z1)\n",
    "  Z2 = np.dot(W2, A1) + b2\n",
    "  A2 = sigmoid(Z2)\n",
    "\n",
    "  cache = {\n",
    "    \"A1\": A1,\n",
    "    \"A2\": A2\n",
    "  }\n",
    "  return A2, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have to compute the loss function. We will use the Cross Entropy Loss function. Calculate_cost(A2, Y) takes as input the result of the NN A2 and the groundtruth matrix Y and returns the cross entropy cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_cost(A2, Y):\n",
    "  cost = -np.sum(np.multiply(Y, np.log(A2)) +  np.multiply(1-Y, np.log(1-A2)))/m\n",
    "  cost = np.squeeze(cost)\n",
    "\n",
    "  return cos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the most difficult part of the Neural Network algorithm, Back Propagation. The code here may seem a bit weird and difficult to understand but we will not dive into details of why it works here. This function will return the gradients of the Loss function with respect to the 4 parameters of our network (W1, W2, b1, b2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_prop(X, Y, cache, parameters):\n",
    "  A1 = cache[\"A1\"]\n",
    "  A2 = cache[\"A2\"]\n",
    "\n",
    "  W2 = parameters[\"W2\"]\n",
    "\n",
    "  dZ2 = A2 - Y\n",
    "  dW2 = np.dot(dZ2, A1.T)/m\n",
    "  db2 = np.sum(dZ2, axis=1, keepdims=True)/m\n",
    "  dZ1 = np.multiply(np.dot(W2.T, dZ2), 1-np.power(A1, 2))\n",
    "  dW1 = np.dot(dZ1, X.T)/m\n",
    "  db1 = np.sum(dZ1, axis=1, keepdims=True)/m\n",
    "\n",
    "  grads = {\n",
    "    \"dW1\": dW1,\n",
    "    \"db1\": db1,\n",
    "    \"dW2\": dW2,\n",
    "    \"db2\": db2\n",
    "  }\n",
    "\n",
    "  return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

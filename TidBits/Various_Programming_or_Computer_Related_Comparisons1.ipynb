{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various Programming/Computer Related Comparisons (1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "* [Heap vs Stack](#HeapVsStack)\n",
    "* [Process vs Threads](#ProcessVsThreads)\n",
    "* [JDK vs JRE vs JVM](#JREvsJVMvsJDK)\n",
    "* [JIT compilers vs regular Compilers](#JITvsRegCompilers)\n",
    "* [Appendix](#appendix)\n",
    "    * [Anatomy of a Program in Memory](#Anatomy_of_a_program_in_memory)\n",
    "    * [FIFO vs LIFO](#LIFOvsFIFO)\n",
    "    * [Understanding the Memory Layout of Linux Executables](memory_layout_in_linux_executables)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='HeapVsStack'></a>\n",
    "## Heap vs Stack"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References: \n",
    "* http://net-informations.com/faq/net/stack-heap.htm\n",
    "* [Stack vs Heap Memory in C++](https://www.youtube.com/watch?v=wJ1L2nSIV1s) (TheChernoProject youtube video)\n",
    "* [Java Stack + Heap with Reference & Instance Variables](https://www.youtube.com/watch?v=UcPuWY0wn3w) Very nice youtube video that explains heap, stack, reference variables (those that reside in the main or some other methods but contain the memory address of or pointer to an object that is created in the Heap; these variables don't contain the values of primitive types) and instance variables (variables of an instance of a class which resides in the heap and these too may be of both ordinary and reference types) and the Garbage collection (which runs on Heap to find and collect the memory areas of objects that were referenced properly previously but no more).\n",
    "* [Memory Fundamentals - part 1 of Java Memory Management](https://www.youtube.com/watch?v=ckYwv4_Qtmo) Every thread (in a java application/process) has its own Stack, whereas the heap is shared across all threads. All objects are stored in the heap, while the local primitive variables such as ints and doubles and the references (memory addresses or pointers) to the objects are stored in the stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarites / Introduction / Putting it in context/perspective\n",
    "* Both stored in the computer's RAM\n",
    "* [This youtube video](https://www.youtube.com/watch?v=_8-ht2AKyH4) says that the memory assigned to a program can be divided into four sections - __Code (Text), Static/Global, Stack, and Heap__.\n",
    "    * [This site](https://www.geeksforgeeks.org/memory-layout-of-c-program/) identifies 5 sections in a C program - __Text segment, Initialized data segment, Uninitialized data segment, Stack, & Heap__.\n",
    "    * The same site says the following:\n",
    "        a. The __size(1) command__ reports the sizes (in bytes) of the text, data, and bss segments. ( for more details please refer man page of size(1) )\n",
    "        b. Uninitialized data segment, often called the “bss” segment, named after an ancient assembler operator that stood for “block started by symbol.” Data in this segment is initialized by the kernel to arithmetic 0 before the program starts executing.        \n",
    "```\n",
    "    [narendra@CentOS]$ gcc memory-layout.c -o memory-layout\n",
    "    [narendra@CentOS]$ size memory-layout\n",
    "    text       data        bss        dec        hex    filename\n",
    "     960        248         16       1224        4c8    memory-layout\n",
    " \n",
    "     Krishna's FunPrompt $ size GitProj/CppStuff/a.out \n",
    "    __TEXT\t__DATA\t__OBJC\tothers\tdec\thex\n",
    "    8192\t4096\t0\t4294971392\t4294983680\t100004000\t\n",
    "    Krishna's FunPrompt $ pwd\n",
    "    /Users/kpadhikari\n",
    "    Krishna's FunPrompt $\n",
    " ```\n",
    "* From [wikipedia](https://en.wikipedia.org/wiki/Data_segment): \n",
    "    * A computer program memory can be largely categorized into two sections: read-only and read-write. This distinction grew from early systems holding their main program in read-only memory such as Mask ROM, PROM or EEPROM. As systems became more complex and programs were loaded from other media into RAM instead of executing from ROM the idea that some portions of the program's memory should not be modified was retained. These became the .text and .rodata segments of the program, and the remainder which could be written to divided into a number of other segments for specific tasks. \n",
    "    * A  typical layout of a simple computer's program memory - text, various data, and stack and heap sections (see Fig. below).    \n",
    "![From [wikipedia](https://en.wikipedia.org/wiki/Data_segment): This shows the typical layout of a simple computer's program memory with the text, various data, and stack and heap sections.](Images/Program_memory_layout.pdf.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* From [hackerearth](https://www.hackerearth.com/practice/notes/memory-layout-of-c-program/) on __Memory Layout of C Program__: In practical words, when we run any C-program, its executable image is loaded into RAM of computer in an organized manner.\n",
    "    This memory layout is organized in following fashion :-  \n",
    "    ![Memory Layout](Images/Program_memory_layout2.png)\n",
    "    * __1>Text or Code Segment :-__ Text segment contains machine code of the compiled program. Usually, the text segment is sharable so that only a single copy needs to be in memory for frequently executed programs, such as text editors, the C compiler, the shells, and so on. The text segment of an executable object file is often read-only segment that prevents a program from being accidentally modified.\n",
    "    * __2>Initialized Data Segment :-__ Initialized data stores all global, static, constant, and external variables ( declared with extern keyword ) that are initialized beforehand. Data segment is not read-only, since the values of the variables can be altered at run time.\n",
    "    \n",
    "    This segment can be further classified into initialized read-only area and initialized read-write area."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```cpp\n",
    "#include <stdio.h>\n",
    "\n",
    "char c[]=\"rishabh tripathi\";     /*  global variable stored in Initialized Data Segment in read-write area*/\n",
    "const char s[]=\"HackerEarth\";    /* global variable stored in Initialized Data Segment in read-only area*/\n",
    "\n",
    "int main()\n",
    "{\n",
    "    static int i=11;          /* static variable stored in Initialized Data Segment*/\n",
    "    return 0;\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   * __3> Uninitialized Data Segment (bss) :-__ Data in this segment is initialized to arithmetic 0 before the program starts executing. Uninitialized data starts at the end of the data segment and contains all global variables and static variables that are initialized to 0 or do not have explicit initialization in source code.\n",
    "\n",
    "```cpp\n",
    "#include <stdio.h>\n",
    "\n",
    "char c;               /* Uninitialized variable stored in bss*/\n",
    "\n",
    "int main()\n",
    "{\n",
    "    static int i;     /* Uninitialized static variable stored in bss */\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "   * __4>Heap :-__ Heap is the segment where dynamic memory allocation usually takes place. When some more memory need to be allocated using malloc and calloc function, heap grows upward. The Heap area is shared by all shared libraries and dynamically loaded modules in a process.\n",
    "\n",
    "```cpp\n",
    "#include <stdio.h>\n",
    "int main()\n",
    "{\n",
    "    char *p=(char*)malloc(sizeof(char));    /* memory allocating in heap segment */\n",
    "    return 0;\n",
    "}\n",
    "```\n",
    "   * __5>Stack :-__ Stack segment is used to store all local variables and is used for passing arguments to the functions along with the return address of the instruction which is to be executed after the function call is over. Local variables have a scope to the block which they are defined in, they are created when control enters into the block. All recursive function calls are added to stack.\n",
    "\n",
    "    The stack and heap are traditionally located at opposite ends of the process's virtual address space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences\n",
    "* Stack is used for static memory allocation (kp: note the coincidental common initial letters 'sta' to help our own memory) and Heap for dynamic memory allocation.\n",
    "* Access to stack memory is very fast because variables allocated on the stack are stored directly to the memory, and the allocation is dealt with during the program compilation.\n",
    "    * When a function or a method calls another function which, in turn, calls another function and so on, the execution of all those functions remain suspended until the very last function returns its value.\n",
    "* The stack is always reserved in a LIFO (_kp: __LIFO == Last In, First Out__ - __like__ storing some sort of solid items in a can or bucket in which there is only one opening, __unlike__ liquid passing through (or stored in) a pipe with two openings??)_) order, the most recently reserved block is always the next block to be freed.\n",
    "    * This makes it really simple to keep track of the stack, freeing a block from the stack is nothing more than adjusting one pointer.\n",
    "* Access to heap memory is a bit slower because the variables are allocated at run time.\n",
    "* Heap size is limited only by the size of the virtual memory.\n",
    "* Elements of the heap have no dependencies with each other and can always be accessed randomly at any time. We can allocate a block at any time and free it at any time.\n",
    "    * This makes it much more complex to keep track of which parts of the heap are allocated or free at any given time.\n",
    "* We can use the stack if we know exactly how much data (kp: memory?) we need to allocate before compile time and it is not too big. We can use heap if we don't know exactly how much data we will need at runtime or if we need to allocate a lot of data. _(kp: It means, even if we know exactly how much we need but if it's a lot, then we should use the heap rather than stack, which is limited in size)_.\n",
    "* In a multi-threaded situation, each thread will have its own completely independent stack, but they will share the heap. __(kp: Sounds like a major difference)__ Stack is thread specific and Heap is application specific. The stack is important to consider in exception handling and thread executions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='ProcessVsThreads'></a>\n",
    "## Process vs Thread(s)\n",
    "\n",
    "### References\n",
    "Ref: http://net-informations.com/faq/net/thread-process.htm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processes & threads are independent sequences of execution.\n",
    "\n",
    "### Differences\n",
    "* Typical difference is that threads run in a shared memory space, while processes run in separate memory spaces.\n",
    "* A process has a self contained execution environment, meaning it has a complete, private set of basic run time resources, particularly each process has its own memory space (in other words, it doesn't share a memory space with any other process). On the other hand, thread(s) exist within a process and every process has at least one thread. _(kp: May be we can think of a process as a thread-container and thread(s) as the content(s))._\n",
    "* Each process provides the resources needed to execute a program. Each process is started with a single thread, known as the primary thread. The process then may have multiple threads thereafter.\n",
    "* On a multiprocessor system, multiple processes can be executed in parallel. _(kp: I think in a single processor machine, true-parallelism isn't possible, rather some processes may run that may feel like running in parallel, but actually they are running alternatingly on tiny time splits.)_ Multiple threads of control can exploit the true parallelism possible on multiprocessor systems.\n",
    "* Threads have direct access to the data segment of its process but processes have their own copy of the data segment of the parent process.\n",
    "* Changes to the main thread may affect the behavior of the other threads of the process, whereas changes to the parent process doesn't affect child processes.\n",
    "* Processes are heavily dependent on system resources available while threads require minimal amounts of resource, so a process is considered as heavyweight while a thread is termed as a lightweight process.\n",
    "\n",
    "#### What is multithreading ?\n",
    "\n",
    "In .NET languages you can write applications that perform multiple tasks simultaneously. Tasks with the potential of holding up other tasks can execute on separate threads is known as multithreading.\n",
    "\n",
    "The following links shows how to multithreading in c# and VB.Net\n",
    "\n",
    "[C# Multithreading Program](http://csharp.net-informations.com/communications/csharp-multi-threaded-socket.htm)   \n",
    "\n",
    "[VB.Net Multithreading Program](http://vb.net-informations.com/communications/vb.net_multithreaded_Socket_programming.htm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JIT - Just In Time (Compiler(s))\n",
    "References: <br />\n",
    "* https://www.investopedia.com/terms/j/jit.asp (unrelated/Wrong reference)\n",
    "* http://net-informations.com/faq/net/clr.htm\n",
    "* https://en.wikipedia.org/wiki/Just-in-time_compilation\n",
    "* https://aboullaite.me/understanding-jit-compiler-just-in-time-compiler/\n",
    "## JIT of JRE ( Java Runtime Environment)\n",
    "The Just-In-Time (JIT) compiler is a component of the Java Runtime Environment that improves the performance of Java applications at run time. Nothing in the JVM affects performance more than the compiler, and choosing a compiler is one of the first decisions made when running a Java application—whether you are a Java developer or an end-user. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Java JIT Compiler: An Overview\n",
    "\n",
    "The __key of java power \"Write once, run everywhere\" is bytecode__. The way bytecodes get converted to the appropriate native instructions _(kp: the [CPU native code](http://net-informations.com/faq/net/clr.htm) or the machine code)_ for an application has a huge impact on the speed of an application. These bytecode can be interpreted, compiled to native code or directly executed on a processor whose Instruction Set Architecture is the bytecode specification. Interpreting the bytecode which is the standard implementation of the Java\n",
    "    \n",
    "    kp: The bytecode can be read/understood/used by the machine/computer in three ways:\n",
    "        * by using interpreter\n",
    "        * by compiling to native code\n",
    "        * by directly executing on a processor if the Instruction Set Architecture is the bytecode specification.\n",
    "        \n",
    "Virtual Machine (JVM) makes execution of programs slow. To improve performance, JIT compilers interact with the JVM at run time and compile appropriate bytecode sequences into native machine code. When using a JIT compiler, the hardware can execute the native code, as opposed to having the JVM interpret the same sequence of bytecode repeatedly and incurring the penalty of a relatively lengthy translation process. This can lead to performance gains in the execution speed, unless methods are executed less frequently. The time that a JIT compiler takes to compile the bytecode is added to the overall execution time, and could lead to a higher execution time than an interpreter for executing the bytecode if the methods that are compiled by the JIT are not invoked frequently. The JIT compiler performs certain optimizations when compiling the bytecode to native code. Since the JIT compiler translates a series of bytecode into native instructions, it can perform some simple optimizations. Some of the common optimizations performed by JIT compilers are data-analysis, translation from stack operations to register operations, reduction of memory accesses by register allocation, elimination of common sub-expressions etc. The higher the degree of optimization done by a JIT compiler, the more time it spends in the execution stage. Therefore a JIT compiler cannot afford to do all the optimizations that is done by a static compiler, both because of the overhead added to the execution time and because it has only a restricted view of the program."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Common Intermediate Language (CIL) or Microsoft Intermediate Language (MSIL)\n",
    "From [wikipedia](https://en.wikipedia.org/wiki/Common_Intermediate_Language): <br />\n",
    "Common Intermediate Language (CIL), formerly called Microsoft Intermediate Language (MSIL), is the lowest-level human-readable programming language defined by the Common Language Infrastructure (CLI) specification and is used by the .NET Framework and Mono. Languages which target a CLI-compatible runtime environment compile to CIL, which is assembled into an object code that has a bytecode-style format. CIL is an object-oriented assembly language, and is entirely stack-based. Its bytecode is translated into native code or—most commonly—executed by a virtual machine.\n",
    "\n",
    "CIL was originally known as Microsoft Intermediate Language (MSIL) during the beta releases of the .NET languages. Due to standardization of C# and the Common Language Infrastructure, the bytecode is now officially known as CIL.[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='JREvsJVMvsJDK'></a>\n",
    "## JRE vs JVM vs JDK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### JVM (Java Virtual Machine\n",
    "JVM (Java Virtual Machine) is an abstract machine. It is a specification that provides runtime environment in which java bytecode can be executed.\n",
    "\n",
    "JVMs are available for many hardware and software platforms. JVM, JRE and JDK are platform dependent because configuration of each OS differs. But, Java is platform independent.\n",
    "\n",
    "The JVM performs following main tasks:\n",
    "\n",
    "    Loads code\n",
    "    Verifies code\n",
    "    Executes code\n",
    "    Provides runtime environment\n",
    "\n",
    "\n",
    "### Java Runtime Environment\n",
    "It is used to provide runtime environment. __It is the implementation of JVM__ _(kp: which means, there can be different implementations and as a result, perhaps there are different versions already.)_. It physically exists. It contains set of libraries + other files that JVM uses at runtime.\n",
    "\n",
    "Implementation of JVMs are also actively released by other companies besides Sun Micro Systems.\n",
    "![JRE](Images/jre2.JPG)\n",
    "\n",
    "### JDK (Java Development Kit)\n",
    "It is something that physically exists and it contains JRE + development tools.\n",
    "![JDK](Images/jdk2.JPG)\n",
    "\n",
    "### References:\n",
    "* https://www.javatpoint.com/difference-between-jdk-jre-and-jvm\n",
    "* https://stackoverflow.com/questions/11547458/what-is-the-difference-between-jvm-jdk-jre-openjdk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Abstract Class Vs Interface\n",
    "Ref: http://net-informations.com/faq/net/abstract.htm <br />\n",
    "\n",
    "An abstract class cannot support multiple inheritance, but an interface can support multiple inheritance. Thus a class may inherit several interfaces but only one abstract class.\n",
    "\n",
    "An interface is an empty shell, just only the signatures of the methods. The methods do not contain anything. The interface can't do anything. It's just a pattern. An Abstract class is a class which will contains both definition and implementation in it.\n",
    "\n",
    "Abstract classes can have consts, members, method stubs and defined methods, whereas interfaces can only have consts and methods.\n",
    "\n",
    "Various access modifiers such as abstract, protected, internal, public, virtual, etc. are useful in abstract Classes but all methods of an interface must be defined as public.\n",
    "\n",
    "A child class can define abstract methods with the same or less restrictive visibility, whereas a class implementing an interface must define the methods with the exact same visibility.\n",
    "\n",
    "If we add a new method to an Interface then we have to track down all the implementations of the interface and define implementation for the new method. But if we add a new method to an abstract class then we have the option of providing default implementation and therefore all the existing code might work properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "kp: <br />\n",
    "\n",
    "Java (and perhaps C++ too, but please verify later) uses interface but abstract class is found in C++."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='appendix'></a>\n",
    "# Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Anatomy_of_a_program_in_memory'></a> (Coped from [Blog by Gustavo Duarte](http://duartes.org/gustavo/blog/post/anatomy-of-a-program-in-memory/))\n",
    "## Anatomy of a Program in Memory\n",
    "\n",
    "Jan 27th, 2009\n",
    "\n",
    "Memory management is the heart of operating systems; it is crucial for both programming and system administration. In the next few posts I’ll cover memory with an eye towards practical aspects, but without shying away from internals. While the concepts are generic, examples are mostly from Linux and Windows on 32-bit x86. This first post describes how programs are laid out in memory.\n",
    "\n",
    "Each process in a multi-tasking OS runs in its own memory sandbox. This sandbox is the __virtual address space__, which in 32-bit mode is __always a 4GB block of memory addresses__. These virtual addresses are mapped to physical memory by __page tables__, which are maintained by the operating system kernel and consulted by the processor. Each process has its own set of page tables, but there is a catch. Once virtual addresses are enabled, they apply to all software running in the machine, _including the kernel itself_. Thus a portion of the virtual address space must be reserved to the kernel:\n",
    "![Kernel User Memory Split](Images/kernelUserMemorySplit.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This does not mean the kernel uses that much physical memory, only that it has that portion of address space available to map whatever physical memory it wishes. Kernel space is flagged in the page tables as exclusive to [privileged code](http://duartes.org/gustavo/blog/post/cpu-rings-privilege-and-protection/) (ring 2 or lower), hence a page fault is triggered if user-mode programs try to touch it. In Linux, kernel space is constantly present and maps the same physical memory in all processes. Kernel code and data are always addressable, ready to handle interrupts or system calls at any time. By contrast, the mapping for the user-mode portion of the address space changes whenever a process switch happens:\n",
    "\n",
    "![Virtual Memory In Process Switch](Images/virtualMemoryInProcessSwitch.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blue regions represent virtual addresses that are mapped to physical memory, whereas white regions are unmapped. In the example above, Firefox has used far more of its virtual address space due to its legendary memory hunger. The distinct bands in the address space correspond to memory segments like the heap, stack, and so on. Keep in mind these segments are simply a range of memory addresses and have nothing to do with [Intel-style segments](http://duartes.org/gustavo/blog/post/memory-translation-and-segmentation). Anyway, here is the standard segment layout in a Linux process:\n",
    "\n",
    "![Linux Flexible Address Space Layout](Images/linuxFlexibleAddressSpaceLayout.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When computing was happy and safe and cuddly, the starting virtual addresses for the segments shown above were __exactly the same__ for nearly every process in a machine. This made it easy to exploit security vulnerabilities remotely. An exploit often needs to reference absolute memory locations: an address on the stack, the address for a library function, etc. Remote attackers must choose this location blindly, counting on the fact that address spaces are all the same. When they are, people get pwned. Thus address space randomization has become popular. Linux randomizes the [stack](http://lxr.linux.no/linux+v2.6.28.1/fs/binfmt_elf.c#L542), [memory mapping segment](http://lxr.linux.no/linux+v2.6.28.1/arch/x86/mm/mmap.c#L84), and [heap](http://lxr.linux.no/linux+v2.6.28.1/arch/x86/kernel/process_32.c#L729) by adding offsets to their starting addresses. Unfortunately the 32-bit address space is pretty tight, leaving little room for randomization and [hampering its effectiveness](http://www.stanford.edu/~blp/papers/asrandom.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The topmost segment in the process address space is the stack, which stores local variables and function parameters in most programming languages. Calling a method or function pushes a new __stack frame__ onto the stack. The stack frame is destroyed when the function returns. This simple design, possible because the data obeys strict [LIFO](http://en.wikipedia.org/wiki/Lifo) order, means that no complex data structure is needed to track stack contents – a simple pointer to the top of the stack will do. Pushing and popping are thus very fast and deterministic. Also, the constant reuse of stack regions tends to keep active stack memory in the [cpu caches](http://duartes.org/gustavo/blog/post/intel-cpu-caches), speeding up access. Each thread in a process gets its own stack.\n",
    "\n",
    "It is possible to exhaust the area mapping the stack by pushing more data than it can fit. This triggers a page fault that is handled in Linux by [expand_stack()](http://www.kernel.org/doc/man-pages/online/pages/man2/mmap.2.html), which in turn calls [acct_stack_growth()](Anatomy of a Program in Memory) to check whether it’s appropriate to grow the stack. If the stack size is below RLIMIT_STACK (usually 8MB), then normally the stack grows and the program continues merrily, unaware of what just happened. This is the normal mechanism whereby stack size adjusts to demand. However, if the maximum stack size has been reached, we have a stack overflow and the program receives a Segmentation Fault. While the mapped stack area expands to meet demand, it does not shrink back when the stack gets smaller. Like the federal budget, it only expands.\n",
    "\n",
    "Dynamic stack growth is the [only situation](http://lxr.linux.no/linux+v2.6.28.1/arch/x86/mm/fault.c#L692) in which access to an unmapped memory region, shown in white above, might be valid. Any other access to unmapped memory triggers a page fault that results in a Segmentation Fault. Some mapped areas are read-only, hence write attempts to these areas also lead to segfaults.\n",
    "\n",
    "Below the stack, we have the memory mapping segment. Here the kernel maps contents of files directly to memory. Any application can ask for such a mapping via the Linux [mmap()](http://www.kernel.org/doc/man-pages/online/pages/man2/mmap.2.html) system call ([implementation](http://lxr.linux.no/linux+v2.6.28.1/arch/x86/kernel/sys_i386_32.c#L27)) or [CreateFileMapping()](http://msdn.microsoft.com/en-us/library/aa366537(VS.85).aspx) / [MapViewOfFile()](http://msdn.microsoft.com/en-us/library/aa366761(VS.85).aspx) in Windows. Memory mapping is a convenient and high-performance way to do file I/O, so it is used for loading dynamic libraries. It is also possible to create an anonymous memory mapping that does not correspond to any files, being used instead for program data. In Linux, if you request a large block of memory via [malloc()](http://www.kernel.org/doc/man-pages/online/pages/man3/malloc.3.html), the C library will create such an anonymous mapping instead of using heap memory. ‘Large’ means larger than MMAP_THRESHOLD bytes, 128 kB by default and adjustable via [mallopt()](http://www.kernel.org/doc/man-pages/online/pages/man3/undocumented.3.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaking of the heap, it comes next in our plunge into address space. The heap provides runtime memory allocation, like the stack, meant for data that must outlive the function doing the allocation, unlike the stack. Most languages provide heap management to programs. Satisfying memory requests is thus a joint affair between the language runtime and the kernel. In C, the interface to heap allocation is [malloc()](http://www.kernel.org/doc/man-pages/online/pages/man3/malloc.3.html) and friends, whereas in a garbage-collected language like C# the interface is the new keyword.\n",
    "\n",
    "If there is enough space in the heap to satisfy a memory request, it can be handled by the language runtime without kernel involvement. Otherwise the heap is enlarged via the [brk()](http://www.kernel.org/doc/man-pages/online/pages/man2/brk.2.html) system call ([implementation](Anatomy of a Program in Memory)) to make room for the requested block. Heap management is [complex](Anatomy of a Program in Memory), requiring sophisticated algorithms that strive for speed and efficient memory usage in the face of our programs’ chaotic allocation patterns. The time needed to service a heap request can vary substantially. Real-time systems have [special-purpose allocators](http://rtportal.upv.es/rtmalloc/) to deal with this problem. Heaps also become fragmented, shown below:\n",
    "\n",
    "![Fragmented Heap](Images/fragmentedHeap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we get to the lowest segments of memory: BSS, data, and program text. Both BSS and data store contents for static (global) variables in C. The difference is that BSS stores the contents of _uninitialized_ static variables, whose values are not set by the programmer in source code. The BSS memory area is anonymous: it does not map any file. If you say static int cntActiveUsers, the contents of cntActiveUsers live in the BSS.\n",
    "\n",
    "The data segment, on the other hand, holds the contents for static variables initialized in source code. This memory area __is not anonymous__. It maps the part of the program’s binary image that contains the initial static values given in source code. So if you say static int cntWorkerBees = 10, the contents of cntWorkerBees live in the data segment and start out as 10. Even though the data segment maps a file, it is a __private memory mapping__, which means that updates to memory are not reflected in the underlying file. This must be the case, otherwise assignments to global variables would change your on-disk binary image. Inconceivable!\n",
    "\n",
    "The data example in the diagram is trickier because it uses a pointer. In that case, the contents of pointer gonzo – a 4-byte memory address – live in the data segment. The actual string it points to does not, however. The string lives in the __text__ segment, which is read-only and stores all of your code in addition to tidbits like string literals. The text segment also maps your binary file in memory, but writes to this area earn your program a Segmentation Fault. This helps prevent pointer bugs, though not as effectively as avoiding C in the first place. Here’s a diagram showing these segments and our example variables:\n",
    "\n",
    "![Mapping Binary Image](Images/mappingBinaryImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can examine the memory areas in a Linux process by reading the file /proc/pid_of_process/maps. Keep in mind that a segment may contain many areas. For example, each memory mapped file normally has its own area in the mmap segment, and dynamic libraries have extra areas similar to BSS and data. The next post will clarify what ‘area’ really means. Also, sometimes people say “data segment” meaning all of data + bss + heap.\n",
    "\n",
    "You can examine binary images using the [nm](http://manpages.ubuntu.com/manpages/intrepid/en/man1/nm.1.html) and [objdump](http://manpages.ubuntu.com/manpages/intrepid/en/man1/objdump.1.html) commands to display symbols, their addresses, segments, and so on. Finally, the virtual address layout described above is the “flexible” layout in Linux, which has been the default for a few years. It assumes that we have a value for RLIMIT_STACK. When that’s not the case, Linux reverts back to the “classic” layout shown below:\n",
    "\n",
    "![Linux Classic Address Space Layout](Images/linuxClassicAddressSpaceLayout.png)\n",
    "\n",
    "That’s it for virtual address space layout. The next post discusses how the kernel keeps track of these memory areas. Coming up we’ll look at memory mapping, how file reading and writing ties into all this and what memory usage figures mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some comments on this article\n",
    "1. Your posts are some of the most informative I’ve ever found on the internet.\n",
    "4. Gustavo Duarte on January 27th, 2009 1:22 am\n",
    "\n",
    "    @michele: Take a look at the end of this post. It has a list of Linux kernel books.\n",
    "\n",
    "    My favorite book is still “Understanding the Linux Kernel” because it explains _everything_ in painstaking detail. It is dry, but the authors put monumental effort into going through everything.\n",
    "\n",
    "    The Intel manuals are free and also excellent.\n",
    "\n",
    "    These books are the best resource I know of. I hope to write more material for this blog and eventually maybe have a short ‘Intro to the Linux Kernel’ document online. However this is subject to my work schedule and so on.\n",
    "7. Great article! Quite shocked to know that windows takes double the kernel memory compared to Linux.\n",
    "8. Excellent writeup! I’m wondering, though, why does the kernel space consume 1gb? That seems like a lot.. And __if you don’t mind divulging a trade secret__, what do you use to draw your diagrams?\n",
    "9. Great post. You left some stuff out though. In modern linux/windows OS’s the __heap base is also randomized__. And in Linux string literals such as char *blah = “hello there”; will be stored in an ELF section called .rodata. Rarely is constant data such as strings held in .text, but it does happen. Good post though, I like the graphics.\n",
    "17. Gustavo Duarte on January 27th, 2009 9:40 am\n",
    "\n",
    "    Thank you all for the feedback!\n",
    "\n",
    "    @web dev: The __kernel is not really using that much memory physical though, it simply has that virtual range available to itself to map whatever physical memory it wishes.__ Thanks for the question though – I clarified this in the post.\n",
    "\n",
    "    __Both the Linux and Windows kernel are extremely well built. It’s hard to find areas where one really has an edge, imho.(kp: imho=in my honest opinion)__ Two outstanding pieces of software.\n",
    "\n",
    "    @numerodix: You know, this __‘tightness’ of the address space is a sort of recent phenomenon. When the kernels were designed, 2 or 3GB seemed like a lot :) So partially it’s an evolutionary artifact, one that is fixed by 64-bit address spaces__.\n",
    "\n",
    "    But also, it is good for performance to give the kernel an ample window into memory. I think the next couple posts should clarify why this is.\n",
    "\n",
    "    @Reader1: thanks for the corrections. I’ll add the heap randomization to the post. Regarding ELF sections, I thought about them, but __I’m always balancing what to include in these blog posts. I try hard to keep it crisp, covering one area well, but without dumbing anything down. But there’s so much interconnected stuff, it’s not always clear where to put the line.__ I think I’m going to leave ELF sections out for now though.\n",
    "\n",
    "    @Jose: thanks for the heads up :)\n",
    "\n",
    "    @el_bot: This is one similar to ELF sections above. The __tradeoff between conciseness and completeness__. I’m planning a post covering the stack in detail, and talking about buffer overflows, and I think that’d go in well there.\n",
    "19. Gustavo Duarte on January 27th, 2009 10:25 am\n",
    "\n",
    "    I use [Visio 2007](https://products.office.com/en-us/microsoft-visio-2007) for the diagrams. Cheers.\n",
    "        * From wikipedia: Microsoft Visio (/ˈvɪz.i.oʊ/ VIZ-ee-oh) (formerly Microsoft Office Visio) is a diagramming and vector graphics application and is part of the Microsoft Office family. The product was first introduced in 1992, made by the Shapeware Corporation. It was acquired by Microsoft in 2000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='LIFOvsFIFO'></a>\n",
    "### LIFO vs FIFO\n",
    "Imagine storage of items two types of containers\n",
    "* A data __buffer like a can with one end open or a room with only one door for entry & exit__ - here we can only use FIFO to get any item or entry/entrant out.\n",
    "* Like a line/queue of people standing on line for FCFS (First Come First Serve), or Like a pipe with both ends open and an addition rule that one end is used for input and the other for output (here we ignore gravity so as not to worry about unintended spills or dropouts of some stored items and 'storage only provides space/place' with no worries about gravity or anything else.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FIFO\n",
    "__From https://en.wikipedia.org/wiki/FIFO_(computing_and_electronics) __<br />\n",
    "FIFO is an acronym for first in, first out, a method for organizing and manipulating a data buffer, where the oldest (first) entry, or 'head' of the queue, is processed first. It is analogous to processing a queue with first-come, first-served (FCFS) behaviour: where the people leave the queue in the order in which they arrive.\n",
    "\n",
    "FCFS is also the jargon term for the FIFO operating system scheduling algorithm, which gives every process central processing unit (CPU) time in the order in which it is demanded.\n",
    "\n",
    "FIFO's opposite is LIFO, last-in-first-out, where the youngest entry or 'top of the stack' is processed first.[1]\n",
    "\n",
    "A priority queue is neither FIFO or LIFO but may adopt similar behaviour temporarily or by default.\n",
    "\n",
    "Queueing theory encompasses these methods for processing data structures, as well as interactions between strict-FIFO queues.\n",
    "\n",
    "__From https://en.wikipedia.org/wiki/FIFO_and_LIFO_accounting __<br />\n",
    "FIFO and LIFO accounting are methods used in managing inventory and financial matters involving the amount of money a company has to tied up within inventory of produced goods, raw materials, parts, components, or feed stocks. They are used to manage assumptions of cost sheet related to inventory, stock repurchases (if purchased at different prices), and various other accounting purposes.\n",
    "\n",
    "#### LIFO\n",
    "__ From https://en.wikipedia.org/wiki/Stack_(abstract_data_type) __\n",
    "\n",
    "In computer science, a stack is an abstract data type that serves as a collection of elements, with two principal operations:\n",
    "\n",
    "    push, which adds an element to the collection, and\n",
    "    pop, which removes the most recently added element that was not yet removed.\n",
    "\n",
    "The order in which elements come off a stack gives rise to its alternative name, LIFO (last in, first out). Additionally, a peek operation may give access to the top without modifying the stack.[1]\n",
    "\n",
    "![Simple representation of a stack runtime with push and pop operations.](Images/Lifo_stack.png)\n",
    "\n",
    "The name \"stack\" for this type of structure comes from the analogy to a set of physical items stacked on top of each other, which makes it easy to take an item off the top of the stack, while getting to an item deeper in the stack may require taking off multiple other items first.[2]\n",
    "\n",
    "Considered as a linear data structure, or more abstractly a sequential collection, the push and pop operations occur only at one end of the structure, referred to as the top of the stack. This makes it possible to implement a stack as a singly linked list and a pointer to the top element.\n",
    "\n",
    "A stack may be implemented to have a bounded capacity. If the stack is full and does not contain enough space to accept an entity to be pushed, the stack is then considered to be in an overflow state. The pop operation removes an item from the top of the stack."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='memory_layout_in_linux_executables'></a>\n",
    "Following is all copied from [this page](https://gist.github.com/CMCDragonkai/10ab53654b2aa6ce55c11cfc5b2432a4)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Understanding the Memory Layout of Linux Executables\n",
    "====================================================\n",
    "\n",
    "Required tools for playing around with memory:\n",
    "\n",
    "* `hexdump`\n",
    "* `objdump`\n",
    "* `readelf`\n",
    "* `xxd`\n",
    "* `gcore`\n",
    "* `strace`\n",
    "* `diff`\n",
    "* `cat`\n",
    "\n",
    "We're going to go through this: https://sploitfun.wordpress.com/2015/02/10/understanding-glibc-malloc/ and http://duartes.org/gustavo/blog/post/anatomy-of-a-program-in-memory/\n",
    "\n",
    "There are actually many C memory allocators. And different memory allocators will layout memory in different ways. Currently glibc's memory allocator is `ptmalloc2`. It was forked from dlmalloc. After fork, threading support was added, and was released in 2006. After being integrated, code changes were made directly to glibc's malloc source code itself. So there are many changes with glibc's malloc that is different from the original `ptmalloc2`.\n",
    "\n",
    "The malloc in glibc, internally invokes either `brk` or `mmap` syscalls to acquire memory from the OS. The `brk` syscall is generally used to increase the size of the heap, while `mmap` will be used to load shared libraries, create new regions for threads, and many other things. It actually switches to using `mmap` instead of `brk` when the amount of memory requested is larger than the `MMAP_THRESHOLD`. We view which calls are being made by using `strace`.\n",
    "\n",
    "In the old days using `dlmalloc`, when 2 threads call malloc at the same time, only one thread can enter the critical section, the freelist data structure of memory chunks is shared among all available threads. Hence memory allocation is a global locking operation.\n",
    "\n",
    "However in ptmalloc2, when 2 threads call malloc at the same time, memory is allocated immediately, since each thread maintains a separate heap, and their own freelist chunk data structure\n",
    "\n",
    "The act of maintain separate heap and freelists for each thread is called \"per-thread arena\".\n",
    "\n",
    "In the last session, we identified that a program memory layout is generally in:\n",
    "\n",
    "```\n",
    "User Stack\n",
    "    |\n",
    "    v\n",
    "Memory Mapped Region for Shared Libraries or Anything Else\n",
    "    ^\n",
    "    |\n",
    "Heap\n",
    "Uninitialised Data (.bss)\n",
    "Initialised Data (.data)\n",
    "Program Text (.text)\n",
    "0\n",
    "```\n",
    "\n",
    "For the purpose of understanding, most of the tools that investigate memory put the low address at the top  and the high address at the bottom.\n",
    "\n",
    "Therefore, it's easier to think of it like this:\n",
    "\n",
    "```\n",
    "0\n",
    "Program Text (.text)\n",
    "Initialised Data (.data)\n",
    "Uninitialised Data (.bss)\n",
    "Heap\n",
    "    |\n",
    "    v\n",
    "Memory Mapped Region for Shared Libraries or Anything Else\n",
    "    ^\n",
    "    |\n",
    "User Stack\n",
    "```\n",
    "\n",
    "The problem is, we didn't have a proper grasp over what exactly is happening. And the above diagram is too simple to fully understand.\n",
    "\n",
    "Let's write some C programs and investigate their memory structure.\n",
    "\n",
    "> Note that neither direct compilation or assembly actually produce an executable. That is done by the linker, which takes the various object code files produced by compilation/assembly, resolves all the names they contain and produces the final executable binary.\n",
    "> http://stackoverflow.com/a/845365/582917\n",
    "\n",
    "This is our first program (compile it using `gcc -pthread memory_layout.c -o memory_layout`:\n",
    "\n",
    "```c\n",
    "#include <stdio.h> // standard io\n",
    "#include <stdlib.h> // C standard library\n",
    "#include <pthread.h> // threading\n",
    "#include <unistd.h> // unix standard library\n",
    "#include <sys/types.h> // system types for linux\n",
    "\n",
    "// getchar basically is like \"read\"\n",
    "// it prompts the user for input\n",
    "// in this case, the input is thrown away\n",
    "// which makes similar to a \"pause\" continuation primitive \n",
    "// but a pause that is resolved through user input, which we promptly throw away!\n",
    "void * thread_func (void * arg) {\n",
    "\n",
    "    printf(\"Before malloc in thread 1\\n\");\n",
    "    getchar();\n",
    "    char * addr = (char *) malloc(1000);\n",
    "    printf(\"After malloc and before free in thread 1\\n\");\n",
    "    getchar();\n",
    "    free(addr);\n",
    "    printf(\"After free in thread 1\\n\");\n",
    "    getchar();\n",
    "\n",
    "}\n",
    "\n",
    "int main () {\n",
    "\n",
    "    char * addr;\n",
    "    printf(\"Welcome to per thread arena example::%d\\n\", getpid());\n",
    "    printf(\"Before malloc in the main thread\\n\");\n",
    "    getchar();\n",
    "    addr = (char *) malloc(1000);\n",
    "    printf(\"After malloc and before free in main thread\\n\");\n",
    "    getchar();\n",
    "    free(addr);\n",
    "    printf(\"After free in main thread\\n\");\n",
    "    getchar();\n",
    "\n",
    "    // pointer to the thread 1\n",
    "    pthread_t thread_1;\n",
    "    // pthread_* functions return 0 upon succeeding, and other numbers upon failing\n",
    "    int pthread_status;\n",
    "\n",
    "    pthread_status = pthread_create(&thread_1, NULL, thread_func, NULL);\n",
    "    \n",
    "    if (pthread_status != 0) {\n",
    "        printf(\"Thread creation error\\n\");\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    // returned status code from thread_1\n",
    "    void * thread_1_status;\n",
    "\n",
    "    pthread_status = pthread_join(thread_1, &thread_1_status);\n",
    "    \n",
    "    if (pthread_status != 0) {\n",
    "        printf(\"Thread join error\\n\");\n",
    "        return -1;\n",
    "    }\n",
    "\n",
    "    return 0;\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "The usage of `getchar` above is to basically pause the computation waiting for user input. This allows us to step through the program, when examining its memory layout.\n",
    "\n",
    "The usage of `pthread` is for creating POSIX threads, which are real kernel threads being scheduled on the Linux OS. The thing si, the usage of threads is interesting for examining how a process memory layout is utilised for many threads. It turns out that each thread requires its own heap and stack.\n",
    "\n",
    "The `pthread` functions kind of weird, because they return a 0 based status code upon success. This is the success of a `pthread` operation, which does involve a side effect on the underlying operating system.\n",
    "\n",
    "As we can see above, there are many uses of the reference error pattern, that is, instead of returning multiple values (through tuples), we use reference containers to store extra metadata or just data itself.\n",
    "\n",
    "Now, we can run the program `./memory_layout` (try using `Ctrl + Z` to suspend the program):\n",
    "\n",
    "```\n",
    "$ ./memory_layout\n",
    "Welcome to per thread arena example::1255\n",
    "Before malloc in the main thread\n",
    "```\n",
    "\n",
    "At this point, the program is paused, we can now inspect the memory contents by looking at `/proc/1255/maps`. This is a kernel supplied virtual file that shows the exact memory layout of the program. It actually summarises each memory section, so it's useful for understanding how memory is layed out without necessarily being able to view a particular byte address.\n",
    "\n",
    "```\n",
    "$ cat /proc/1255/maps # you can also use `watch -d cat /proc/1255/maps` to get updates\n",
    "00400000-00401000 r-xp 00000000 fc:00 1457150                            /home/vagrant/c_tests/memory_layout\n",
    "00600000-00601000 r--p 00000000 fc:00 1457150                            /home/vagrant/c_tests/memory_layout\n",
    "00601000-00602000 rw-p 00001000 fc:00 1457150                            /home/vagrant/c_tests/memory_layout\n",
    "7f849c31b000-7f849c4d6000 r-xp 00000000 fc:00 1579071                    /lib/x86_64-linux-gnu/libc-2.19.so\n",
    "7f849c4d6000-7f849c6d6000 ---p 001bb000 fc:00 1579071                    /lib/x86_64-linux-gnu/libc-2.19.so\n",
    "7f849c6d6000-7f849c6da000 r--p 001bb000 fc:00 1579071                    /lib/x86_64-linux-gnu/libc-2.19.so\n",
    "7f849c6da000-7f849c6dc000 rw-p 001bf000 fc:00 1579071                    /lib/x86_64-linux-gnu/libc-2.19.so\n",
    "7f849c6dc000-7f849c6e1000 rw-p 00000000 00:00 0\n",
    "7f849c6e1000-7f849c6fa000 r-xp 00000000 fc:00 1579084                    /lib/x86_64-linux-gnu/libpthread-2.19.so\n",
    "7f849c6fa000-7f849c8f9000 ---p 00019000 fc:00 1579084                    /lib/x86_64-linux-gnu/libpthread-2.19.so\n",
    "7f849c8f9000-7f849c8fa000 r--p 00018000 fc:00 1579084                    /lib/x86_64-linux-gnu/libpthread-2.19.so\n",
    "7f849c8fa000-7f849c8fb000 rw-p 00019000 fc:00 1579084                    /lib/x86_64-linux-gnu/libpthread-2.19.so\n",
    "7f849c8fb000-7f849c8ff000 rw-p 00000000 00:00 0\n",
    "7f849c8ff000-7f849c922000 r-xp 00000000 fc:00 1579072                    /lib/x86_64-linux-gnu/ld-2.19.so\n",
    "7f849cb10000-7f849cb13000 rw-p 00000000 00:00 0\n",
    "7f849cb1d000-7f849cb21000 rw-p 00000000 00:00 0\n",
    "7f849cb21000-7f849cb22000 r--p 00022000 fc:00 1579072                    /lib/x86_64-linux-gnu/ld-2.19.so\n",
    "7f849cb22000-7f849cb23000 rw-p 00023000 fc:00 1579072                    /lib/x86_64-linux-gnu/ld-2.19.so\n",
    "7f849cb23000-7f849cb24000 rw-p 00000000 00:00 0\n",
    "7fffb5d61000-7fffb5d82000 rw-p 00000000 00:00 0                          [stack]\n",
    "7fffb5dfe000-7fffb5e00000 r-xp 00000000 00:00 0                          [vdso]\n",
    "ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]\n",
    "```\n",
    "\n",
    "Each row in /proc/$PID/maps describes a region of contiguous virtual memory in a process. Each row has the following fields:\n",
    "\n",
    "* address - starting and ending address in the region's process address space\n",
    "* perms - describes how pages can be accessed `rwxp` or `rwxs`, where `s` meaning private or shared pages, if a process attempts to access memory not allowed by the permissions, a segmentation fault occurs\n",
    "* offset - if the region was mapped by a file using `mmap`, this is the offset in the file where the mapping begins\n",
    "* dev - if the region was mapped from a file, this is major and minor device number in hex where the file lives, the major number points to a device driver, and the minor number is interpreted by the device driver, or the minor number is the specific device for a device driver, like multiple floppy drives\n",
    "* inode - if the region was mapped from a file, this is the file number\n",
    "* pathname - if the region was mapped from a file, this is the name of the file, there are special regions with names like [heap], [stack] and [vdso], [vdso] stands for virtual dynamic shared object, its used by system calls to switch to kernel mode\n",
    "\n",
    "Some regions don't have any file path or special names in the pathname field, these are anonymous regions. Anonymous regions are created by mmap, but are not attached to any file, they are used for miscellaneous things like shared memory, buffers not on the heap, and the pthread library uses anonymous mapped regions as stacks for new threads.\n",
    "\n",
    "There's no 100% guarantee that contiguous virtual memory means contiguous physical memory. To do this, you would have to use an OS with no virtual memory system. But it's a good chance that contiguous virtual memory does equal contiguous physical memory, at least there's no pointer chasing. Still at the hardware level, there's a special device for virtual to physical memory translation. So it's still very fast.\n",
    "\n",
    "It is quite important to use the `bc` tool because we need to convert between hex and decimal quite often here. We can use it like: `bc <<< 'obase=10; ibase=16; 4010000 - 4000000'`, which essentially does a `4010000 - 4000000` subtraction using hex digits, and then convers the result to base 10 decimal.\n",
    "\n",
    "Just a side note about the major minor numbers. You can use `ls -l /dev | grep 252` or `lsblk | grep 252` to look up devices corresponding to a `major:minor` number. Where `0d252 ~ 0xfc`.\n",
    "\n",
    "This lists all the major and minor number allocations for Linux Device Drivers: http://www.lanana.org/docs/device-list/devices-2.6+.txt\n",
    "\n",
    "It also shows that anything between 240 - 254 is for local/experimental usecase. Also 232 - 239 is unassigned. And 255 is reserved. We can identify that the device in question right now is a device mapper device. So it is using the range reserved for local/experimental use. Major and minor numbers only go up to 255 because it's the largest decimal natural in a single byte. A single byte is: `0b11111111` or `0xFF`. A single hex digit is a nibble. 2 hex digits is a byte.\n",
    "\n",
    "The first thing to realise, is that the memory addresses starts from low to high, but every time you run this program, many of the regions will have different addresses. So that means for some regions, addresses aren't statically allocated. This is actually due to a security feature, by randomising the address space for certain regions, it makes it more difficult for attackers to acquire a particular piece of memory they are interested in. However there are regions that are always fixed, because you need them to be fixed so you know how to load the program. We can see that program data and executable memory is always fixed along with `vsyscall`. It is actually possible to create what people call a \"PIE\" (position independent executable), which actually even makes the program data and executable memory randomised as well, however this is not enabled by default, and it also prevents the program from being compiled statically, forcing it to be linked (https://sourceware.org/ml/binutils/2012-02/msg00249.html). Also \"PIE\" executables incur some performance problems (different kinds of problems on 32 bit vs 64 bit computers). The randomisation of the address for some of the regions is called \"PIC\" (position independent code), and has been enabled by default on Linux for quite some time. For more information, see: http://blog.fpmurphy.com/2008/06/position-independent-executables.html and http://eli.thegreenplace.net/2011/08/25/load-time-relocation-of-shared-libraries\n",
    "\n",
    "It is possible to compile the above program using `gcc -fPIE -pie ./hello.c -o hello`, which produces a \"PIE\" executable. There's some talk on nixpkgs to make compilation to \"PIE\" by default for 64 bit binaries, but 32 bit binaries remain unPIEd due to serious performance problems. See: https://github.com/NixOS/nixpkgs/issues/7220\n",
    "\n",
    "BTW: Wouldn't it be great if we had a tool that examined `/proc/$PID/maps` and gave exact human readable byte sizes instead?\n",
    "\n",
    "So let's go into detail for each region. Remember, this is still at the beginning of the program, where no `malloc` has occurred, so there is no `[heap]` region.\n",
    "\n",
    "```\n",
    "     0 - 400000 - 4194304 B - 4096 KiB ~ 4 MiB - NOT ALLOCATED\n",
    "400000 - 401000 - 4096 B    - 4 KiB\n",
    "600000 - 601000 - 4096 B    - 4 KiB\n",
    "601000 - 602000 - 4096 B    - 4 KiB\n",
    "```\n",
    "\n",
    "This is our initial range of memory. I added an extra component which starts at `0` address and reaches `40 00 00` address. Addresses appear to be left inclusive, right exclusive. But remember that addresses start at 0. Therefore it is legitimate to use `bc <<< 'obase=10;ibase=16 400000 - 0'` to acquire the actual amount of bytes in that range without adding or subtracing 1. In this case, the first unallocated region is roughly 4 MiB. When I say unallocated, I mean it is not represented in the the `/proc/$PID/maps`. This could mean either of 2 things, either the file doesn't show all the allocated memory, or it doesn't consider such memory to be worth showing, or there is really no memory allocated there.\n",
    "\n",
    "We can find out whether there really is memory there, by creating a pointer to the memory address somewhere between `0` and `400000`, and try dereferencing it. This can be done by casting an integer into a pointer. I've tried it before, and it results in a segfault, this means there really is no memory allocated between `0-400000`\n",
    "\n",
    "```c\n",
    "#include <stdio.h>\n",
    "\n",
    "int main () {\n",
    "\n",
    "    // 0x0 is hex literal that defaults to signed integer\n",
    "    // here we are casting it to a void pointer\n",
    "    // and then assigning it to a value declared to be a void pointer\n",
    "    // this is the correct way to create an arbitrary pointer in C\n",
    "    void * addr = (void *) 0x0;\n",
    "\n",
    "    // in order to print out what exists at that pointer, we must dereference the pointer\n",
    "    // but C doesn't know how to handle a value of void type\n",
    "    // which means, we recast the void pointer to a char pointer\n",
    "    // a char is some arbitrary byte, so hopefully it's a printable ASCII value\n",
    "    // actually, we don't need to hope, because we have made printf specifically print the hex representation of the char, therefore it does not need to be a printable ascii value\n",
    "    printf(\"0x%x\\n\", ((char *) addr)[0]); // prints 0x0\n",
    "    printf(\"0x%x\\n\", ((char *) addr)[1]); // prints 0x1\n",
    "    printf(\"0x%x\\n\", ((char *) addr)[2]); // prints 0x2\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "Running the above gives us a simple `segmentation fault`. Thus proving that `/proc/$PID/maps` is giving us the truth, there really is nothing between `0-400000`.\n",
    "\n",
    "The question becomes, why is there this roughly 4 MiB gap? Why not start allocating memory from 0? Well this was just an arbitrary choice by the malloc and linker implementors. They just decided that that on 64 bit ELF executables, the entry point of a non-PIE executable should be `0x400000`, whereas for 32 bit ELF executables, the entry point is `0x08048000`. An interesting fact is that if you produce a position independent executable, the starting address instead changes to `0x0`.\n",
    "\n",
    "See: \n",
    "\n",
    "* http://stackoverflow.com/questions/7187981/whats-the-memory-before-0x08048000-used-for-in-32-bit-machine\n",
    "* http://stackoverflow.com/questions/12488010/why-the-entry-point-address-in-my-executable-is-0x8048330-0x330-being-offset-of\n",
    "* http://stackoverflow.com/questions/14314021/why-linux-gnu-linker-chose-address-0x400000\n",
    "\n",
    "> The entry address is set by the link editor, at the time when it creates the executable. The loader maps the program file at the address(es) specified by the ELF headers before transferring control to the entry address.\n",
    "\n",
    "> The load address is arbitrary, but was standardized back with SYSV for x86. It's different for every architecture. What goes above and below is also arbitrary, and is often taken up by linked in libraries and mmap() regions.\n",
    "\n",
    "What it basically means is that the program executable is loaded into memory before it can start doing things. The entry point of the executable can be acquired by `readelf`. But here's another question, why is the entry point given by `readelf`, not at `0x400000`. It turns out that, that entry point is consider the actual point where the OS should start executing, whereas the position between `0x400000` and entry point is used for the EHDR and PHDR, meaning ELF headers and Program Headers. We'll look into this in detail later.\n",
    "\n",
    "```\n",
    "$ readelf --file-header ./memory_layout | grep 'Entry point address'\n",
    "  Entry point address:               0x400720\n",
    "```\n",
    "\n",
    "Next we have the:\n",
    "\n",
    "```\n",
    "400000 - 401000 - 4096 B    - 4 KiB\n",
    "600000 - 601000 - 4096 B    - 4 KiB\n",
    "601000 - 602000 - 4096 B    - 4 KiB\n",
    "```\n",
    "\n",
    "As you can see, we have 3 sections of memory, each of them 4 KiB, and allocated from `/home/vagrant/c_tests/memory_layout`.\n",
    "\n",
    "What are these sections?\n",
    "\n",
    "The first segment: \"Text Segment\".\n",
    "\n",
    "The second segment: \"Data Segment\".\n",
    "\n",
    "The third segment: \"BSS Segment\".\n",
    "\n",
    "Text segment stores the binary image of the process. The data segment stores static variables initialised by the programmer, for example `static char * foo = \"bar\";`. The BSS segment stores uninitialised static variables, which are filled with zeros, for example `static char * username;`.\n",
    "\n",
    "Our program is so simple right now, that each seemingly fits perfectly into 4 KiB. How can it be so perfect!?\n",
    "\n",
    "Well, the page size of the Linux OS and many other OSes, is set to 4 KiB by default. This means the minimum addressable memory segment is 4 KiB. See: https://en.wikipedia.org/wiki/Page_%28computer_memory%29\n",
    "\n",
    "> A page, memory page, or virtual page is a fixed-length contiguous block of virtual memory, described by a single entry in the page table. It is the smallest unit of data for memory management in a virtual memory operating system.\n",
    "\n",
    "Running `getconf PAGESIZE` shows 4096 bytes.\n",
    "\n",
    "Therefore, this means each segment is probably far smaller than `4096` bytes, but it gets padded up to 4096 bytes.\n",
    "\n",
    "As we shown before, it's possible to create an arbitrary pointer, and print out the value stored that byte. We can now do this for the segments shown above.\n",
    "\n",
    "But hey, we can do better. Rather than just hacking the individual bytes. We can recognise that this data is actually organised as structs.\n",
    "\n",
    "What kind of structs? We can look at the `readelf` source code to uncover the relevant structs. These structs don't appear to be part of the standard C library, so we cannot just include things to get this working. But the code is simple, so we can just copy and paste. See: http://rpm5.org/docs/api/readelf_8h-source.html\n",
    "\n",
    "Check this out:\n",
    "\n",
    "```c\n",
    "// compile with gcc -std=c99 -o elfheaders ./elfheaders.c\n",
    "#include <stdio.h>\n",
    "#include <stdint.h>\n",
    "\n",
    "// from: http://rpm5.org/docs/api/readelf_8h-source.html\n",
    "// here we're only concerned about 64 bit executables, the 32 bit executables have different sized headers\n",
    "\n",
    "typedef uint64_t Elf64_Addr;\n",
    "typedef uint64_t Elf64_Off;\n",
    "typedef uint64_t Elf64_Xword;\n",
    "typedef uint32_t Elf64_Word;\n",
    "typedef uint16_t Elf64_Half;\n",
    "typedef uint8_t  Elf64_Char;\n",
    "\n",
    "#define EI_NIDENT 16\n",
    "\n",
    "// this struct is exactly 64 bytes\n",
    "// this means it goes from 0x400000 - 0x400040\n",
    "typedef struct {\n",
    "    Elf64_Char  e_ident[EI_NIDENT]; // 16 B\n",
    "    Elf64_Half  e_type;             // 2 B\n",
    "    Elf64_Half  e_machine;          // 2 B\n",
    "    Elf64_Word  e_version;          // 4 B\n",
    "    Elf64_Addr  e_entry;            // 8 B\n",
    "    Elf64_Off   e_phoff;            // 8 B\n",
    "    Elf64_Off   e_shoff;            // 8 B\n",
    "    Elf64_Word  e_flags;            // 4 B\n",
    "    Elf64_Half  e_ehsize;           // 2 B\n",
    "    Elf64_Half  e_phentsize;        // 2 B\n",
    "    Elf64_Half  e_phnum;            // 2 B\n",
    "    Elf64_Half  e_shentsize;        // 2 B\n",
    "    Elf64_Half  e_shnum;            // 2 B\n",
    "    Elf64_Half  e_shstrndx;         // 2 B\n",
    "} Elf64_Ehdr;\n",
    "\n",
    "// this struct is exactly 56 bytes\n",
    "// this means it goes from 0x400040 - 0x400078\n",
    "typedef struct {\n",
    "     Elf64_Word  p_type;   // 4 B\n",
    "     Elf64_Word  p_flags;  // 4 B\n",
    "     Elf64_Off   p_offset; // 8 B\n",
    "     Elf64_Addr  p_vaddr;  // 8 B\n",
    "     Elf64_Addr  p_paddr;  // 8 B\n",
    "     Elf64_Xword p_filesz; // 8 B\n",
    "     Elf64_Xword p_memsz;  // 8 B\n",
    "     Elf64_Xword p_align;  // 8 B\n",
    "} Elf64_Phdr;\n",
    "\n",
    "int main(int argc, char *argv[]){\n",
    "\n",
    "    // from examination of objdump and /proc/ID/maps, we can see that this is the first thing loaded into memory\n",
    "    // earliest in the virtual memory address space, for a 64 bit ELF executable\n",
    "    // %lx is required for 64 bit hex, while %x is just for 32 bit hex\n",
    "\n",
    "    Elf64_Ehdr * ehdr_addr = (Elf64_Ehdr *) 0x400000;\n",
    "\n",
    "    printf(\"Magic:                      0x\");\n",
    "    for (unsigned int i = 0; i < EI_NIDENT; ++i) {\n",
    "        printf(\"%x\", ehdr_addr->e_ident[i]);\n",
    "    }\n",
    "    printf(\"\\n\");\n",
    "    printf(\"Type:                       0x%x\\n\", ehdr_addr->e_type);\n",
    "    printf(\"Machine:                    0x%x\\n\", ehdr_addr->e_machine);\n",
    "    printf(\"Version:                    0x%x\\n\", ehdr_addr->e_version);\n",
    "    printf(\"Entry:                      %p\\n\", (void *) ehdr_addr->e_entry);\n",
    "    printf(\"Phdr Offset:                0x%lx\\n\", ehdr_addr->e_phoff); \n",
    "    printf(\"Section Offset:             0x%lx\\n\", ehdr_addr->e_shoff);\n",
    "    printf(\"Flags:                      0x%x\\n\", ehdr_addr->e_flags);\n",
    "    printf(\"ELF Header Size:            0x%x\\n\", ehdr_addr->e_ehsize);\n",
    "    printf(\"Phdr Header Size:           0x%x\\n\", ehdr_addr->e_phentsize);\n",
    "    printf(\"Phdr Entry Count:           0x%x\\n\", ehdr_addr->e_phnum);\n",
    "    printf(\"Section Header Size:        0x%x\\n\", ehdr_addr->e_shentsize);\n",
    "    printf(\"Section Header Count:       0x%x\\n\", ehdr_addr->e_shnum);\n",
    "    printf(\"Section Header Table Index: 0x%x\\n\", ehdr_addr->e_shstrndx);\n",
    "\n",
    "    Elf64_Phdr * phdr_addr = (Elf64_Phdr *) 0x400040;\n",
    "\n",
    "    printf(\"Type:                     %u\\n\", phdr_addr->p_type); // 6 - PT_PHDR - segment type\n",
    "    printf(\"Flags:                    %u\\n\", phdr_addr->p_flags); // 5 - PF_R + PF_X - r-x permissions equal to chmod binary 101\n",
    "    printf(\"Offset:                   0x%lx\\n\", phdr_addr->p_offset); // 0x40 - byte offset from the beginning of the file at which the first segment is located\n",
    "    printf(\"Program Virtual Address:  %p\\n\", (void *) phdr_addr->p_vaddr); // 0x400040 - virtual address at which the first segment is located in memory\n",
    "    printf(\"Program Physical Address: %p\\n\", (void *) phdr_addr->p_paddr); // 0x400040 - physical address at which the first segment is located in memory (irrelevant on Linux)\n",
    "    printf(\"Loaded file size:         0x%lx\\n\", phdr_addr->p_filesz); // 504 - bytes loaded from the file for the PHDR\n",
    "    printf(\"Loaded mem size:          0x%lx\\n\", phdr_addr->p_memsz); // 504 - bytes loaded into memory for the PHDR\n",
    "    printf(\"Alignment:                %lu\\n\", phdr_addr->p_align); // 8 - alignment using modular arithmetic (mod p_vaddr palign)  === (mod p_offset p_align)\n",
    "    \n",
    "    return 0;\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "Running the above gives:\n",
    "\n",
    "```\n",
    "$ ./elfheaders\n",
    "Magic:                      0x7f454c46211000000000\n",
    "Type:                       0x2\n",
    "Machine:                    0x3e\n",
    "Version:                    0x1\n",
    "Entry:                      0x400490\n",
    "Phdr Offset:                0x40\n",
    "Section Offset:             0x1178\n",
    "Flags:                      0x0\n",
    "ELF Header Size:            0x40\n",
    "Phdr Header Size:           0x38\n",
    "Phdr Entry Count:           0x9\n",
    "Section Header Size:        0x40\n",
    "Section Header Count:       0x1e\n",
    "Section Header Table Index: 0x1b\n",
    "Type:                     6\n",
    "Flags:                    5\n",
    "Offset:                   0x40\n",
    "Program Virtual Address:  0x400040\n",
    "Program Physical Address: 0x400040\n",
    "Loaded file size:         0x1f8\n",
    "Loaded mem size:          0x1f8\n",
    "Alignment:                8\n",
    "```\n",
    "\n",
    "Compare the above output with:\n",
    "\n",
    "```\n",
    "$ readelf --file-header ./elfheaders\n",
    "ELF Header:\n",
    "  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00\n",
    "  Class:                             ELF64\n",
    "  Data:                              2's complement, little endian\n",
    "  Version:                           1 (current)\n",
    "  OS/ABI:                            UNIX - System V\n",
    "  ABI Version:                       0\n",
    "  Type:                              EXEC (Executable file)\n",
    "  Machine:                           Advanced Micro Devices X86-64\n",
    "  Version:                           0x1\n",
    "  Entry point address:               0x400490\n",
    "  Start of program headers:          64 (bytes into file)\n",
    "  Start of section headers:          4472 (bytes into file)\n",
    "  Flags:                             0x0\n",
    "  Size of this header:               64 (bytes)\n",
    "  Size of program headers:           56 (bytes)\n",
    "  Number of program headers:         9\n",
    "  Size of section headers:           64 (bytes)\n",
    "  Number of section headers:         30\n",
    "  Section header string table index: 27\n",
    "```\n",
    "\n",
    "We've basically just written our own little `readelf` program.\n",
    "\n",
    "So it's starting to make sense as to what's actually located at the start of `0x400000 - 0x401000`, it's all the ELF executable headers that tell the OS how to use this program, and all the other interesting metadata. Specifically this is about what's located between the program's actual entry point (for `./elfheader`: `0x400490` and for `./memory_layout`: `0x400720`) and the actual start of memory at `0x400000`. There are more program headers to study, but this is enough for now. See: http://www.ouah.org/RevEng/x430.htm \n",
    "\n",
    "But where does the OS actually get this data from? It has to acquire this data, before it put into memory. Well it turns out the answer is very simple. It's just the file itself.\n",
    "\n",
    "Let's use `hexdump` to view the actual binary contents of the file, and also later use `objdump` to disassemble it to assembly to make some sense of the machine code.\n",
    "\n",
    "Obviously the starting memory address, wouldn't be starting file address. So instead of `0x400000`, files should most likely start at `0x0`.\n",
    "\n",
    "```\n",
    "$ hexdump -C -s 0x0 ./memory_layout # the -s option is just for offset, it's actually redundant here, but will be useful later\n",
    "00000000  7f 45 4c 46 02 01 01 00  00 00 00 00 00 00 00 00  |.ELF............|\n",
    "00000010  02 00 3e 00 01 00 00 00  20 07 40 00 00 00 00 00  |..>..... .@.....|\n",
    "00000020  40 00 00 00 00 00 00 00  a8 11 00 00 00 00 00 00  |@...............|\n",
    "00000030  00 00 00 00 40 00 38 00  09 00 40 00 1e 00 1b 00  |....@.8...@.....|\n",
    "00000040  06 00 00 00 05 00 00 00  40 00 00 00 00 00 00 00  |........@.......|\n",
    "00000050  40 00 40 00 00 00 00 00  40 00 40 00 00 00 00 00  |@.@.....@.@.....|\n",
    "00000060  f8 01 00 00 00 00 00 00  f8 01 00 00 00 00 00 00  |................|\n",
    "00000070  08 00 00 00 00 00 00 00  03 00 00 00 04 00 00 00  |................|\n",
    "00000080  38 02 00 00 00 00 00 00  38 02 40 00 00 00 00 00  |8.......8.@.....|\n",
    "00000090  38 02 40 00 00 00 00 00  1c 00 00 00 00 00 00 00  |8.@.............|\n",
    "000000a0  1c 00 00 00 00 00 00 00  01 00 00 00 00 00 00 00  |................|\n",
    "000000b0  01 00 00 00 05 00 00 00  00 00 00 00 00 00 00 00  |................|\n",
    "000000c0  00 00 40 00 00 00 00 00  00 00 40 00 00 00 00 00  |..@.......@.....|\n",
    "000000d0  34 0c 00 00 00 00 00 00  34 0c 00 00 00 00 00 00  |4.......4.......|\n",
    "000000e0  00 00 20 00 00 00 00 00  01 00 00 00 06 00 00 00  |.. .............|\n",
    "000000f0  00 0e 00 00 00 00 00 00  00 0e 60 00 00 00 00 00  |..........`.....|\n",
    "00000100  00 0e 60 00 00 00 00 00  78 02 00 00 00 00 00 00  |..`.....x.......|\n",
    "00000110  80 02 00 00 00 00 00 00  00 00 20 00 00 00 00 00  |.......... .....|\n",
    "00000120  02 00 00 00 06 00 00 00  18 0e 00 00 00 00 00 00  |................|\n",
    "00000130  18 0e 60 00 00 00 00 00  18 0e 60 00 00 00 00 00  |..`.......`.....|\n",
    "00000140  e0 01 00 00 00 00 00 00  e0 01 00 00 00 00 00 00  |................|\n",
    "00000150  08 00 00 00 00 00 00 00  04 00 00 00 04 00 00 00  |................|\n",
    "00000160  54 02 00 00 00 00 00 00  54 02 40 00 00 00 00 00  |T.......T.@.....|\n",
    "00000170  54 02 40 00 00 00 00 00  44 00 00 00 00 00 00 00  |T.@.....D.......|\n",
    "00000180  44 00 00 00 00 00 00 00  04 00 00 00 00 00 00 00  |D...............|\n",
    "00000190  50 e5 74 64 04 00 00 00  e0 0a 00 00 00 00 00 00  |P.td............|\n",
    "000001a0  e0 0a 40 00 00 00 00 00  e0 0a 40 00 00 00 00 00  |..@.......@.....|\n",
    "000001b0  3c 00 00 00 00 00 00 00  3c 00 00 00 00 00 00 00  |<.......<.......|\n",
    "000001c0  04 00 00 00 00 00 00 00  51 e5 74 64 06 00 00 00  |........Q.td....|\n",
    "000001d0  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00  |................|\n",
    "*\n",
    "...\n",
    "```\n",
    "\n",
    "It's quite a long piece of text so piping it into `less` is a good idea. Note that `*` means \"same as above line\".\n",
    "\n",
    "Firstly checkout the first 16 bytes: `7f 45 4c 46 02 01 01 00  00 00 00 00 00 00 00 00`.\n",
    "\n",
    "Note how this is the same as the magic bytes shown by `readelf`:\n",
    "\n",
    "```\n",
    "$ readelf -h ./memory_layout | grep Magic\n",
    "  Magic:   7f 45 4c 46 02 01 01 00 00 00 00 00 00 00 00 00\n",
    "```\n",
    "\n",
    "So it turns out that one can say that `0x400000` for a non-PIE 64 bit ELF executable compiled by `gcc` on Linux is the exact the same starting point as the `0x0` for the actual executable file itself.\n",
    "\n",
    "The file headers are indeed being loaded into memory. But can we tell if the entire file is loaded into memory or not? Let's first check the file size.\n",
    "\n",
    "```\n",
    "$ stat memory_layout | grep Size\n",
    "  Size: 8932            Blocks: 24         IO Block: 4096   regular file\n",
    "```\n",
    "\n",
    "Shows that the file is 8932 bytes, which is roughly 8.7 KiB.\n",
    "\n",
    "Our memory layout showed that at most 4 KiB + 4 KiB + 4 KiB was mapped from the `memory_layout` executable file.\n",
    "\n",
    "There's plenty of space, and certainly enough to fit the entire contents of the file.\n",
    "\n",
    "But we can prove this by iterating over the entire memory contents, and check the relevant offsets in memory to see if they match the contents on file.\n",
    "\n",
    "To do this, we need to investigate `/proc/$PID/mem`. However it's not a normal file that you can cat from, but you must do some interesting syscalls to get some output from it. There's no standard unix tool to read from it, instead we would need to write a C program read it. There's an example program here: http://unix.stackexchange.com/a/251769/56970\n",
    "\n",
    "Fortunately, there's a thing called `gdb`, and we can use `gcore` to dump a process's memory contents onto disk. It require super user privileges, because we are essentially accessing a process's memory, and memory is usually meant to be isolated!\n",
    "\n",
    "```\n",
    "$ sudo gcore 1255\n",
    "\n",
    "Program received signal SIGTTIN, Stopped (tty input).\n",
    "[Thread debugging using libthread_db enabled]\n",
    "Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n",
    "0x00007f849c407350 in read () from /lib/x86_64-linux-gnu/libc.so.6\n",
    "Saved corefile core.1255\n",
    "\n",
    "[1]+  Stopped                 ./memory_layout\n",
    "```\n",
    "\n",
    "This produces a file called `core.1255`. This file is the memory dump, so to view it, we must use `hexedit`. \n",
    "\n",
    "```\n",
    "$ hexdump -C ./core.1255 | less\n",
    "```\n",
    "\n",
    "Now that we have the entire memory contents. Lets try and compare it with the executable file itself. Before we can do that, we must turn our binary file into printable ASCII. Essentially ASCII armoring the binary programs. The `xxd` is better for this purpose because `hexdump` gives us `|` characters which can give confusing output when we use `diff`.\n",
    "\n",
    "```\n",
    "$ xxd ./core.1255 > ./memory.hex\n",
    "$ xxd ./memory_layout > ./file.hex\n",
    "```\n",
    "\n",
    "Immediately we can see that 2 sizes are not the same. The `./memory.hex` ~ 1.1 MiB is far larger than the `./file.hex` ~ 37 KiB. This is because the memory dump also contains all the shared libraries and the anonymously mapped regions. But we are not expecting them to be the same, just whether the entire file itself exists in memory.\n",
    "\n",
    "Both files can now be compared using `diff`.\n",
    "\n",
    "```\n",
    "$ diff --side-by-side ./file.hex ./memory.hex # try piping into less\n",
    "0000000: 7f45 4c46 0201 0100 0000 0000 0000 0000  .ELF.......   0000000: 7f45 4c46 0201 0100 0000 0000 0000 0000  .ELF.......\n",
    "0000010: 0200 3e00 0100 0000 2007 4000 0000 0000  ..>..... .@ | 0000010: 0400 3e00 0100 0000 0000 0000 0000 0000  ..>........\n",
    "0000020: 4000 0000 0000 0000 a811 0000 0000 0000  @.......... | 0000020: 4000 0000 0000 0000 1022 0400 0000 0000  @........\".\n",
    "0000030: 0000 0000 4000 3800 0900 4000 1e00 1b00  ....@.8...@ | 0000030: 0000 0000 4000 3800 1500 4000 1700 1600  ....@.8...@\n",
    "0000040: 0600 0000 0500 0000 4000 0000 0000 0000  ........@.. | 0000040: 0400 0000 0400 0000 d804 0000 0000 0000  ...........\n",
    "0000050: 4000 4000 0000 0000 4000 4000 0000 0000  @.@.....@.@ | 0000050: 0000 0000 0000 0000 0000 0000 0000 0000  ...........\n",
    "0000060: f801 0000 0000 0000 f801 0000 0000 0000  ........... | 0000060: 200d 0000 0000 0000 0000 0000 0000 0000   ..........\n",
    "0000070: 0800 0000 0000 0000 0300 0000 0400 0000  ........... | 0000070: 0100 0000 0000 0000 0100 0000 0500 0000  ...........\n",
    "0000080: 3802 0000 0000 0000 3802 4000 0000 0000  8.......8.@ | 0000080: f811 0000 0000 0000 0000 4000 0000 0000  ..........@\n",
    "0000090: 3802 4000 0000 0000 1c00 0000 0000 0000  8.@........ | 0000090: 0000 0000 0000 0000 0000 0000 0000 0000  ...........\n",
    "00000a0: 1c00 0000 0000 0000 0100 0000 0000 0000  ........... | 00000a0: 0010 0000 0000 0000 0100 0000 0000 0000  ...........\n",
    "00000b0: 0100 0000 0500 0000 0000 0000 0000 0000  ........... | 00000b0: 0100 0000 0400 0000 f811 0000 0000 0000  ...........\n",
    "00000c0: 0000 4000 0000 0000 0000 4000 0000 0000  ..@.......@ | 00000c0: 0000 6000 0000 0000 0000 0000 0000 0000  ..`........\n",
    "00000d0: 340c 0000 0000 0000 340c 0000 0000 0000  4.......4.. | 00000d0: 0010 0000 0000 0000 0010 0000 0000 0000  ...........\n",
    "00000e0: 0000 2000 0000 0000 0100 0000 0600 0000  .. ........ | 00000e0: 0100 0000 0000 0000 0100 0000 0600 0000  ...........\n",
    "00000f0: 000e 0000 0000 0000 000e 6000 0000 0000  ..........` | 00000f0: f821 0000 0000 0000 0010 6000 0000 0000  .!........`\n",
    "0000100: 000e 6000 0000 0000 7802 0000 0000 0000  ..`.....x.. | 0000100: 0000 0000 0000 0000 0010 0000 0000 0000  ...........\n",
    "0000110: 8002 0000 0000 0000 0000 2000 0000 0000  ..........  | 0000110: 0010 0000 0000 0000 0100 0000 0000 0000  ...........\n",
    "0000120: 0200 0000 0600 0000 180e 0000 0000 0000  ........... | 0000120: 0100 0000 0500 0000 f831 0000 0000 0000  .........1.\n",
    "0000130: 180e 6000 0000 0000 180e 6000 0000 0000  ..`.......` | 0000130: 00b0 319c 847f 0000 0000 0000 0000 0000  ..1........\n",
    "0000140: e001 0000 0000 0000 e001 0000 0000 0000  ........... | 0000140: 0000 0000 0000 0000 00b0 1b00 0000 0000  ...........\n",
    "0000150: 0800 0000 0000 0000 0400 0000 0400 0000  ........... | 0000150: 0100 0000 0000 0000 0100 0000 0400 0000  ...........\n",
    "0000160: 5402 0000 0000 0000 5402 4000 0000 0000  T.......T.@ | 0000160: f831 0000 0000 0000 0060 6d9c 847f 0000  .1.......`m\n",
    "0000170: 5402 4000 0000 0000 4400 0000 0000 0000  T.@.....D.. | 0000170: 0000 0000 0000 0000 0040 0000 0000 0000  .........@.\n",
    "0000180: 4400 0000 0000 0000 0400 0000 0000 0000  D.......... | 0000180: 0040 0000 0000 0000 0100 0000 0000 0000  .@.........\n",
    "0000190: 50e5 7464 0400 0000 e00a 0000 0000 0000  P.td....... | 0000190: 0100 0000 0600 0000 f871 0000 0000 0000  .........q.\n",
    "00001a0: e00a 4000 0000 0000 e00a 4000 0000 0000  ..@.......@ | 00001a0: 00a0 6d9c 847f 0000 0000 0000 0000 0000  ..m........\n",
    "00001b0: 3c00 0000 0000 0000 3c00 0000 0000 0000  <.......<.. | 00001b0: 0020 0000 0000 0000 0020 0000 0000 0000  . ....... .\n",
    "00001c0: 0400 0000 0000 0000 51e5 7464 0600 0000  ........Q.t | 00001c0: 0100 0000 0000 0000 0100 0000 0600 0000  ...........\n",
    "00001d0: 0000 0000 0000 0000 0000 0000 0000 0000  ........... | 00001d0: f891 0000 0000 0000 00c0 6d9c 847f 0000  ..........m\n",
    "00001e0: 0000 0000 0000 0000 0000 0000 0000 0000  ........... | 00001e0: 0000 0000 0000 0000 0050 0000 0000 0000  .........P.\n",
    "00001f0: 0000 0000 0000 0000 1000 0000 0000 0000  ........... | 00001f0: 0050 0000 0000 0000 0100 0000 0000 0000  .P.........\n",
    "0000200: 52e5 7464 0400 0000 000e 0000 0000 0000  R.td....... | 0000200: 0100 0000 0500 0000 f8e1 0000 0000 0000  ...........\n",
    "0000210: 000e 6000 0000 0000 000e 6000 0000 0000  ..`.......` | 0000210: 0010 6e9c 847f 0000 0000 0000 0000 0000  ..n........\n",
    "0000220: 0002 0000 0000 0000 0002 0000 0000 0000  ........... | 0000220: 0000 0000 0000 0000 0090 0100 0000 0000  ...........\n",
    "0000230: 0100 0000 0000 0000 2f6c 6962 3634 2f6c  ......../li | 0000230: 0100 0000 0000 0000 0100 0000 0400 0000  ...........\n",
    "...\n",
    "```\n",
    "\n",
    "What this shows us that even though there are some similiarities between the memory contents and the file contents, they are not exactly the same. In fact we see a deviation between the 2 dumps starting from 17 bytes, which is just pass the ELF magic bytes.\n",
    "\n",
    "This shows that even though there's a mapping from the file to memory, it isn't exactly the same bytes. Either that, or somewhere in the dumping and hex translation, bytes got changed. It's hard to tell at this point in time.\n",
    "\n",
    "Anyway moving on, we can alo use `objdump` to disassemble the executable file to see the actual assembly instructions that exist at the file. One thing to note, is that `objdump` uses the program's virtual memory address, as if were to be executed. It's not using the actual address at the file. Since we know the memory regions from `/proc/$PID/maps`, we can inspect the first `400000 - 401000` region.\n",
    "\n",
    "```\n",
    "$ objdump --disassemble-all --start-address=0x000000 --stop-address=0x401000 ./memory_layout # use less of course\n",
    "\n",
    "./memory_layout:     file format elf64-x86-64\n",
    "\n",
    "\n",
    "Disassembly of section .interp:\n",
    "\n",
    "0000000000400238 <.interp>:\n",
    "  400238:       2f                      (bad)\n",
    "  400239:       6c                      insb   (%dx),%es:(%rdi)\n",
    "  40023a:       69 62 36 34 2f 6c 64    imul   $0x646c2f34,0x36(%rdx),%esp\n",
    "  400241:       2d 6c 69 6e 75          sub    $0x756e696c,%eax\n",
    "  400246:       78 2d                   js     400275 <_init-0x3d3>\n",
    "  400248:       78 38                   js     400282 <_init-0x3c6>\n",
    "  40024a:       36                      ss\n",
    "  40024b:       2d 36 34 2e 73          sub    $0x732e3436,%eax\n",
    "  400250:       6f                      outsl  %ds:(%rsi),(%dx)\n",
    "  400251:       2e 32 00                xor    %cs:(%rax),%al\n",
    "\n",
    "Disassembly of section .note.ABI-tag:\n",
    "\n",
    "0000000000400254 <.note.ABI-tag>:\n",
    "  400254:       04 00                   add    $0x0,%al\n",
    "  400256:       00 00                   add    %al,(%rax)\n",
    "  400258:       10 00                   adc    %al,(%rax)\n",
    "  40025a:       00 00                   add    %al,(%rax)\n",
    "  40025c:       01 00                   add    %eax,(%rax)\n",
    "  40025e:       00 00                   add    %al,(%rax)\n",
    "  400260:       47                      rex.RXB\n",
    "  400261:       4e 55                   rex.WRX push %rbp\n",
    "  400263:       00 00                   add    %al,(%rax)\n",
    "  400265:       00 00                   add    %al,(%rax)\n",
    "  400267:       00 02                   add    %al,(%rdx)\n",
    "  400269:       00 00                   add    %al,(%rax)\n",
    "...\n",
    "```\n",
    "\n",
    "Unlike `gcore` or manually dereferencing arbitrary pointers, we can see that `objdump` cannot or will not show us memory contents from `400000 - 400238`. Instead it starts showing from `400238`. This is because the stuff from `400000 - 400238` are not assembly instructions, they are just metadata, so `objdump` doesn't bother with them as it's designed to dump assembly code. Another thing to understand is that the elipsis `        ...` (not shown in the above example) (not to be confused with my own `...` meaning the output is an excerpt) mean null bytes. The `objdump` shows a byte by byte machine code with its decompiled equivalent assembly instruction. This is a disassembler, so the output assembly is not exactly what a human would write, because there can be optimisation, and lots of semantic information is thrown away. It's important to note that the hex addresses on the right represent the starting byte address, if there are multiple hex byte digits on right, that means they are joined up as a single assembly instruction. So the gap between `400251 - 400254` is represented by the 3 hex bytes at `2e 32 00`.\n",
    "\n",
    "Let's jump to somewhere interesting, such as the actual \"entry point\" `0x400720` as reported by `readelf --file-header ./memory_layout`.\n",
    "\n",
    "```\n",
    "$ objdump --disassemble-all --start-address=0x000000 --stop-address=0x401000 ./memory_layout | less +/400720\n",
    "...\n",
    "Disassembly of section .text:\n",
    "\n",
    "0000000000400720 <_start>:\n",
    "  400720:       31 ed                   xor    %ebp,%ebp\n",
    "  400722:       49 89 d1                mov    %rdx,%r9\n",
    "  400725:       5e                      pop    %rsi\n",
    "  400726:       48 89 e2                mov    %rsp,%rdx\n",
    "  400729:       48 83 e4 f0             and    $0xfffffffffffffff0,%rsp\n",
    "  40072d:       50                      push   %rax\n",
    "  40072e:       54                      push   %rsp\n",
    "  40072f:       49 c7 c0 a0 09 40 00    mov    $0x4009a0,%r8\n",
    "  400736:       48 c7 c1 30 09 40 00    mov    $0x400930,%rcx\n",
    "  40073d:       48 c7 c7 62 08 40 00    mov    $0x400862,%rdi\n",
    "  400744:       e8 87 ff ff ff          callq  4006d0 <__libc_start_main@plt>\n",
    "  400749:       f4                      hlt\n",
    "  40074a:       66 0f 1f 44 00 00       nopw   0x0(%rax,%rax,1)\n",
    "...\n",
    "```\n",
    "\n",
    "Scrolling a bit up, we see that `objdump` reports this as the actual `.text` section, and at `400720`, this is the entry point of the program. What we have here, is the real first \"procedure\" that is executed by the CPU, the function behind the `main` function. And I think you get to play with this directly in C when you eschew the runtime library to produce a stand alone C executable. The assembly here is x86 64 bit assembly (https://en.wikipedia.org/wiki/X86_assembly_language), which I guess is meant to run on backwards compatible Intel/AMD 64 bit processors. I don't know anymore about this particular assembly, so we'll have to study it later in http://www.cs.virginia.edu/~evans/cs216/guides/x86.html\n",
    "\n",
    "What about our other 2 sections (we can see that there's a skip of `401000 - 600000`, this can also just be an arbitrary choice from the linker implementation):\n",
    "\n",
    "```\n",
    "600000 - 601000 - 4096 B    - 4 KiB\n",
    "601000 - 602000 - 4096 B    - 4 KiB\n",
    "```\n",
    "\n",
    "```\n",
    "$ objdump --disassemble-all --start-address=0x600000 --stop-address=0x602000 ./memory_layout | less\n",
    "```\n",
    "\n",
    "There isn't much to talk about right now. It seems that `0x600000` contains more data and assembly. But the actual addresses for `.data` and `.bss` appears to be:\n",
    "\n",
    "```\n",
    "Disassembly of section .data:\n",
    "\n",
    "0000000000601068 <__data_start>:\n",
    "        ...\n",
    "\n",
    "0000000000601070 <__dso_handle>:\n",
    "        ...\n",
    "\n",
    "Disassembly of section .bss:\n",
    "\n",
    "0000000000601078 <__bss_start>:\n",
    "        ...\n",
    "```\n",
    "\n",
    "It turns out we don't have anything in `.data` and `.bss`. This is because we do not have any static variables in our `./memory_layout.c` program!\n",
    "\n",
    "To recap, our initial understanding of memory layout was:\n",
    "\n",
    "```\n",
    "0\n",
    "Program Text (.text)\n",
    "Initialised Data (.data)\n",
    "Uninitialised Data (.bss)\n",
    "Heap\n",
    "    |\n",
    "    v\n",
    "Memory Mapped Region for Shared Libraries or Anything Else\n",
    "    ^\n",
    "    |\n",
    "User Stack\n",
    "```\n",
    "\n",
    "Now we realise that it's actually:\n",
    "\n",
    "```\n",
    "0\n",
    "Nothing here, because it was just an arbitrary choice by the linker\n",
    "ELF and Program and Section Headers - 0x400000 on 64 bit\n",
    "Program Text (.text) - Entry Point as Reported by readelf\n",
    "Nothing Here either\n",
    "Some unknown assembly and data - 0x600000\n",
    "Initialised Data (.data) - 0x601068\n",
    "Uninitialised Data (.bss) - 0x601078\n",
    "Heap\n",
    "    |\n",
    "    v\n",
    "Memory Mapped Region for Shared Libraries or Anything Else\n",
    "    ^\n",
    "    |\n",
    "User Stack\n",
    "```\n",
    "\n",
    "Ok let's move on. After our executable file memory, we have a huge jump from `601000 -  7f849c31b000`.\n",
    "\n",
    "```\n",
    "00400000-00401000 r-xp 00000000 fc:00 1457150                            /home/vagrant/c_tests/memory_layout\n",
    "00600000-00601000 r--p 00000000 fc:00 1457150                            /home/vagrant/c_tests/memory_layout\n",
    "00601000-00602000 rw-p 00001000 fc:00 1457150                            /home/vagrant/c_tests/memory_layout\n",
    "\n",
    "... WHAT IS GOING ON HERE? ...\n",
    "\n",
    "7f849c31b000-7f849c4d6000 r-xp 00000000 fc:00 1579071                    /lib/x86_64-linux-gnu/libc-2.19.so\n",
    "7f849c4d6000-7f849c6d6000 ---p 001bb000 fc:00 1579071                    /lib/x86_64-linux-gnu/libc-2.19.so\n",
    "7f849c6d6000-7f849c6da000 r--p 001bb000 fc:00 1579071                    /lib/x86_64-linux-gnu/libc-2.19.so\n",
    "7f849c6da000-7f849c6dc000 rw-p 001bf000 fc:00 1579071                    /lib/x86_64-linux-gnu/libc-2.19.so\n",
    "7f849c6dc000-7f849c6e1000 rw-p 00000000 00:00 0\n",
    "7f849c6e1000-7f849c6fa000 r-xp 00000000 fc:00 1579084                    /lib/x86_64-linux-gnu/libpthread-2.19.so\n",
    "7f849c6fa000-7f849c8f9000 ---p 00019000 fc:00 1579084                    /lib/x86_64-linux-gnu/libpthread-2.19.so\n",
    "7f849c8f9000-7f849c8fa000 r--p 00018000 fc:00 1579084                    /lib/x86_64-linux-gnu/libpthread-2.19.so\n",
    "7f849c8fa000-7f849c8fb000 rw-p 00019000 fc:00 1579084                    /lib/x86_64-linux-gnu/libpthread-2.19.so\n",
    "7f849c8fb000-7f849c8ff000 rw-p 00000000 00:00 0\n",
    "7f849c8ff000-7f849c922000 r-xp 00000000 fc:00 1579072                    /lib/x86_64-linux-gnu/ld-2.19.so\n",
    "7f849cb10000-7f849cb13000 rw-p 00000000 00:00 0\n",
    "7f849cb1d000-7f849cb21000 rw-p 00000000 00:00 0\n",
    "7f849cb21000-7f849cb22000 r--p 00022000 fc:00 1579072                    /lib/x86_64-linux-gnu/ld-2.19.so\n",
    "7f849cb22000-7f849cb23000 rw-p 00023000 fc:00 1579072                    /lib/x86_64-linux-gnu/ld-2.19.so\n",
    "7f849cb23000-7f849cb24000 rw-p 00000000 00:00 0\n",
    "7fffb5d61000-7fffb5d82000 rw-p 00000000 00:00 0                          [stack]\n",
    "7fffb5dfe000-7fffb5e00000 r-xp 00000000 00:00 0                          [vdso]\n",
    "ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]\n",
    "```\n",
    "\n",
    "That is roughly a massive step of 127 Tebibytes. Why such a large jump in the address space? Well this is where the malloc implementation comes in. This documentation https://github.com/torvalds/linux/blob/master/Documentation/x86/x86_64/mm.txt shows how the memory is structured in this way:\n",
    "\n",
    "> ```\n",
    "> Virtual memory map with 4 level page tables:\n",
    "> \n",
    "> 0000000000000000 - 00007fffffffffff (=47 bits) user space, different per mm hole caused by [48:63] sign extension\n",
    "> ffff800000000000 - ffff87ffffffffff (=43 bits) guard hole, reserved for hypervisor\n",
    "> ffff880000000000 - ffffc7ffffffffff (=64 TB) direct mapping of all phys. memory\n",
    "> ffffc80000000000 - ffffc8ffffffffff (=40 bits) hole\n",
    "> ffffc90000000000 - ffffe8ffffffffff (=45 bits) vmalloc/ioremap space\n",
    "> ffffe90000000000 - ffffe9ffffffffff (=40 bits) hole\n",
    "> ffffea0000000000 - ffffeaffffffffff (=40 bits) virtual memory map (1TB)\n",
    "> ... unused hole ...\n",
    "> ffffec0000000000 - fffffc0000000000 (=44 bits) kasan shadow memory (16TB)\n",
    "> ... unused hole ...\n",
    "> ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks\n",
    "> ... unused hole ...\n",
    "> ffffffef00000000 - ffffffff00000000 (=64 GB) EFI region mapping space\n",
    "> ... unused hole ...\n",
    "> ffffffff80000000 - ffffffffa0000000 (=512 MB)  kernel text mapping, from phys 0\n",
    "> ffffffffa0000000 - ffffffffff5fffff (=1525 MB) module mapping space\n",
    "> ffffffffff600000 - ffffffffffdfffff (=8 MB) vsyscalls\n",
    "> ffffffffffe00000 - ffffffffffffffff (=2 MB) unused hole\n",
    "> ```\n",
    "\n",
    "What we can see is that, is that Linux's memory map reserves the first `0000000000000000 - 00007fffffffffff` as user space memory. It turns out that 47 bits is enough to reserve roughly 128 TiB. http://unix.stackexchange.com/a/64490/56970\n",
    "\n",
    "Well  if we take look at the first and last of these sections of memory:\n",
    "\n",
    "```\n",
    "7f849c31b000-7f849c4d6000 r-xp 00000000 fc:00 1579071                    /lib/x86_64-linux-gnu/libc-2.19.so\n",
    "...\n",
    "7fffb5dfe000-7fffb5e00000 r-xp 00000000 00:00 0                          [vdso]\n",
    "```\n",
    "\n",
    "It appears that these regions are almost at the very bottom of the 128 TiB range reserved for user space memory. Considering that there's a 127 TiB gap, this basically means that our malloc uses the user space range `0000000000000000 - 00007fffffffffff` from both ends. From the low end, it grows the heap upwards (upwards in address numbers). While at the high end, it grows the stack downwards (downwards in address numbers).\n",
    "\n",
    "At the same time, the stack is actually a fixed section of memory, so it cannot actually grow as much as the heap. On the high end, but lower than the stack, we see lots of memory regions assigned for the shared libraries and anonymous buffers which are probably used by the shared libraries.\n",
    "\n",
    "We can also view the shared libraries that are being used by the executable. This determines which shared libraries will also be loaded into memory at startup. However remember that libraries and code can also be dynamically loaded, and cannot be seen by the linker. By the way `ldd` stands for \"list dynamic dependencies\".\n",
    "\n",
    "```\n",
    "$ ldd ./memory_layout\n",
    "        linux-vdso.so.1 =>  (0x00007fff1a573000)\n",
    "        libpthread.so.0 => /lib/x86_64-linux-gnu/libpthread.so.0 (0x00007f361ab4e000)\n",
    "        libc.so.6 => /lib/x86_64-linux-gnu/libc.so.6 (0x00007f361a788000)\n",
    "        /lib64/ld-linux-x86-64.so.2 (0x00007f361ad7a000)\n",
    "```\n",
    "\n",
    "You'll notice that if you run `ldd` multiple times, each time it prints a different address for the shared libraries. This corresponds to rerunning the program multiple times and checking that `/proc/$PID/maps` also shows different addresses for the shared libraries. This is due to the \"PIE\" position independent code discussed above. Basically every time you use `ldd` it does call the linker, and the linker performs address randomisation. For more information about the reasoning behind address space randomisation, see: [ASLR](https://en.wikipedia.org/wiki/Address_space_layout_randomization). Also you can check whether the kernel has enabled ASLR by running `cat /proc/sys/kernel/randomize_va_space`.\n",
    "\n",
    "We can see that there are in fact 4 shared libraries. The `vdso` library is not loaded from the filesystem, but provided by the OS.\n",
    "\n",
    "Also: `/lib64/ld-linux-x86-64.so.2 => /lib/x86_64-linux-gnu/ld-2.19.so`, it's a symlink.\n",
    "\n",
    "Finally we're on the last few regions:\n",
    "\n",
    "```\n",
    "7fffb5d61000-7fffb5d82000 rw-p 00000000 00:00 0                          [stack]\n",
    "7fffb5dfe000-7fffb5e00000 r-xp 00000000 00:00 0                          [vdso]\n",
    "ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]\n",
    "```\n",
    "\n",
    "Here are the relevant sizes for each region:\n",
    "\n",
    "```\n",
    "7fffb5d61000     - 7fffb5d82000     - [stack]    - 135168 B - 132 KiB \n",
    "7fffb5dfe000     - 7fffb5e00000     - [vdso]     - 8192 B   - 8 KiB\n",
    "ffffffffff600000 - ffffffffff601000 - [vsyscall] - 4096 B   - 4 KiB\n",
    "```\n",
    "\n",
    "Our initial stack size is allocated to 132 KiB. I suspect this can be changed with runtime or compiler flags.\n",
    "\n",
    "So what is `vdso` and `vsyscall`? Both are mechanisms to allow faster syscalls, that is syscalls without context switching between user space and kernel space. The `vsyscall` has now been replaced by `vdso`, but the `vsyscall` is left there for compatibility reasons. The main differences are that:\n",
    "\n",
    "* `vsyscall` - is always fixed to `ffffffffff600000` and maximum size of 8 MiB, even if PIC or PIE is enabled\n",
    "* `vdso` - not fixed, but acts like a shared library, thus its address is subject to ASLR (address space layout randomisation)\n",
    "* `vsyscall` - provides 3 syscalls: `gettimeofday` (`0xffffffffff600000`), `time` (`0xffffffffff600400`), `getcpu` (`0xffffffffff600800`), even though it given the reserved range `ffffffffff600000 - ffffffffffdfffff` of 8 MiB by Linux in 64 bit ELF executables.\n",
    "* `vdso` - provides 4 syscalls: `__vdso_clock_gettime`, `__vdso_getcpu`, `__vdso_gettimeofday` and `__vdso_time`, however more syscalls can be added to `vdso` in the future.\n",
    "\n",
    "For more info about `vdso` and `vsyscall`, see: https://0xax.gitbooks.io/linux-insides/content/SysCall/syscall-3.html\n",
    "\n",
    "It's worth pointing out, that now that we're past the 128 TiB reserved for user space memory, we're now looking at segments of memory provided and managed by the OS. As listed here: https://github.com/torvalds/linux/blob/master/Documentation/x86/x86_64/mm.txt These sections are what we're talking about.\n",
    "\n",
    "```\n",
    "ffff800000000000 - ffff87ffffffffff (=43 bits) guard hole, reserved for hypervisor\n",
    "ffff880000000000 - ffffc7ffffffffff (=64 TB) direct mapping of all phys. memory\n",
    "ffffc80000000000 - ffffc8ffffffffff (=40 bits) hole\n",
    "ffffc90000000000 - ffffe8ffffffffff (=45 bits) vmalloc/ioremap space\n",
    "ffffe90000000000 - ffffe9ffffffffff (=40 bits) hole\n",
    "ffffea0000000000 - ffffeaffffffffff (=40 bits) virtual memory map (1TB)\n",
    "... unused hole ...\n",
    "ffffec0000000000 - fffffc0000000000 (=44 bits) kasan shadow memory (16TB)\n",
    "... unused hole ...\n",
    "ffffff0000000000 - ffffff7fffffffff (=39 bits) %esp fixup stacks\n",
    "... unused hole ...\n",
    "ffffffef00000000 - ffffffff00000000 (=64 GB) EFI region mapping space\n",
    "... unused hole ...\n",
    "ffffffff80000000 - ffffffffa0000000 (=512 MB)  kernel text mapping, from phys 0\n",
    "ffffffffa0000000 - ffffffffff5fffff (=1525 MB) module mapping space\n",
    "ffffffffff600000 - ffffffffffdfffff (=8 MB) vsyscalls\n",
    "ffffffffffe00000 - ffffffffffffffff (=2 MB) unused hole\n",
    "```\n",
    "\n",
    "Out of the above sections, we currently only see the `vsyscall` region. The rest have not appeared yet.\n",
    "\n",
    "Let's now move on with the program, and allocate our first heap. Our `/proc/$PID/maps` is now (note that the addresses have changed because I have reran the program):\n",
    "\n",
    "```\n",
    "$ ./memory_layout\n",
    "Welcome to per thread arena example::1546\n",
    "Before malloc in the main thread\n",
    "\n",
    "After malloc and before free in main thread\n",
    "^Z\n",
    "[1]+  Stopped                 ./memory_layout\n",
    "$ cat /proc/1546/maps\n",
    "00400000-00401000 r-xp 00000000 fc:00 1457150                            /home/vagrant/c_tests/memory_layout\n",
    "00600000-00601000 r--p 00000000 fc:00 1457150                            /home/vagrant/c_tests/memory_layout\n",
    "00601000-00602000 rw-p 00001000 fc:00 1457150                            /home/vagrant/c_tests/memory_layout\n",
    "019a3000-019c4000 rw-p 00000000 00:00 0                                  [heap]\n",
    "...\n",
    "```\n",
    "\n",
    "We now see our first `[heap]` region. It exactly: 135168 B - 132 KiB. (Currently the same as our stack size!) Remember we allocated specifically 1000 bytes: `addr = (char *) malloc(1000);` at the beginning. So how did a 1000 Bytes become 132 Kibibytes? Well as we said before, anything smaller than `MMAP_THRESHOLD` uses `brk` syscall. It appears that the `brk`/`sbrk` is called with a padded size in order to reduce the number of syscalls made and the number of context switches. Most programs most likely require more heap than 1000 Bytes, so the system may as well pad the `brk` call to cache some heap memory, and new `brk` or `mmap` calls for heap increase will only occur once you exhaust the padded heap of 132 KiB. The padding calculation is done with:\n",
    "\n",
    "```\n",
    "  /* Request enough space for nb + pad + overhead */\n",
    "  size = nb + mp_.top_pad + MINSIZE;\n",
    "```\n",
    "\n",
    "Where `mp_.top_pad` is by default set to 128 * 1024 = 128 KiB. We still have 4 KiB difference. But remember our page size `getconf PAGESIZE` gives us 4096, meaning each page is 4 KiB. This means when allocating 1000 Bytes in our program, a full page is allocated being 4 KiB. And 4 KiB + 128 KiB is 132 KiB, which is the size of our heap. This padding is not a padding to a fixed size, but padding that is always added to the amount allocated via `brk`/`sbrk`. This means that 128 KiB by default is always added to how ever much memory you're trying allocate. However this padding only applies to `brk`/`sbrk`, and not `mmap`, remember that the past the `MMAP_THRESHOLD`, `mmap` takes over from `brk`/`sbrk`. Which means the padding will no longer be applied. However I'm not sure whether the `MMAP_THRESHOLD` is checked prior to the padding, or after the padding. It seems like it should be prior to the padding.\n",
    "\n",
    "The padding size can be changed with a call like `mallopt(M_TOP_PAD, 1);`, which changes the `M_TOP_PAD` to be 1 Byte. Mallocing 1000 Bytes now will create only a page of 4 KiB.\n",
    "\n",
    "For more information, see: http://stackoverflow.com/a/23951267/582917\n",
    "\n",
    "Why is the older `brk`/`sbrk` getting replaced by newer `mmap`, when an allocation is equal or greater than the `MMAP_THRESHOLD`? Well the `brk`/`sbrk` calls only allow increasing the size of the heap contiguously. If you're just using `malloc` for small things, all of it should be able to be allocated contiguously in the heap, and when it reaches the heap end, the heap can be extended in size without any problems. But for larger allocatiosn, `mmap` is used, and this heap space does need to be contiguously joined with the `brk`/`sbrk` heap space. So it's more flexible. Memory fragmentation is reduced for small objects in this situation. Also the `mmap` call is more flexible, so that `brk`/`sbrk` can be implemented with `mmap`, whereas `mmap` cannot be implemented with `brk`/`sbrk`. One limitation of `brk`/`sbrk`, is that if the top byte in the `brk`/`sbrk` heap space is not freed, the heap size cannot be reduced.\n",
    "\n",
    "Let's look at a simple program that allocates more than `MMAP_THRESHOLD` (it can also be overridden using `mallopt`):\n",
    "\n",
    "```c\n",
    "#include <stdlib.h>\n",
    "#include <stdio.h>\n",
    "\n",
    "int main () {\n",
    "\n",
    "    printf(\"Look at /proc/%d/maps\\n\", getpid());\n",
    "\n",
    "    // allocate 200 KiB, forcing a mmap instead of brk\n",
    "    char * addr = (char *) malloc(204800);\n",
    "\n",
    "    getchar();\n",
    "\n",
    "    free(addr);\n",
    "\n",
    "    return 0;\n",
    "\n",
    "}\n",
    "```\n",
    "\n",
    "Running the above with `strace` gives us:\n",
    "\n",
    "```\n",
    "$ strace ./mmap\n",
    "...\n",
    "mmap(NULL, 3953344, PROT_READ|PROT_EXEC, MAP_PRIVATE|MAP_DENYWRITE, 3, 0) = 0x7f3b113ce000\n",
    "mprotect(0x7f3b11589000, 2097152, PROT_NONE) = 0\n",
    "mmap(0x7f3b11789000, 24576, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_DENYWRITE, 3, 0x1bb000) = 0x7f3b11789000\n",
    "mmap(0x7f3b1178f000, 17088, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_FIXED|MAP_ANONYMOUS, -1, 0) = 0x7f3b1178f000\n",
    "close(3)                                = 0\n",
    "mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f3b119a7000\n",
    "mmap(NULL, 8192, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f3b119a5000\n",
    "arch_prctl(ARCH_SET_FS, 0x7f3b119a5740) = 0\n",
    "mprotect(0x7f3b11789000, 16384, PROT_READ) = 0\n",
    "mprotect(0x600000, 4096, PROT_READ)     = 0\n",
    "mprotect(0x7f3b119b6000, 4096, PROT_READ) = 0\n",
    "munmap(0x7f3b119a8000, 45778)           = 0\n",
    "getpid()                                = 1604\n",
    "fstat(1, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 0), ...}) = 0\n",
    "mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f3b119b3000\n",
    "write(1, \"Look at /proc/1604/maps\\n\", 24Look at /proc/1604/maps\n",
    ") = 24\n",
    "mmap(NULL, 208896, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f3b11972000\n",
    "fstat(0, {st_mode=S_IFCHR|0620, st_rdev=makedev(136, 0), ...}) = 0\n",
    "mmap(NULL, 4096, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f3b119b2000\n",
    "...\n",
    "```\n",
    "\n",
    "There's many `mmap` calls in the above strace. How do we find the `mmap` that our program called, and not the shared libraries or the linker or other stuff? Well the closest call is this one:\n",
    "\n",
    "```\n",
    "mmap(NULL, 208896, PROT_READ|PROT_WRITE, MAP_PRIVATE|MAP_ANONYMOUS, -1, 0) = 0x7f3b11972000\n",
    "```\n",
    "\n",
    "It's actually 204800 B + 4096 B = 208 896 B. I'm not sure why an extra 4 KiB is added, because 200 KiB is exactly divisible by the page size of 4 KiB. It maybe another feature. One thing to note is there isn't any kind of obvious way we can pinpoint our program's syscalls over some other syscall, but we can look at the context of the call, that is the previous and subsequent lines to find the exact control flow. Using things like `getchar` can also pause the `strace`. Consider that immeidately after mmapping 204800 bytes, there's a `fstat` and another `mmap` call, before finally `getchar` is called. I have no idea where these calls are coming from, so in the future, we should look for some easy way to label the syscalls so we can find them quicker. The `strace` tells us that this memory mapped region is mapped to `0x7f3b11972000`. Looking at the process's `/proc/$PID/maps`:\n",
    "\n",
    "```\n",
    "$ cat /proc/1604/maps\n",
    "00400000-00401000 r-xp 00000000 fc:00 1446413                            /home/vagrant/c_tests/test\n",
    "00600000-00601000 r--p 00000000 fc:00 1446413                            /home/vagrant/c_tests/test\n",
    "00601000-00602000 rw-p 00001000 fc:00 1446413                            /home/vagrant/c_tests/test\n",
    "7f3b113ce000-7f3b11589000 r-xp 00000000 fc:00 1579071                    /lib/x86_64-linux-gnu/libc-2.19.so\n",
    "7f3b11589000-7f3b11789000 ---p 001bb000 fc:00 1579071                    /lib/x86_64-linux-gnu/libc-2.19.so\n",
    "7f3b11789000-7f3b1178d000 r--p 001bb000 fc:00 1579071                    /lib/x86_64-linux-gnu/libc-2.19.so\n",
    "7f3b1178d000-7f3b1178f000 rw-p 001bf000 fc:00 1579071                    /lib/x86_64-linux-gnu/libc-2.19.so\n",
    "7f3b1178f000-7f3b11794000 rw-p 00000000 00:00 0\n",
    "7f3b11794000-7f3b117b7000 r-xp 00000000 fc:00 1579072                    /lib/x86_64-linux-gnu/ld-2.19.so\n",
    "7f3b11972000-7f3b119a8000 rw-p 00000000 00:00 0\n",
    "7f3b119b2000-7f3b119b6000 rw-p 00000000 00:00 0\n",
    "7f3b119b6000-7f3b119b7000 r--p 00022000 fc:00 1579072                    /lib/x86_64-linux-gnu/ld-2.19.so\n",
    "7f3b119b7000-7f3b119b8000 rw-p 00023000 fc:00 1579072                    /lib/x86_64-linux-gnu/ld-2.19.so\n",
    "7f3b119b8000-7f3b119b9000 rw-p 00000000 00:00 0\n",
    "7fff8f747000-7fff8f768000 rw-p 00000000 00:00 0                          [stack]\n",
    "7fff8f7fe000-7fff8f800000 r-xp 00000000 00:00 0                          [vdso]\n",
    "ffffffffff600000-ffffffffff601000 r-xp 00000000 00:00 0                  [vsyscall]\n",
    "```\n",
    "\n",
    "We can find out mmapped heap here:\n",
    "\n",
    "```\n",
    "7f3b11972000-7f3b119a8000 rw-p 00000000 00:00 0\n",
    "```\n",
    "\n",
    "As you can see, when malloc switches to using `mmap`, the regions acquired are not part of the so called `[heap]` region, which is only provided by the `brk`/`sbrk` calls. It's unlabelled! We can also see that this kind of heap is not put in the same area as the `brk`/`sbrk` heap, which we understood to start at the low end and grow upwards in address space. Instead this mmapped heap is located in the same area as the shared libraries, putting it at the high end of the reserved user space address range. However this region as shown by `/proc/$PID/maps` is actually 221184 B - 216 KiB. It is exactly 12 KiB from 208896. Another mystery! Why do we have different byte sizes, even though `mmap` in the `strace` called exactly `208896`?\n",
    "\n",
    "Looking at another `mmap` call also shows that the corresponding region in `/proc/$PID/maps` has a difference of 12 KiB. It is possible that 12 KiB here represents some sort of memory mapping overhead, used by malloc to keep track or understand the type of memory that is available here. Or it could just be extra padding as well. So we can say here that something is consistently adding 12 KiB to whatever we're mmapping, and that there is an extra 4 KiB added to my 200 KiB `malloc`.\n",
    "\n",
    "By the way, there's also a tool called `binwalk` that's really useful for examining firmware images which may include multiple executable files and metadata. Remember you can in fact embed a file into an executable. Which is kind of like how viruses work. I used it to inspect the initrd of NixOS and figure out how it was structured. Combine it with `dd` you can cut and slice and splice binary blobs easily!\n",
    "\n",
    "At this point, we can continue investigating the heap from our original program, and the thread heaps as well. But I'm stopping here for now.\n",
    "\n",
    "To be continued..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

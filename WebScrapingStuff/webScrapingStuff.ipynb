{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "69c9821b-ae3b-4bdc-ae26-548864017b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kpadhikari/GitStuff/KPAdhikari/PythonStuff/WebScrapingStuff\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819364aa-3803-43e9-8b59-e9fe87ac9f4d",
   "metadata": {},
   "source": [
    "Although I had visited this page (https://realpython.com/python-web-scraping-practical-introduction/#a-primer-on-regular-expressions) quite a while ago, and had thought of going through it step by step all the way but got distracted. Today, I am coming back and want to complete it. (2/19/2022)\n",
    "\n",
    "# A Practical Introduction to Web Scraping in Python\n",
    "by David Amos\n",
    "\n",
    "Table of Contents\n",
    "\n",
    "    Scrape and Parse Text From Websites\n",
    "        Your First Web Scraper\n",
    "        Extract Text From HTML With String Methods\n",
    "        A Primer on Regular Expressions\n",
    "        Extract Text From HTML With Regular Expressions\n",
    "        Check Your Understanding\n",
    "    Use an HTML Parser for Web Scraping in Python\n",
    "        Install Beautiful Soup\n",
    "        Create a BeautifulSoup Object\n",
    "        Use a BeautifulSoup Object\n",
    "        Check Your Understanding\n",
    "    Interact With HTML Forms\n",
    "        Install MechanicalSoup\n",
    "        Create a Browser Object\n",
    "        Submit a Form With MechanicalSoup\n",
    "        Check Your Understanding\n",
    "    Interact With Websites in Real Time\n",
    "    Conclusion\n",
    "    Additional Resources\n",
    "    \n",
    "Web scraping is the process of collecting and parsing raw data from the Web, and the Python community has come up with some pretty powerful web scraping tools.\n",
    "\n",
    "The Internet hosts perhaps the greatest source of information—and misinformation—on the planet. Many disciplines, such as data science, business intelligence, and investigative reporting, can benefit enormously from collecting and analyzing data from websites.\n",
    "\n",
    "In this tutorial, you’ll learn how to:\n",
    "\n",
    "* Parse website data using string methods and regular expressions\n",
    "* Parse website data using an HTML parser\n",
    "* Interact with forms and other website components\n",
    "    \n",
    "**Note:** This tutorial is adapted from the chapter “Interacting With the Web” in Python Basics: A Practical Introduction to Python 3.\n",
    "\n",
    "The book uses Python’s built-in IDLE editor to create and edit Python files and interact with the Python shell, so you will see occasional references to IDLE throughout this tutorial. However, you should have no problems running the example code from the editor and environment of your choice.\n",
    "\n",
    "## Scrape and Parse Text From Websites\n",
    "Collecting data from websites using an automated process is known as web scraping. Some websites explicitly forbid users from scraping their data with automated tools like the ones you’ll create in this tutorial. Websites do this for two possible reasons:\n",
    "\n",
    "* The site has a good reason to protect its data. For instance, Google Maps doesn’t let you request too many results too quickly.\n",
    "* Making many repeated requests to a website’s server may use up bandwidth, slowing down the website for other users and potentially overloading the server such that the website stops responding entirely.\n",
    "\n",
    "**Important:** Before using your Python skills for web scraping, you should always check your target website’s acceptable use policy to see if accessing the website with automated tools is a violation of its terms of use. Legally, web scraping against the wishes of a website is very much a gray area.\n",
    "\n",
    "Please be aware that the following techniques may be illegal when used on websites that prohibit web scraping.\n",
    "\n",
    "Let’s start by grabbing all the HTML code from a single web page. You’ll use a page on Real Python that’s been set up for use with this tutorial.\n",
    "\n",
    "## Your First Web Scraper\n",
    "One useful package for web scraping that you can find in Python’s [standard library](https://docs.python.org/3/library/) is `urllib`, which contains tools for working with URLs. In particular, the [urllib.request](https://docs.python.org/3/library/urllib.request.html#module-urllib.request) module contains a function called `urlopen()` that can be used to open a URL within a program.\n",
    "\n",
    "Let's open webpage at the url http://olympus.realpython.org/profiles/aphrodite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ae277e8-01f4-4975-9363-9621e102e738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'http.client.HTTPResponse'>\n",
      "<http.client.HTTPResponse object at 0x7f97a9c2e280>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<http.client.HTTPResponse at 0x7f97a9c2e280>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "\n",
    "# When I used the Following markdown-guide page, I got \"HTTPError: HTTP Error 403: Forbidden\"\n",
    "#    which means - \"the server has determined that you are not allowed access ...\"\n",
    "#url = \"https://ia.net/writer/support/general/markdown-guide\"\n",
    "#url = \"https://www.bbc.com/culture/article/20211215-why-the-tiny-house-movement-is-big\"   #Works but too much stuff\n",
    "url = \"http://olympus.realpython.org/profiles/aphrodite\"\n",
    "\n",
    "page = urlopen(url)\n",
    "print(type(page))\n",
    "print(page)\n",
    "page"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf73518-2733-455a-8b02-b4b4555eafb2",
   "metadata": {},
   "source": [
    "Please remember that the urlopen() returns an HTTPResponse object as we see above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3bd840-0789-4f8d-b9d6-662674428586",
   "metadata": {},
   "source": [
    "To extract the html out of the 'page', first we use the HTTPResponse object's .read() method, which returns a sequence of bytes. Then we use .decode() to decode the bytes to a string using [UTF-8](https://realpython.com/python-encodings-guide/#unicode-vs-utf-8)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9afec3ba-e5c3-42d0-9ffc-97c3a0fbb59c",
   "metadata": {},
   "outputs": [],
   "source": [
    "html_bytes = page.read()\n",
    "html = html_bytes.decode('utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c75697f7-f7da-4db0-9709-5bdc43ee4614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> \n",
      "\n",
      "278 < | h | \n",
      " | < \n",
      "\n",
      "\n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<title>Profile: Aphrodite</title>\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br><br>\n",
      "<img src=\"/static/aphrodite.gif\" />\n",
      "<h2>Name: Aphrodite</h2>\n",
      "<br><br>\n",
      "Favorite animal: Dove\n",
      "<br><br>\n",
      "Favorite color: Red\n",
      "<br><br>\n",
      "Hometown: Mount Olympus\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(html), \"\\n\")\n",
    "print(len(html), html[0], \"|\", html[1], \"|\", html[6], \"|\", html[7], \"\\n\\n\\n\")\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc1ed74-1ea7-4e61-a33d-fd3f1c908f63",
   "metadata": {},
   "source": [
    "Once you have the HTML as text, you can extract information from it in a couple of different ways.\n",
    "\n",
    "## Extract Text From HTML With String Methods\n",
    "One way to extract information from a web page’s HTML is to use [string methods](https://realpython.com/python-strings/#built-in-string-methods). For instance, you can use .find() to search through the text of the HTML for the `<title>` tags and extract the title of the web page.\n",
    "\n",
    "Let’s extract the title of the web page you requested in the previous example. If you know the index of the first character of the title and the first character of the closing `</title>` tag, then you can use a [string slice](https://realpython.com/python-strings/#string-slicing) to extract the title.\n",
    "\n",
    "Since .find() returns the index of the first occurrence of a substring, you can get the index of the opening `<title>` tag by passing the string \"`<title>`\" to `.find()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b47a48e9-66e0-4161-8c0f-2216797e3ce0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title_index = html.find(\"<title>\")\n",
    "title_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33c3ba27-7942-469b-b0e4-bbc2af037668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<\n",
      "h\n",
      "t\n",
      "m\n",
      "l\n",
      ">\n",
      "\n",
      "\n",
      "<\n",
      "h\n",
      "e\n",
      "a\n",
      "d\n",
      ">\n",
      "\n",
      "\n",
      "<\n"
     ]
    }
   ],
   "source": [
    "for i in range(title_index+1):\n",
    "    print(html[i])\n",
    "    \n",
    "#In the result of this loop, look at the gap at i=6 and i=13, they are created because at those \n",
    "#.   positions, we have the newline characters - something equivalent to \"\\n\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ebe482-3fd1-4fdd-9ea6-86351eb54dfb",
   "metadata": {},
   "source": [
    "You don’t want the index of the `<title>` tag, though. You want the index of the title itself. To get the index of the first letter in the title, you can add the length of the string \"`<title>`\" to title_index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "15ae09fc-d969-4d33-a568-a42657ba4144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(len('<title>'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "47b56e5f-7935-4ebb-a5d9-2e2b9c271fb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "P\n"
     ]
    }
   ],
   "source": [
    "start_index = title_index + len('<title>')\n",
    "print(start_index)\n",
    "print(html[start_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "445970c7-a6b2-4e6b-98d5-6c452c6107fe",
   "metadata": {},
   "source": [
    "Now get the index of the closing </title> tag by passing the string \"</title>\" to .find():\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "95cbbb9a-14cc-465a-9b3d-755e497c1af5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "end_index = html.find(\"</title>\")\n",
    "end_index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d80fe651-f78a-42c1-80b5-f5152018561a",
   "metadata": {},
   "source": [
    "Finally, you can extract the title by slicing the html string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5a85e1c1-bebb-46b4-8859-f0d34b7aa8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Profile: Aphrodite'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "title = html[start_index:end_index]\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea3dbe5b-5d0c-4fe1-a8d3-9119c65d6611",
   "metadata": {},
   "source": [
    "Real-world HTML can be much more complicated and far less predictable than the HTML on the Aphrodite profile page. Here’s [another profile page](http://olympus.realpython.org/profiles/poseidon) with some messier HTML that you can scrape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "608f6a92-1d96-458c-94ff-9add2b640fe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: Aphrodite'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trying all together for <h1>\n",
    "html[html.find(\"<h2>\")+len(\"<h2>\"):html.find(\"</h2>\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447eec4a-7bc7-4652-8a0e-3aa23c6d8972",
   "metadata": {},
   "source": [
    "Real-world HTML can be much more complicated and far less predictable than the HTML on the Aphrodite profile page. Here’s [another profile page](http://olympus.realpython.org/profiles/poseidon) with some messier HTML that you can scrape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb5536cc-2344-40e0-9934-39ebae4424fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<title >Profile: Poseidon</title>\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br><br>\n",
      "<img src=\"/static/poseidon.jpg\" />\n",
      "<h2>Name: Poseidon</h2>\n",
      "<br><br>\n",
      "Favorite animal: Dolphin\n",
      "<br><br>\n",
      "Favorite color: Blue\n",
      "<br><br>\n",
      "Hometown: Sea\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<html>\\n<head>\\n<title >Profile: Poseidon</title>\\n</head>\\n<body bgcolor=\"yellow\">\\n<center>\\n<br><br>\\n<img src=\"/static/poseidon.jpg\" />\\n<h2>Name: Poseidon</h2>\\n<br><br>\\nFavorite animal: Dolphin\\n<br><br>\\nFavorite color: Blue\\n<br><br>\\nHometown: Sea\\n</center>\\n</body>\\n</html>\\n'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlopen\n",
    "def getHTML(url):\n",
    "    page = urlopen(url)\n",
    "    return page.read().decode(\"utf-8\")\n",
    "\n",
    "html = getHTML(\"http://olympus.realpython.org/profiles/poseidon\")\n",
    "\n",
    "print(html)\n",
    "html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "640d3cf5-fc65-4748-a990-d693b32f9908",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<head>\\n<title >Profile: Poseidon'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def getFromTag(html, tagStr):\n",
    "    properTag = \"<\" + tagStr + \">\"\n",
    "    start_index = html.find(properTag) + len(properTag)\n",
    "    end_index = html.find(\"</\" + tagStr + \">\")\n",
    "    return html[start_index:end_index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b53637b9-406e-4345-af91-ee84e0d8780b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Name: Poseidon'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFromTag(html,\"h2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "662726f2-c0bc-487b-949f-3cdea55e70e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<title >Profile: Poseidon</title>\\n'"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFromTag(html,\"head\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d445df24-ec62-4c06-a600-b3b35dd8f36f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<head>\\n<title >Profile: Poseidon'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFromTag(html, \"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe651c4-b6f2-40c8-84e1-db5798d8376e",
   "metadata": {},
   "source": [
    "Whoops! There’s a bit of HTML mixed in with the title. Why’s that?\n",
    "\n",
    "The HTML for the `/profiles/poseidon` page looks similar to the `/profiles/aphrodite` page, but there’s a small difference. The opening `<title>` tag has an extra space before the closing angle bracket (`>`), rendering it as `<title >`.\n",
    "\n",
    "`html.find(\"<title>\")` returns -1 because the exact substring \"`<title>`\" doesn’t exist. When -1 is added to len(\"`<title>`\"), which is 7, the start_index variable is assigned the value 6.\n",
    "\n",
    "The character at index 6 of the string html is a newline character (\\n) right before the opening angle bracket (<) of the `<head>` tag. This means that `html[start_index:end_index]` returns all the HTML starting with that newline and ending just before the `</title>` tag.\n",
    "\n",
    "These sorts of problems can occur in countless unpredictable ways. You need a more reliable way to extract text from HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6acf224-1e09-4155-9967-f24d525b556a",
   "metadata": {},
   "source": [
    "## A Primer on Regular Expressions\n",
    "**Regular expressions** — or **regexes** for short — <font color=\"magenta\"><b>are patterns</b> that can be used to search for text within a string</font>. Python supports regular expressions through the standard library’s re module.\n",
    "\n",
    "Note: Regular expressions aren’t particular to Python. They’re a general programming concept and can be used with any programming language.\n",
    "\n",
    "To work with regular expressions, the first thing you need to do is <font color=\"magenta\">import the re module</font>.\n",
    "\n",
    "Regular expressions use special characters called **metacharacters** to denote different patterns. For instance, the <font color=\"magenta\">asterisk character (*) stands for zero or more instances of whatever character comes/stands just before the asterisk</font>.\n",
    "\n",
    "In the following example, you use findall() to find any text within a string that matches a given regular expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b7ef884b-2088-4923-ba86-267e763b617f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ac']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "re.findall(\"ab*c\",\"ac\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb64648-abb9-4351-b391-ecf11464bd87",
   "metadata": {},
   "source": [
    "The first argument of re.findall() is the regular expression that you want to match, and the second argument is the string to test. In the above example, you search for the pattern \"ab*c\" in the string \"ac\".\n",
    "\n",
    "The regular expression \"ab*c\" matches any part of the string that begins with an \"a\", ends with a \"c\", and has zero or more instances of \"b\" between the two<font color=\"magenta\">(kp: and nothing else in between. For example, because of the 'd' in between a and c, it returns an empty list when the string is 'abdc' as we can see below)</font>. re.findall() returns a list of all matches. The string \"ac\" matches this pattern, so it’s returned in the [list](https://realpython.com/python-lists-tuples/).\n",
    "\n",
    "Here’s the same pattern applied to different strings:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a34139b7-9c78-4b4c-a051-def7160b821e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc']\n",
      "['abc']\n",
      "['abc', 'ac']\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(\"ab*c\",\"abcd\"))\n",
    "print(re.findall(\"ab*c\",\"abcc\"))\n",
    "print(re.findall(\"ab*c\",\"abcac\"))\n",
    "print(re.findall(\"ab*c\",\"abdc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18929dc8-1eda-4e3f-9aa5-98ce3d75584f",
   "metadata": {},
   "source": [
    "Notice that if no match is found, then `findall()` returns an empty list.\n",
    "\n",
    "Pattern matching is case sensitive. If you want to match this pattern regardless of the case, then you can pass a third argument with the value `re.IGNORECASE`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "45113278-9529-488a-9f44-9a8622db5569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "['Abc', 'aC']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(\"ab*c\",\"AbcaC\"))\n",
    "print(re.findall(\"ab*c\",\"AbcaC\", re.IGNORECASE))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bdb00f-ca17-4755-9fb7-f5b4ccc919f1",
   "metadata": {},
   "source": [
    "You can use a period (.) to stand for any single character <font color=\"red\">(kp: but not empty or nothing, which is not a character)</font> in a regular expression. <font color=\"magenta\">(kp: Looks to me that it is somewhat equivalent to the single character wild-card '?' in Linux/Unix shell/terminal commands, except it cannot be empty unlike with '?'. Remember, the asterisk '`*`' is the most generic wild card with anything possible in place of `*`)</font> For instance, you could find all the strings that contain the letters \"a\" and \"c\" separated by a single character as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ab520750-f15d-409e-b5a5-2bb2fbd5dd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc']\n",
      "[]\n",
      "[]\n",
      "['acc']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(\"a.c\",\"abc\"))\n",
    "print(re.findall(\"a.c\",\"abbc\"))\n",
    "print(re.findall(\"a.c\",\"ac\"))\n",
    "print(re.findall(\"a.c\",\"acc\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9e943a-d3f6-408c-9aa5-bb2538514d54",
   "metadata": {},
   "source": [
    "The pattern `.*` inside a regular expression stands for any character repeated any number of times. For instance, `\"a.*c\"` can be used to find every substring that starts with \"a\" and ends with \"c\", regardless of which letter — or letters — are in between:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9aa934a2-b572-449e-8e14-31cff077fc02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['abc']\n",
      "['abbc']\n",
      "['ac']\n",
      "['acc']\n",
      "######## kp: However, I find it puzzling that the following two don't give the exact same results\n",
      "      This is because Python’s regular expressions are greedy, meaning they try to find the longest \n",
      "      possible match when characters like * are used, as is also seen with re.sub() example below.\n",
      "      So, we can use the non-greedy matching pattern *?, which works the same way as * except that\n",
      "      it matches the shortest possible string of text.\n",
      "['abc', 'ac']\n",
      "['abcac']\n",
      "['abc', 'ac']\n"
     ]
    }
   ],
   "source": [
    "print(re.findall(\"a.*c\",\"abc\"))\n",
    "print(re.findall(\"a.*c\",\"abbc\"))\n",
    "print(re.findall(\"a.*c\",\"ac\"))\n",
    "print(re.findall(\"a.*c\",\"acc\"))\n",
    "print(\"######## kp: However, I find it puzzling that the following two don't give the exact same results\")\n",
    "print(\"      This is because Python’s regular expressions are greedy, meaning they try to find the longest \")\n",
    "print(\"      possible match when characters like * are used, as is also seen with re.sub() example below.\")\n",
    "print(\"      So, we can use the non-greedy matching pattern *?, which works the same way as * except that\")\n",
    "print(\"      it matches the shortest possible string of text.\")\n",
    "print(re.findall(\"ab*c\",\"abcac\"))\n",
    "print(re.findall(\"a.*c\",\"abcac\"))\n",
    "print(re.findall(\"a.*?c\",\"abcac\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "680db3b3-1884-4697-8bab-36c9554d1343",
   "metadata": {},
   "source": [
    "Often, you use `re.search()` to search for a particular pattern inside a string. This function is somewhat more complicated than `re.findall()` because it returns an object called a `MatchObject` that stores different groups of data. This is because there might be matches inside other matches, and `re.search()` returns every possible result.\n",
    "\n",
    "The details of the MatchObject are irrelevant here. For now, just know that calling `.group()` on a MatchObject will return the first and most inclusive result, which in most cases is just what you want:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "163cecc9-7c67-4cc7-be8f-2c6516d4ed36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ABC'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "match_results = re.search(\"ab*c\", \"ABC\", re.IGNORECASE)\n",
    "match_results.group()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb191b31-2ca2-4810-8b15-b830b583d001",
   "metadata": {},
   "source": [
    "There’s one more function in the re module that’s useful for parsing out text. re.sub(), which is short for substitute, allows you to replace text in a string that matches a regular expression with new text. It behaves sort of like the .replace() string method.\n",
    "\n",
    "The arguments passed to re.sub() are the regular expression, followed by the replacement text, followed by the string. Here’s an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "b36be485-76ef-4010-9e5d-ce7e0a34ea49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Everything is ELEPHANTS.'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Everything is <replaced> if it's in <tags>.\"\n",
    "string = re.sub(\"<.*>\", \"ELEPHANTS\", string)\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c87fe7c-18b3-4cde-ba27-d9e01bac3535",
   "metadata": {},
   "source": [
    "Perhaps that wasn’t quite what you expected to happen.\n",
    "\n",
    "`re.sub()` uses the regular expression \"`<.*>`\" to find and replace everything between the first < and last >, which spans from the beginning of `<replaced>` to the end of `<tags>`. This is because <font color=\"red\">Python’s regular expressions are <b>greedy</b></font>, meaning they try to find the longest possible match when characters like * are used. <font color=\"magenta\">(kp: This explains why I saw a puzzling (initially) result when I tried `re.findall(\"a.*c\",\"abcac\")` above. Which was then \"demystified\" with an '?' added after `*` to the existing regex)</font>\n",
    "\n",
    "Alternatively, you can use the non-greedy matching pattern `*?`, which works the same way as `*` except that it matches the shortest possible string of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "e534143b-fe2f-4864-aedd-7320b7f02ac0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Everything is ELEPHANTS if it's in ELEPHANTS.\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string = \"Everything is <replaced> if it's in <tags>.\"\n",
    "string = re.sub(\"<.*?>\", \"ELEPHANTS\", string)\n",
    "string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5356d46-b03d-4a52-b1b2-85e2655fb419",
   "metadata": {},
   "source": [
    "This time, `re.sub()` finds two matches, `<replaced>` and `<tags>`, and substitutes the string \"ELEPHANTS\" for both matches.\n",
    "\n",
    "## Extract Text From HTML With Regular Expressions\n",
    "Armed with all this knowledge, let’s now try to parse out the title from a [new profile page](http://olympus.realpython.org/profiles/dionysus), which includes this rather carelessly written line of HTML i.e. `<TITLE >Profile: Dionysus</title  / >`. The `.find()` method would have a difficult time dealing with the inconsistencies here, but with the clever use of regular expressions, you can handle this code quickly and efficiently:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "8b6d647a-db9b-456d-965f-45aa95da7be1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<TITLE >Profile: Dionysus</title  / >\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br><br>\n",
      "<img src=\"/static/dionysus.jpg\" />\n",
      "<h2>Name: Dionysus</h2>\n",
      "<img src=\"/static/grapes.png\"><br><br>\n",
      "Hometown: Mount Olympus\n",
      "<br><br>\n",
      "Favorite animal: Leopard <br>\n",
      "<br>\n",
      "Favorite Color: Wine\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html = getHTML(\"http://olympus.realpython.org/profiles/dionysus\")\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "e2e0adeb-9795-4970-a0cd-8843f4eb9983",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def getFromTagRegEx(html, tagStr):\n",
    "    #print(tagStr)\n",
    "    pattern = \"<\" + tagStr + \".*?>.*?</\" + tagStr + \".*?>\"\n",
    "    #print(\"RegEx pattern used: \", pattern)\n",
    "    match_results = re.search(pattern, html, re.IGNORECASE)\n",
    "    print(match_results)\n",
    "    treasure = match_results.group()\n",
    "    return re.sub(\"<.*?>\", \"\", treasure) # Remove HTML tags    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "5c4d0085-7a75-4c97-869f-0dc61a8e8d64",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n<head>\\n<TITLE >Profile: Dionysus</title  / >\\n</head>\\n<body bgcolor=\"yellow\">\\n<center>\\n<br><br>\\n<img src=\"/static/dionysus.jpg\" />\\n<h2>Name: Dionysus</h2>\\n<img src=\"/static/grapes.png\"><br><br>\\nHometown: Mount Olympus\\n<br><br>\\nFavorite animal: Leopard <br>\\n<br>\\nFavorite Color: Wine\\n</center>\\n</body>\\n</html>'"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFromTag(html,\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "85211528-9d00-4083-9ba8-b7c535acb0b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(14, 51), match='<TITLE >Profile: Dionysus</title  / >'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Profile: Dionysus'"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getFromTagRegEx(html,\"title\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a82909a-c37e-4e87-852a-ba7b529018a4",
   "metadata": {},
   "source": [
    "Let’s take a closer look at the first regular expression in the pattern string by breaking it down into three parts:\n",
    "<font color=\"magenta\">(kp: The description below was based on the case when 'tagStr' was equal to \"title\" in the original tutorial. The function is actually my own. So, just think of the word 'title' anywhere 'tagStr' shows up in the function **getFromTagRegEx** above, when trying to appreciate the lines below.)</font>\n",
    "* <title.*?> matches the opening `<TITLE >` tag in html. The `<title` part of the pattern matches with `<TITLE` because `re.search()` is called with `re.IGNORECASE`, and `.*?>` matches any text after `<TITLE` up to the first instance of `>`.\n",
    "* `.*?` non-greedily matches all text after the opening `<TITLE >`, stopping at the first match for `</title.*?>`.\n",
    "* `</title.*?>` differs from the first pattern only in its use of the / character, so it matches the closing `</title / >` tag in html.\n",
    "\n",
    "The second regular expression, the string `\"<.*?>\"`, also uses the non-greedy .*? to match all the HTML tags in the title string. By replacing any matches with \"\", re.sub() removes all the tags and returns only the text.\n",
    "\n",
    "**Note:** `Web scraping in Python or any other language can be tedious. No two websites are organized the same way, and HTML is often messy. Moreover, websites change over time. Web scrapers that work today are not guaranteed to work next year—or next week, for that matter!`\n",
    "\n",
    "Regular expressions are a powerful tool when used correctly. This introduction barely scratches the surface. For more about regular expressions and how to use them, check out the two-part series [Regular Expressions: Regexes in Python](https://realpython.com/regex-python/).\n",
    "\n",
    "### Check Your Understanding\n",
    "\n",
    "Q: Write a program that grabs the full HTML from the following URL:  url = \"http://olympus.realpython.org/profiles/dionysus \"\n",
    "\n",
    "Then use `.find()` to display the text following “Name:” and “Favorite Color:” (not including any leading spaces or trailing HTML tags that might appear on the same line)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "72fd87e4-5ff2-4039-93b5-c58b05f0df60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "<TITLE >Profile: Dionysus</title  / >\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br><br>\n",
      "<img src=\"/static/dionysus.jpg\" />\n",
      "<h2>Name: Dionysus</h2>\n",
      "<img src=\"/static/grapes.png\"><br><br>\n",
      "Hometown: Mount Olympus\n",
      "<br><br>\n",
      "Favorite animal: Leopard <br>\n",
      "<br>\n",
      "Favorite Color: Wine\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "html = getHTML(\"http://olympus.realpython.org/profiles/dionysus\")\n",
    "print(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "85b0e32e-6db6-446b-8779-c8ded49d21df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dionysus\n"
     ]
    }
   ],
   "source": [
    "name = getFromTagRegEx(html, \"h2\")\n",
    "#name = name + \">\" #Just adding an arbitrary symbol \">\" \n",
    "#name = re.search(name,\"Name:.*?>\", re.IGNORECASE)\n",
    "#name = name.group()\n",
    "print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3e0e13a4-59ca-47a8-97db-465362b42c31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Name:']"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = \"Name:\" #\"<\" + tagStr + \".*?>.*?</\" + tagStr + \".*?>\"\n",
    "re.findall(pattern, html, re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2d4b142f-2e0e-495d-825f-a8d3f90af140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(141, 157), match='Name: Dionysus</'>\n",
      " Dionysus\n",
      "Dionysus\n"
     ]
    }
   ],
   "source": [
    "pattern = \"Name:.*?</\"  #Pattern to find everything that starts with \"Name:\" and ends with \"</\"\n",
    "match_results = re.search(pattern, html, re.IGNORECASE)\n",
    "print(match_results)\n",
    "treasure = match_results.group()\n",
    "#https://note.nkmk.me/en/python-str-replace-translate-re-sub/\n",
    "# Patterns delimited by | can be used to replace multiple strings with a single string\n",
    "#re.sub(\"Name:\", \"\", treasure) \n",
    "print(re.sub(\"Name:|</\", \"\", treasure)) #It keeps the leading or trailing space\n",
    "print(re.sub(\"Name:|</| \", \"\", treasure)) #It removes all the spaces (not a good option if we have 2 or more words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2530e8-a0bf-4bb9-837e-12c7de4a5446",
   "metadata": {},
   "source": [
    "### Trimming/Removing Leading and/or Trailing Whitespaces\n",
    "https://www.regular-expressions.info/examples.html#:~:text=Search%20for%20%5E%5B%20%5Ct%5D,%5B%20%5Ct%5D%2B%24.\n",
    "\n",
    "You can easily trim unnecessary whitespace from the start and the end of a string or the lines in a text file by doing a regex search-and-replace. Search for `^[ \\t]+` and replace with nothing to delete leading whitespace (spaces and tabs). Search for `[ \\t]+$` to trim trailing whitespace. Do both by combining the regular expressions into `^[ \\t]+|[ \\t]+$`. Instead of `[ \\t]` which matches a space or a tab, you can expand the character class into `[ \\t\\r\\n]` if you also want to strip line breaks. Or you can use the shorthand `\\s` instead.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "b4ebf136-df7b-487d-b61d-ecd94f3d9294",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' Dionysus'"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.sub(\"Name:|</\", \"\", treasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f8f91dd6-eb3a-4397-8058-26f3548cf754",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dionysus'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://stackoverflow.com/questions/7518172/regular-expression-python-remove-leading-whitespace\n",
    "# \"\\s+\" and \"\\s+$\" are for leading and trailing white spaces.\n",
    "re.sub(\"Name:|</|\\s+|\\s+$\", \"\", treasure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "889653aa-fcd4-4a3f-b82e-c72f9e6f9c31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(268, 289), match='Favorite Color: Wine\\n'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Wine'"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pattern = \"Favorite Color:.*?\\n\"  #Pattern to find everything that starts with \"Name:\" and ends with \"\\n\"\n",
    "match_results = re.search(pattern, html, re.IGNORECASE)\n",
    "print(match_results)\n",
    "treasure = match_results.group()\n",
    "re.sub(\"Favorite Color:|\\n|\\s+|\\s+$\", \"\", treasure)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b588fcd5-d040-4a79-bd2b-79480f9af770",
   "metadata": {},
   "source": [
    "## Solution to above exercise problem from the same tutorial site (revealed from 'Show/Hide' button)\n",
    "\n",
    "First, import the urlopen function from the urlib.request module:\n",
    "\n",
    "    from urllib.request import urlopen\n",
    "\n",
    "Then open the URL and use the .read() method of the HTTPResponse object returned by urlopen() to read the page’s HTML:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "974ec064-2467-47ee-951f-4e957a6386ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://olympus.realpython.org/profiles/dionysus\"\n",
    "html_page = urlopen(url)\n",
    "html_text = html_page.read().decode(\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a8b51fa-6fce-4d27-bc18-b4220bd6c123",
   "metadata": {},
   "source": [
    ".read() returns a byte string, so you use .decode() to decode the bytes using the UTF-8 encoding.\n",
    "\n",
    "Now that you have the HTML source of the web page as a string assigned to the html_text variable, you can extract Dionysus’s name and favorite color from his profile. The structure of the HTML for Dionysus’s profile is the same as Aphrodite’s profile that you saw earlier.\n",
    "\n",
    "You can get the name by finding the string \"Name:\" in the text and extracting everything that comes after the first occurence of the string and before the next HTML tag. That is, you need to extract everything after the colon (:) and before the first angle bracket (<). You can use the same technique to extract the favorite color.\n",
    "\n",
    "The following [for loop](https://realpython.com/python-for-loop/) extracts this text for both the name and favorite color:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "dd6d3cd7-2f9d-4ab0-891c-ca9ea55db100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dionysus\n",
      "Wine\n"
     ]
    }
   ],
   "source": [
    "for string in [\"Name: \", \"Favorite Color:\"]:\n",
    "    string_start_idx = html_text.find(string)\n",
    "    text_start_idx = string_start_idx + len(string)\n",
    "\n",
    "    next_html_tag_offset = html_text[text_start_idx:].find(\"<\")\n",
    "    text_end_idx = text_start_idx + next_html_tag_offset\n",
    "\n",
    "    raw_text = html_text[text_start_idx : text_end_idx]\n",
    "    clean_text = raw_text.strip(\" \\r\\n\\t\")\n",
    "    print(clean_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af9636d-644c-4026-83b3-1f924001a5b1",
   "metadata": {},
   "source": [
    "It looks like there’s a lot going on in this forloop, but it’s just a little bit of arithmetic to calculate the right indices for extracting the desired text. Let’s break it down:\n",
    "\n",
    "1. You use `html_text.find()` to find the starting index of the string, either \"Name:\" or \"Favorite Color:\", and then assign the index to `string_start_idx`.\n",
    "\n",
    "2. Since the text to extract starts just after the colon in \"Name:\" or \"Favorite Color:\", you get the index of the the character immediately after the colon by adding the length of the string to `start_string_idx` and assign the result to `text_start_idx`.\n",
    "\n",
    "3. You calculate the ending index of the text to extract by determining the index of the first angle bracket (<) relative to text_start_idx and assign this value to `next_html_tag_offset`. Then you add that value to `text_start_idx` and assign the result to `text_end_idx`.\n",
    "\n",
    "4. You extract the text by `slicing html_text` from `text_start_idx` `to text_end_idx` and assign this string to `raw_text`.\n",
    "\n",
    "5. You remove any whitespace from the beginning and end of `raw_text` using `.strip()` and assign the result to `clean_text`.\n",
    "\n",
    "At the end of the loop, you use print() to display the extracted text. The final output looks like this:\n",
    "\n",
    "    Dionysus\n",
    "    Wine\n",
    "    \n",
    "This solution is one of many that solves this problem, so if you got the same output with a different solution, then you did great!\n",
    "\n",
    "When you’re ready, you can move on to the next section.\n",
    "\n",
    "## Use an HTML Parser for Web Scraping in Python\n",
    "Although regular expressions are great for pattern matching in general, sometimes it’s easier to use an HTML parser that’s explicitly designed for parsing out HTML pages. There are many Python tools written for this purpose, but the [Beautiful Soup](http://www.crummy.com/software/BeautifulSoup/) library is a good one to start with.\n",
    "\n",
    "### Install Beautiful Soup\n",
    "To install Beautiful Soup, you can run the following in your terminal:\n",
    "\n",
    "    `$ python3 -m pip install beautifulsoup4`\n",
    "    \n",
    "Run `pip show` to see the details of the package you just installed:\n",
    "\n",
    "```\n",
    "$ python3 -m pip show beautifulsoup4\n",
    "Name: beautifulsoup4\n",
    "Version: 4.9.1\n",
    "Summary: Screen-scraping library\n",
    "Home-page: http://www.crummy.com/software/BeautifulSoup/bs4/\n",
    "Author: Leonard Richardson\n",
    "Author-email: leonardr@segfault.org\n",
    "License: MIT\n",
    "Location: c:\\realpython\\venv\\lib\\site-packages\n",
    "Requires:\n",
    "Required-by:\n",
    "In particular, notice that the latest version at the time of writing was 4.9.1.\n",
    "```\n",
    "\n",
    "<font color=\"magenta\">I didn't have to do the installation because it seems the Anaconda already came pre-equipped with it.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a20aa149-e81f-48f1-b09b-e3514a76ae35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/kpadhikari/opt/anaconda3/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "f20844e5-f977-44bb-83ba-deeade3648a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!which beautifulsoup4  #Doesn't show anything because it's not an executable program like python, but just a library"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb543f2-2aa6-4720-ad30-599d582858c1",
   "metadata": {},
   "source": [
    "## Create a BeautifulSoup Object\n",
    "Type the following program into a new editor window:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "68fb6759-85b5-4b21-8c54-369144b97642",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "\n",
    "html = getHTML(\"http://olympus.realpython.org/profiles/dionysus\") #This func was defined above by me \n",
    "\n",
    "soup = BeautifulSoup(html, \"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "7084a77a-0823-4649-99ed-3473d7d0415a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'> <class 'bs4.BeautifulSoup'> \n",
      "\n",
      "<html>\n",
      "<head>\n",
      "<title>Profile: Dionysus</title>\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br/><br/>\n",
      "<img src=\"/static/dionysus.jpg\"/>\n",
      "<h2>Name: Dionysus</h2>\n",
      "<img src=\"/static/grapes.png\"/><br/><br/>\n",
      "Hometown: Mount Olympus\n",
      "<br/><br/>\n",
      "Favorite animal: Leopard <br/>\n",
      "<br/>\n",
      "Favorite Color: Wine\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(html), type(soup), \"\\n\")\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe26ecf-e55b-47aa-9c97-4f0357a36310",
   "metadata": {},
   "source": [
    "Above program does three things:\n",
    "\n",
    "* Opens the URL http://olympus.realpython.org/profiles/dionysus using urlopen() from the urllib.request module\n",
    "\n",
    "* Reads the HTML from the page as a string and assigns it to the html variable\n",
    "\n",
    "* Creates a BeautifulSoup object and assigns it to the soup variable\n",
    "\n",
    "The BeautifulSoup object assigned to soup is created with two arguments. The first argument is the HTML to be parsed, and the second argument, the string \"html.parser\", tells the object which parser to use behind the scenes. \"html.parser\" represents Python’s built-in HTML parser.\n",
    "\n",
    "### Use a BeautifulSoup Object\n",
    "Save and run the above program. When it’s finished running, you can use the soup variable in the interactive window to parse the content of html in various ways.\n",
    "\n",
    "For example, BeautifulSoup objects have a .get_text() method that can be used to extract all the text from the document and automatically remove any HTML tags.\n",
    "\n",
    "Type the following code into IDLE’s interactive window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "b740a51e-68d5-4ed7-a70e-e61f942a7373",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Profile: Dionysus\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Name: Dionysus\n",
      "\n",
      "Hometown: Mount Olympus\n",
      "\n",
      "Favorite animal: Leopard \n",
      "\n",
      "Favorite Color: Wine\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(soup.get_text()) #It seems it removes all the html tags"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f27c85-f13d-419f-bdd2-59e8255a6b1f",
   "metadata": {},
   "source": [
    "There are a lot of blank lines in this output. These are the result of newline characters in the HTML document’s text. You can remove them with the string `.replace()` method if you need to.\n",
    "\n",
    "Often, you need to get only specific text from an HTML document. <font color=\"red\">Using Beautiful Soup first to extract the text and then using the `.find()` string method is sometimes easier than working with regular expressions.</font>\n",
    "\n",
    "However, sometimes the HTML tags themselves are the elements that point out the data you want to retrieve. For instance, perhaps you want to retrieve the URLs for all the images on the page. These links are contained in the src attribute of `<img>` HTML tags.\n",
    "\n",
    "In this case, you can use `find_all()` to return a list of all instances of that particular tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "8cd02bfa-09bb-4c22-9078-fe3eab2296a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img src=\"/static/dionysus.jpg\"/>, <img src=\"/static/grapes.png\"/>]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all(\"img\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7bfdd55-0d81-44f9-b7b6-dea587a54a2a",
   "metadata": {},
   "source": [
    "This returns a list of all `<img>` tags in the HTML document. The objects in the list look like they might be strings representing the tags, but they’re actually instances of the Tag object provided by Beautiful Soup. Tag objects provide a simple interface for working with the information they contain.\n",
    "\n",
    "Let’s explore this a little by first unpacking the Tag objects from the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "03a81298-9e66-47b3-8113-1fa7deb8cedf",
   "metadata": {},
   "outputs": [],
   "source": [
    "image1, image2 = soup.find_all(\"img\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "430ea697-1669-49bb-a584-fb946dd0bcdf",
   "metadata": {},
   "source": [
    "Each Tag object has a .name property that returns a string containing the HTML tag type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "d03f040b-f2aa-46d5-8c34-ace561470e82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'img'"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1.name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c90976-71cf-4fb1-a674-65d17044fa4f",
   "metadata": {},
   "source": [
    "You can access the HTML attributes _(kp: such as color, font-size, style etc)_ of the Tag object by putting their name between square brackets, just as if the attributes were keys in a dictionary.\n",
    "\n",
    "For example, the `<img src=\"/static/dionysus.jpg\"/>` tag has a single attribute, src, with the value `\"/static/dionysus.jpg\"`. Likewise, an HTML tag such as the link `<a href=\"https://realpython.com\" target=\"_blank\">` has two attributes, href and target.\n",
    "\n",
    "To get the source of the images in the Dionysus profile page, you access the src attribute using the dictionary notation mentioned above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "76702f9d-abe8-4974-9c8d-d9ebfd325752",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/static/dionysus.jpg'"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image1['src']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "dbe8fb0d-399c-425c-bef2-3ee384cc65ed",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/static/grapes.png'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image2['src']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb44edf-405e-46ac-b72b-e4ed9e76a549",
   "metadata": {},
   "source": [
    "Certain tags in HTML documents can be accessed by properties of the Tag object. For example, to get the `<title>` tag in a document, you can use the `.title` property <font color=\"magenta\">kp: I think this works well only with those tags which has only one instance of it - such as 'title', 'heade' etc and not those such as 'p', 'br', 'hr' etc which can have more than one instances)</font>:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3ed75472-e664-41bd-82e0-4db5377568be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>Profile: Dionysus</title>"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee00576-83d2-4887-be81-465b969b88d3",
   "metadata": {},
   "source": [
    "If you look at the source of the Dionysus profile by navigating to the profile page, right-clicking on the page, and selecting View page source, then you’ll notice that the `<title>` tag as written in the document looks like this:\n",
    "\n",
    "    <title >Profile: Dionysus</title/>\n",
    "    \n",
    "Beautiful Soup automatically cleans up the tags for you by removing the extra space in the opening tag and the extraneous forward slash (/) in the closing tag.\n",
    "\n",
    "You can also retrieve just the string between the title tags with the .string property of the Tag object:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "2efeae2f-7415-4fbc-ad85-fa86062b5f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Profile: Dionysus'"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.title.string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "5df629cd-22b3-4cbd-8616-8af316867897",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<br/>"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.br #There are multiple instances of <br/> but it returns only one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5e2692bc-feb4-4ec6-93ae-f4f83ccbdfe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<head>\n",
       "<title>Profile: Dionysus</title>\n",
       "</head>"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.head"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d1dfd9-f483-4b79-aa53-6c70c323f91f",
   "metadata": {},
   "source": [
    "One of the more useful features of Beautiful Soup is the ability to search for specific kinds of tags whose attributes match certain values. For example, if you want to find all the `<img>` tags that have a src attribute equal to the value `/static/dionysus.jpg`, then you can provide the following additional argument to .find_all():"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "55059f7e-275d-46cd-855b-768e6244c950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<img src=\"/static/dionysus.jpg\"/>]"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.find_all('img', src='/static/dionysus.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e6c5da-5bc5-4128-92a8-385429a7202f",
   "metadata": {},
   "source": [
    "This example is somewhat arbitrary, and the usefulness of this technique may not be apparent from the example. If you spend some time browsing various websites and viewing their page sources, then you’ll notice that many websites have extremely complicated HTML structures.\n",
    "\n",
    "When scraping data from websites with Python, you’re often interested in particular parts of the page. By spending some time looking through the HTML document, you can identify tags with unique attributes that you can use to extract the data you need.\n",
    "\n",
    "Then, instead of relying on complicated regular expressions or using .find() to search through the document, you can directly access the particular tag you’re interested in and extract the data you need.\n",
    "\n",
    "In some cases, you may find that Beautiful Soup doesn’t offer the functionality you need. The [lxml](http://lxml.de/) library is somewhat trickier to get started with but offers far more flexibility than Beautiful Soup for parsing HTML documents. You may want to check it out once you’re comfortable using Beautiful Soup.\n",
    "\n",
    "    Note: HTML parsers like Beautiful Soup can save you a lot of time and effort when it comes to locating specific data in web pages. However, sometimes HTML is so poorly written and disorganized that even a sophisticated parser like Beautiful Soup can’t interpret the HTML tags properly.\n",
    "\n",
    "    In this case, you’re often left with using .find() and regular expression techniques to try to parse out the information you need.\n",
    "\n",
    "<font color=\"red\">BeautifulSoup is great for scraping data from a website’s HTML, but it doesn’t provide any way to work with HTML forms. For example, if you need to search a website for some query and then scrape the results, then BeautifulSoup alone won’t get you very far.</font>\n",
    "\n",
    "### Check your understanding\n",
    "Write a program that grabs the full HTML from the [page](http://olympus.realpython.org/profiles) at the URL http://olympus.realpython.org/profiles.\n",
    "\n",
    "Using Beautiful Soup, print out a list of all the links on the page by looking for HTML tags with the name `a` and retrieving the value taken on by the href attribute of each tag.\n",
    "\n",
    "The final output should look like this:\n",
    "\n",
    "    http://olympus.realpython.org/profiles/aphrodite\n",
    "    http://olympus.realpython.org/profiles/poseidon\n",
    "    http://olympus.realpython.org/profiles/dionysus\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "9461c9e9-ff23-4196-a13d-3f2aabe2d0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<html>\n",
       "<head>\n",
       "<title>All Profiles</title>\n",
       "</head>\n",
       "<body bgcolor=\"yellow\">\n",
       "<center>\n",
       "<br/><br/>\n",
       "<h1>All Profiles:</h1>\n",
       "<br/><br/>\n",
       "<h2>\n",
       "<a href=\"/profiles/aphrodite\">Aphrodite</a>\n",
       "<br/><br/>\n",
       "<a href=\"/profiles/poseidon\">Poseidon</a>\n",
       "<br/><br/>\n",
       "<a href=\"/profiles/dionysus\">Dionysus</a>\n",
       "</h2>\n",
       "</center>\n",
       "</body>\n",
       "</html>"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "html = getHTML('http://olympus.realpython.org/profiles')\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "bc7e7170-0f1f-4fc5-bfda-2795d307b03c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"/profiles/aphrodite\">Aphrodite</a>"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "d1ac421c-f441-40c4-ac3e-28ad717a3fbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/profiles/aphrodite'"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.a['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "f8841450-a347-4f59-b339-356853403cc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<h2>\n",
       "<a href=\"/profiles/aphrodite\">Aphrodite</a>\n",
       "<br/><br/>\n",
       "<a href=\"/profiles/poseidon\">Poseidon</a>\n",
       "<br/><br/>\n",
       "<a href=\"/profiles/dionysus\">Dionysus</a>\n",
       "</h2>"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.h2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "5d00c910-a895-4bce-a727-f816b0f5040e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<a href=\"/profiles/aphrodite\">Aphrodite</a>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup.h2.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "8852b099-2c5f-423b-9c20-b2d2dc643fd4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/profiles/aphrodite\">Aphrodite</a>,\n",
       " <a href=\"/profiles/poseidon\">Poseidon</a>,\n",
       " <a href=\"/profiles/dionysus\">Dionysus</a>]"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = soup.find_all('a')\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "156d517f-3e36-4d61-ad84-e176bfe945f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/profiles/aphrodite\n",
      "/profiles/poseidon\n",
      "/profiles/dionysus\n"
     ]
    }
   ],
   "source": [
    "for link in links:\n",
    "    print(link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "cc3a4cb0-4892-4298-aae0-fb2efb1cc6ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://olympus.realpython.org//profiles/aphrodite\n",
      "http://olympus.realpython.org//profiles/poseidon\n",
      "http://olympus.realpython.org//profiles/dionysus\n"
     ]
    }
   ],
   "source": [
    "for link in links:\n",
    "    print('http://olympus.realpython.org/' + link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dffa40d-e351-4455-8b19-c511007f06d9",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The Solution to above exercise problem from Show/Hide \n",
    "First, import the urlopen function from the urlib.request module and the BeautifulSoup class from the bs4 package:\n",
    "\n",
    "    from urllib.request import urlopen\n",
    "    from bs4 import BeautifulSoup\n",
    "Each link URL on the /profiles page is a relative URL, so create a base_url variable with the base URL of the website:\n",
    "\n",
    "    base_url = \"http://olympus.realpython.org\"\n",
    "    \n",
    "You can build a full URL by concatenating base_url with a relative URL.\n",
    "\n",
    "Now open the /profiles page with urlopen() and use .read() to get the HTML source:\n",
    "\n",
    "    html_page = urlopen(base_url + \"/profiles\")\n",
    "    html_text = html_page.read().decode(\"utf-8\")\n",
    "    \n",
    "With the HTML source downloaded and decoded, you can create a new BeautifulSoup object to parse the HTML:\n",
    "\n",
    "    soup = BeautifulSoup(html_text, \"html.parser\")\n",
    "    \n",
    "`soup.find_all(\"a\")` returns a list of all links in the HTML source. You can loop over this list to print out all the links on the webpage:\n",
    "\n",
    "    for link in soup.find_all(\"a\"):\n",
    "        link_url = base_url + link[\"href\"]\n",
    "        print(link_url)\n",
    "    \n",
    "The relative URL for each link can be accessed through the \"href\" subscript. Concatenate this value with `base_url` to create the full `link_url`.\n",
    "\n",
    "When you’re ready, you can move on to the next section. <font color=\"magenta\">(kp: Basically, that's what I did above myself without looking at this solution)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532a77a7-d782-42fa-bccb-c44d49ebe0e5",
   "metadata": {},
   "source": [
    "## Interact With HTML Forms\n",
    "The urllib module you’ve been working with so far in this tutorial is well suited for requesting the contents of a web page. <font color=\"red\">Sometimes, though, you need to interact with a web page to obtain the content you need. For example, you might need to submit a form or click a button to display hidden content.</font>\n",
    "\n",
    "Note: This tutorial is adapted from the chapter “Interacting With the Web” in [Python Basics: A Practical Introduction to Python 3](https://realpython.com/products/python-basics-book/). If you enjoy what you’re reading, then be sure to check out the rest of the book.\n",
    "\n",
    "The Python standard library doesn’t provide a built-in means for working with web pages interactively, but many third-party packages are available from PyPI. Among these, [MechanicalSoup](https://github.com/hickford/MechanicalSoup) is a popular and relatively straightforward package to use.\n",
    "\n",
    "<font color=\"red\" size=\"5.0\">In essence, MechanicalSoup installs what’s known as a headless browser, which is a web browser with no graphical user interface.</font> This browser is controlled programmatically via a Python program.\n",
    "\n",
    "### Install MechanicalSoup\n",
    "You can install MechanicalSoup with pip in your terminal:\n",
    "\n",
    "    `$ python3 -m pip install MechanicalSoup`\n",
    "    \n",
    "You can now view some details about the package with pip show:\n",
    "```\n",
    "    $ python3 -m pip show mechanicalsoup\n",
    "    Name: MechanicalSoup\n",
    "    Version: 0.12.0\n",
    "    Summary: A Python library for automating interaction with websites\n",
    "    Home-page: https://mechanicalsoup.readthedocs.io/\n",
    "    Author: UNKNOWN\n",
    "    Author-email: UNKNOWN\n",
    "    License: MIT\n",
    "    Location: c:\\realpython\\venv\\lib\\site-packages\n",
    "    Requires: requests, beautifulsoup4, six, lxml\n",
    "    Required-by:\n",
    "```    \n",
    "In particular, notice that the latest version at the time of writing was `0.12.0`. You’ll need to close and restart your IDLE session for MechanicalSoup to load and be recognized after it’s been installed.\n",
    "\n",
    "It seems the Anaconda doesn't have the 'mechanicalsoup' library because it gave `ModuleNotFoundError: No module named 'mechanicalsoup'`. So, I am installing it here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "9cdfa633-5b61-4054-b2cc-c7e0166b05fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting MechanicalSoup\n",
      "  Using cached MechanicalSoup-1.1.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: beautifulsoup4>=4.7 in /Users/kpadhikari/opt/anaconda3/lib/python3.9/site-packages (from MechanicalSoup) (4.10.0)\n",
      "Requirement already satisfied: lxml in /Users/kpadhikari/opt/anaconda3/lib/python3.9/site-packages (from MechanicalSoup) (4.6.3)\n",
      "Requirement already satisfied: requests>=2.22.0 in /Users/kpadhikari/opt/anaconda3/lib/python3.9/site-packages (from MechanicalSoup) (2.26.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/kpadhikari/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4>=4.7->MechanicalSoup) (2.2.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/kpadhikari/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.22.0->MechanicalSoup) (3.2)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/kpadhikari/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.22.0->MechanicalSoup) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/kpadhikari/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.22.0->MechanicalSoup) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/kpadhikari/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.22.0->MechanicalSoup) (2021.10.8)\n",
      "Installing collected packages: MechanicalSoup\n",
      "Successfully installed MechanicalSoup-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!python3 -m pip install MechanicalSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b63fb12-1b1d-4ac3-92a0-023e3db079e6",
   "metadata": {},
   "source": [
    "### Create a Browser Object\n",
    "\n",
    "Type the following into IDLE’s interactive window:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "d739cdc0-232b-484c-9914-a4fb5715f7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'mechanicalsoup.browser.Browser'>\n",
      "\n",
      " <mechanicalsoup.browser.Browser object at 0x7f97ae00a910>\n"
     ]
    }
   ],
   "source": [
    "import mechanicalsoup\n",
    "browser = mechanicalsoup.Browser()\n",
    "print(type(browser))\n",
    "print(\"\\n\", browser)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d813edcd-4218-43d0-8331-7c34ce2b5be2",
   "metadata": {},
   "source": [
    "Browser objects represent the headless web browser. You can use them to request a page from the Internet by passing a URL to their .get() method:\n",
    "\n",
    "    >>> url = \"http://olympus.realpython.org/login\"\n",
    "    >>> page = browser.get(url)\n",
    "    \n",
    "page is a Response object that stores the response from requesting the URL from the browser:\n",
    "\n",
    "    >>> page\n",
    "    <Response [200]>\n",
    "    \n",
    "The number 200 represents the status code returned by the request. A status code of 200 means that the request was successful. An unsuccessful request might show a status code of 404 if the URL doesn’t exist or 500 if there’s a server error when making the request.\n",
    "\n",
    "MechanicalSoup uses Beautiful Soup to parse the HTML from the request. page has a .soup attribute that represents a BeautifulSoup object:\n",
    "\n",
    "    >>> type(page.soup)\n",
    "    <class 'bs4.BeautifulSoup'>\n",
    "    \n",
    "You can view the HTML by inspecting the .soup attribute:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "1f18e2b1-2c9c-4e20-98c2-85459e023327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"http://olympus.realpython.org/login\"\n",
    "page = browser.get(url)\n",
    "page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6b059536-5a33-4cb9-b94f-2012b704d611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'requests.models.Response'>\n",
      "<class 'bs4.BeautifulSoup'> \n",
      " -------kpa kpa kpa------\n",
      "<html>\n",
      "<head>\n",
      "<title>Log In</title>\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br/><br/>\n",
      "<h2>Please log in to access Mount Olympus:</h2>\n",
      "<br/><br/>\n",
      "<form action=\"/login\" method=\"post\" name=\"login\">\n",
      "Username: <input name=\"user\" type=\"text\"/><br/>\n",
      "Password: <input name=\"pwd\" type=\"password\"/><br/><br/>\n",
      "<input type=\"submit\" value=\"Submit\"/>\n",
      "</form>\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(type(page))\n",
    "print(type(page.soup), \"\\n -------kpa kpa kpa------\")\n",
    "print(page.soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0365379c-d035-4a8c-9412-e6fb1b20436d",
   "metadata": {},
   "source": [
    "Notice this page has a `<form>` on it with `<input>` elements for a username and a password.\n",
    "\n",
    "### Submit a Form With MechanicalSoup\n",
    "Open the `/login` page (http://olympus.realpython.org/login) from the previous example in a browser and look at it yourself before moving on. Try typing in a random username and password combination. If you guess incorrectly, then the message “Wrong username or password!” is displayed at the bottom of the page.\n",
    "\n",
    "However, if you provide the correct login credentials (username zeus and password ThunderDude), then you’re redirected to the /profiles page.\n",
    "\n",
    "In the next example, you’ll see how to use MechanicalSoup to fill out and submit this form using Python!\n",
    "\n",
    "The important section of HTML code is the login form—that is, everything inside the `<form>` tags. The `<form>` on this page has the name attribute set to login. This form contains two `<input>` elements, one named user and the other named pwd. The third `<input>` element is the Submit button.\n",
    "\n",
    "Now that you know the underlying structure of the login form, as well as the credentials needed to log in, let’s take a look at a program. that fills the form out and submits it.\n",
    "\n",
    "In a new editor window, type in the following program:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "23739bc2-8225-4c78-8b24-4f8d089b38d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n"
     ]
    }
   ],
   "source": [
    "import mechanicalsoup\n",
    "\n",
    "# 1\n",
    "browser = mechanicalsoup.Browser()\n",
    "url = \"http://olympus.realpython.org/login\"\n",
    "login_page = browser.get(url)\n",
    "login_html = login_page.soup\n",
    "\n",
    "# 2\n",
    "form = login_html.select(\"form\")[0]\n",
    "form.select(\"input\")[0][\"value\"] = \"zeus\"\n",
    "form.select(\"input\")[1][\"value\"] = \"ThunderDude\"\n",
    "\n",
    "# 3\n",
    "profiles_page = browser.submit(form, login_page.url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "b1e8ab1f-7272-426c-b658-9984d0eb6362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200]>\n",
      "<html>\n",
      "<head>\n",
      "<title>Log In</title>\n",
      "</head>\n",
      "<body bgcolor=\"yellow\">\n",
      "<center>\n",
      "<br/><br/>\n",
      "<h2>Please log in to access Mount Olympus:</h2>\n",
      "<br/><br/>\n",
      "<form action=\"/login\" method=\"post\" name=\"login\">\n",
      "Username: <input name=\"user\" type=\"text\" value=\"zeus\"/><br/>\n",
      "Password: <input name=\"pwd\" type=\"password\" value=\"ThunderDude\"/><br/><br/>\n",
      "<input type=\"submit\" value=\"Submit\"/>\n",
      "</form>\n",
      "</center>\n",
      "</body>\n",
      "</html>\n",
      "\n",
      "<form action=\"/login\" method=\"post\" name=\"login\">\n",
      "Username: <input name=\"user\" type=\"text\" value=\"zeus\"/><br/>\n",
      "Password: <input name=\"pwd\" type=\"password\" value=\"ThunderDude\"/><br/><br/>\n",
      "<input type=\"submit\" value=\"Submit\"/>\n",
      "</form>\n",
      "<Response [200]>\n",
      "<class 'requests.models.Response'>\n",
      "http://olympus.realpython.org/profiles\n",
      "b'<html>\\n<head>\\n<title>All Profiles</title>\\n</head>\\n<body bgcolor=\"yellow\">\\n<center>\\n<br><br>\\n<h1>All Profiles:</h1>\\n<br><br>\\n<h2>\\n<a href=\"/profiles/aphrodite\">Aphrodite</a>\\n<br><br>\\n<a href=\"/profiles/poseidon\">Poseidon</a>\\n<br><br>\\n<a href=\"/profiles/dionysus\">Dionysus</a>\\n</h2>\\n</center>\\n</body>\\n</html>\\n'\n"
     ]
    }
   ],
   "source": [
    "print(login_page)\n",
    "print(login_html)\n",
    "print(form)\n",
    "print(profiles_page)\n",
    "print(type(profiles_page))\n",
    "print(profiles_page.url)\n",
    "print(profiles_page.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c31b124-d1f9-4c07-9043-f3427fdea1a4",
   "metadata": {},
   "source": [
    "Let’s break down the above example:\n",
    "\n",
    "1. You create a Browser instance and use it to request the URL http://olympus.realpython.org/login. You assign the HTML content of the page to the login_html variable using the .soup property.\n",
    "\n",
    "2. `login_html.select(\"form\")` returns a list of all `<form>` elements on the page. Since the page has only one `<form>` element, you can access the form by retrieving the element at index 0 of the list. The next two lines select the username and password inputs and set their value to \"zeus\" and \"ThunderDude\", respectively.\n",
    "\n",
    "3. You submit the form with `browser.submit()`. Notice that you pass two arguments to this method, the form object and the URL of the login_page, which you access via `login_page.url`.\n",
    "\n",
    "In the interactive window, you confirm that the submission successfully redirected to the /profiles page. If something had gone wrong, then the value of `profiles_page.url` would still be `\"http://olympus.realpython.org/login\"`.\n",
    "\n",
    "<font color=\"Green\">\n",
    "    <b>Note:</b> Hackers can use automated programs like the one above to brute force logins by rapidly trying many different usernames and passwords until they find a working combination.\n",
    "\n",
    "Besides this being highly illegal, almost all websites these days lock you out and report your IP address if they see you making too many failed requests, so don’t try it!\n",
    "</font>\n",
    "\n",
    "Now that we have the profiles_page variable set, let’s see how to programmatically obtain the URL for each link on the /profiles page.\n",
    "\n",
    "To do this, you use .select() again, this time passing the string \"a\" to select all the `<a>` anchor elements on the page:\n",
    "\n",
    "```\n",
    ">>> links = profiles_page.soup.select(\"a\")\n",
    "Now you can iterate over each link and print the href attribute:\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "a518a87d-e73f-450c-914a-b961fad40842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<a href=\"/profiles/aphrodite\">Aphrodite</a>,\n",
       " <a href=\"/profiles/poseidon\">Poseidon</a>,\n",
       " <a href=\"/profiles/dionysus\">Dionysus</a>]"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "links = profiles_page.soup.select(\"a\")\n",
    "links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "35a2ada4-e154-4aee-8a7c-6398b147b100",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aphrodite: /profiles/aphrodite\n",
      "Poseidon: /profiles/poseidon\n",
      "Dionysus: /profiles/dionysus\n"
     ]
    }
   ],
   "source": [
    "for link in links:\n",
    "    address = link['href'] #it gives the value of href attribute for the current 'a' or 'anchor' tag\n",
    "    text = link.text       # it gives the hyperlinked text i.e. the text between the tags <a> and </a>\n",
    "    print(f\"{text}: {address}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e5afa6-66a1-4569-8890-15b940fdc15d",
   "metadata": {},
   "source": [
    "The URLs contained in each href attribute are relative URLs, which aren’t very helpful if you want to navigate to them later using MechanicalSoup. If you happen to know the full URL, then you can assign the portion needed to construct a full URL.\n",
    "\n",
    "In this case, the base URL is just http://olympus.realpython.org. Then you can concatenate the base URL with the relative URLs found in the src attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "46ed7868-9d5c-401d-b198-97d53da59098",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aphrodite: http://olympus.realpython.org/profiles/aphrodite\n",
      "Poseidon: http://olympus.realpython.org/profiles/poseidon\n",
      "Dionysus: http://olympus.realpython.org/profiles/dionysus\n"
     ]
    }
   ],
   "source": [
    "base_url = \"http://olympus.realpython.org\"\n",
    "\n",
    "for link in links:\n",
    "    address = base_url + link[\"href\"]\n",
    "    text = link.text\n",
    "    print(f\"{text}: {address}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb209ee-d989-4188-964b-730d23f41199",
   "metadata": {},
   "source": [
    "You can do a lot with just .get(), .select(), and .submit(). That said, MechanicalSoup is capable of much more. To learn more about MechanicalSoup, check out the [official docs](https://mechanicalsoup.readthedocs.io/en/stable/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769ecbc6-afe8-4b03-bf88-b1cde0631350",
   "metadata": {},
   "source": [
    "### Check Your Understanding (From Show/Hide)\n",
    "Use MechanicalSoup to provide the correct username (zeus) and password (ThunderDude) to the login form located at the URL http://olympus.realpython.org/login.\n",
    "\n",
    "Once the form is submitted, display the title of the current page to determine that you’ve been redirected to the /profiles page.\n",
    "\n",
    "Your program should print the text `<title>All Profiles</title>`.\n",
    "\n",
    "#### My Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "66d3e69e-8014-4c68-b773-32c5e2894a60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<title>All Profiles</title>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<title>All Profiles</title>]"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mechanicalsoup\n",
    "\n",
    "\"\"\"\n",
    "# 1\n",
    "browser = mechanicalsoup.Browser()\n",
    "url = \"http://olympus.realpython.org/login\"\n",
    "login_page = browser.get(url)\n",
    "login_html = login_page.soup\n",
    "\n",
    "# 2\n",
    "form = login_html.select(\"form\")[0]\n",
    "form.select(\"input\")[0][\"value\"] = \"zeus\"\n",
    "form.select(\"input\")[1][\"value\"] = \"ThunderDude\"\n",
    "\n",
    "# 3\n",
    "profiles_page = browser.submit(form, login_page.url)\n",
    "\"\"\"\n",
    "# I am simply reusing above lines (from previous cells)\n",
    "print(profiles_page.soup.select(\"title\"))\n",
    "profiles_page.soup.select(\"title\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "5a019890-e695-406f-8e07-d84f2c122732",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>All Profiles</title>"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "profiles_page.soup.select(\"title\")[0] # To get rid of the brackets of the list, we must use the index of a particular cell\n",
    "#Please note that 'select' returns a list of all the tags of the type entered as the argument such as 'title', 'a' etc.\n",
    "#.   Sometimes, there could be only one instance such as with 'title' or 'head'. However, there could be more of 'h1', 'h2',\n",
    "#.   'p', 'br', 'hr', etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08968cef-539c-49a0-afa2-a95a42d0de1b",
   "metadata": {},
   "source": [
    "### Solution from the Tutorial (revealed by the Show/Hide button)\n",
    "First, import the mechanicalsoup package and create a Broswer object:\n",
    "\n",
    "```python\n",
    "import mechanicalsoup\n",
    "\n",
    "browser = mechanicalsoup.Browser()\n",
    "```\n",
    "\n",
    "Point the browser to the login page by passing the URL to browser.get() and grab the HTML with the .soup attribute:\n",
    "\n",
    "```python\n",
    "login_url = \"http://olympus.realpython.org/login\"\n",
    "login_page = browser.get(login_url)\n",
    "login_html = login_page.soup\n",
    "```\n",
    "\n",
    "login_html is a BeautifulSoup instance. Since the page has only a single form on it, you can access the form via login_html.form. Using .select(), select the username and password inputs and fill them with the username \"zeus\" and the password \"ThunderDude\":\n",
    "\n",
    "```python\n",
    "form = login_html.form\n",
    "form.select(\"input\")[0][\"value\"] = \"zeus\"\n",
    "form.select(\"input\")[1][\"value\"] = \"ThunderDude\"\n",
    "```\n",
    "Now that the form is filled out, you can submit it with browser.submit():\n",
    "\n",
    "```python\n",
    "profiles_page = browser.submit(form, login_page.url)\n",
    "```\n",
    "\n",
    "If you filled the form with the correct username and password, then profiles_page should actually point to the /profiles page. You can confirm this by printing the title of the page assigned to profiles_page:\n",
    "\n",
    "    print(profiles_page.soup.title)\n",
    "\n",
    "You should see the following text displayed:\n",
    "\n",
    "`<title>All Profiles</title>`\n",
    "\n",
    "If instead you see the text Log In or something else, then the form submission failed.\n",
    "\n",
    "When you’re ready, you can move on to the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "c9910388-b7d6-4e4a-9265-419aedd14726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<title>All Profiles</title>\n"
     ]
    }
   ],
   "source": [
    "print(profiles_page.soup.title)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ca05bb8-9b26-4d6d-a922-a5f2d9c8041a",
   "metadata": {},
   "source": [
    "## Interact With Websites in Real Time\n",
    "Sometimes you want to be able to fetch real-time data from a website that offers continually updated information.\n",
    "\n",
    "In the dark days before you learned Python programming, you had to sit in front of a browser, clicking the Refresh button to reload the page each time you wanted to check if updated content was available. But now you can automate this process using the .get() method of the MechanicalSoup Browser object.\n",
    "\n",
    "Open your browser of choice and navigate to the URL http://olympus.realpython.org/dice. This /dice page simulates a roll of a six-sided die, updating the result each time you refresh the browser. Below, you’ll write a program that repeatedly scrapes the page for a new result.\n",
    "\n",
    "The first thing you need to do is determine which element on the page contains the result of the die roll. Do this now by right-clicking anywhere on the page and selecting View page source. A little more than halfway down the HTML code is an `<h2>` tag that looks like this:\n",
    "\n",
    "```html\n",
    "<h2 id=\"result\">4</h2>\n",
    "```\n",
    "\n",
    "The text of the `<h2>` tag might be different for you, but this is the page element you need for scraping the result.\n",
    "\n",
    "    **Note:** For this example, you can easily check that there’s only one element on the page with id=\"result\". Although the id attribute is supposed to be unique, in practice you should always check that the element you’re interested in is uniquely identified.\n",
    "\n",
    "Let’s start by writing a simple program that opens the /dice page, scrapes the result, and prints it to the console:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "b5361418-19ee-468d-be7b-1ff4df47578f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tag = <h2 id=\"result\">6</h2> \n",
      "\n",
      "The result of your dice roll is: 6\n"
     ]
    }
   ],
   "source": [
    "import mechanicalsoup\n",
    "\n",
    "browser = mechanicalsoup.Browser()\n",
    "page = browser.get(\"http://olympus.realpython.org/dice\")\n",
    "tag = page.soup.select(\"#result\")[0]\n",
    "result = tag.text\n",
    "\n",
    "print(f'tag = {tag} \\n') #kp:\n",
    "\n",
    "print(f\"The result of your dice roll is: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bade7e44-d04f-4471-8229-70b233030b10",
   "metadata": {},
   "source": [
    "This example uses the BeautifulSoup object’s .select() method to find the element with id=result. The string \"#result\" that you pass to .select() uses the [CSS ID selector](https://developer.mozilla.org/en-US/docs/Web/CSS/ID_selectors) `#` to indicate that result is an id value.\n",
    "\n",
    "To periodically get a new result, you’ll need to create a loop that loads the page at each step. So everything below the line `browser = mechanicalsoup.Browser()` in the above code needs to go in the body of the loop.\n",
    "\n",
    "For this example, let’s get four rolls of the dice at ten-second intervals. To do that, the last line of your code needs to tell Python to pause running for ten seconds. You can do this with [sleep()](https://realpython.com/python-sleep/) from Python’s [time module](https://realpython.com/python-time-module/). sleep() takes a single argument that represents the amount of time to sleep in seconds.\n",
    "\n",
    "Here’s an example that illustrates how sleep() works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "e8635860-4852-4389-96fd-cc8088432393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'm about to wait for five seconds...\n",
      "Done waiting!\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "print(\"I'm about to wait for five seconds...\")\n",
    "time.sleep(5)\n",
    "print(\"Done waiting!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154167dc-327f-409c-8ba7-d1796542e6e0",
   "metadata": {},
   "source": [
    "When you run this code, you’ll see that the \"Done waiting!\" message isn’t displayed until 5 seconds have passed from when the first print() function was executed.\n",
    "\n",
    "For the die roll example, you’ll need to pass the number 10 to sleep(). Here’s the updated program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "b1d8c19b-f15e-4ae4-be0c-1925eae46438",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of your dice roll is: 1\n",
      "The result of your dice roll is: 4\n",
      "The result of your dice roll is: 6\n",
      "The result of your dice roll is: 5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import mechanicalsoup\n",
    "\n",
    "browser = mechanicalsoup.Browser()\n",
    "\n",
    "for i in range(4):\n",
    "    page = browser.get(\"http://olympus.realpython.org/dice\")\n",
    "    tag = page.soup.select(\"#result\")[0]\n",
    "    result = tag.text\n",
    "    print(f\"The result of your dice roll is: {result}\")\n",
    "    time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eb823f-d126-4aa9-81cf-001d4c001668",
   "metadata": {},
   "source": [
    "When you run the program, you’ll immediately see the first result printed to the console. After ten seconds, the second result is displayed, then the third, and finally the fourth. What happens after the fourth result is printed?\n",
    "\n",
    "The program continues running for another ten seconds before it finally stops!\n",
    "\n",
    "Well, of course it does—that’s what you told it to do! But it’s kind of a waste of time. You can stop it from doing this by using an if statement to run time.sleep() for only the first three requests:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "3dfa0724-6841-49df-89c5-c57a624bc7a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The result of your dice roll is: 4\n",
      "The result of your dice roll is: 6\n",
      "The result of your dice roll is: 1\n",
      "The result of your dice roll is: 5\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import mechanicalsoup\n",
    "\n",
    "browser = mechanicalsoup.Browser()\n",
    "\n",
    "for i in range(4):\n",
    "    page = browser.get(\"http://olympus.realpython.org/dice\")\n",
    "    tag = page.soup.select(\"#result\")[0]\n",
    "    result = tag.text\n",
    "    print(f\"The result of your dice roll is: {result}\")\n",
    "\n",
    "    # Wait 10 seconds if this isn't the last request\n",
    "    if i < 3:\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b669bc-fc86-4eb7-8e94-7e6366965c52",
   "metadata": {},
   "source": [
    "With techniques like this, you can scrape data from websites that periodically update their data. However, you should be aware that requesting a page multiple times in rapid succession can be seen as suspicious, or even malicious, use of a website.\n",
    "\n",
    "<font color=\"green\">\n",
    "    <b>Important:</b> Most websites publish a Terms of Use document. You can often find a link to it in the website’s footer.\n",
    "\n",
    "Always read this document before attempting to scrape data from a website. If you can’t find the Terms of Use, try to contact the website owner and ask them if they have any policies regarding request volume.\n",
    "\n",
    "Failure to comply with the Terms of Use could result in your IP being blocked, so be careful and be respectful! </font>\n",
    "\n",
    "It’s even possible to crash a server with an excessive number of requests, so you can imagine that many websites are concerned about the volume of requests to their server! Always check the Terms of Use and be respectful when sending multiple requests to a website.\n",
    "\n",
    "## Conclusion\n",
    "Although it’s possible to parse data from the Web using tools in Python’s standard library, there are many tools on PyPI that can help simplify the process.\n",
    "\n",
    "In this tutorial, you learned how to:\n",
    "\n",
    "* Request a web page using Python’s built-in urllib module\n",
    "* Parse HTML using Beautiful Soup\n",
    "* Interact with web forms using MechanicalSoup\n",
    "* Repeatedly request data from a website to check for updates\n",
    "\n",
    "Writing automated web scraping programs is fun, and the Internet has no shortage of content that can lead to all sorts of exciting projects.\n",
    "\n",
    "Just remember, not everyone wants you pulling data from their web servers. Always check a website’s Terms of Use before you start scraping, and be respectful about how you time your web requests so that you don’t flood a server with traffic.\n",
    "\n",
    "## Additional Resources\n",
    "For more information on web scraping with Python, check out the following resources:\n",
    "\n",
    "* [Beautiful Soup: Build a Web Scraper With Python](https://realpython.com/beautiful-soup-web-scraper-python/)\n",
    "* [API Integration in Python](https://realpython.com/api-integration-in-python/)\n",
    "* [Python & APIs: A Winning Combo for Reading Public Data](https://realpython.com/python-api/)\n",
    "\n",
    "Note: If you enjoyed what you learned in this sample from Python Basics: A Practical Introduction to Python 3, then be sure to check out the rest of the book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "601982ce-a7a9-40c2-8aa6-5f12ae8d529e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

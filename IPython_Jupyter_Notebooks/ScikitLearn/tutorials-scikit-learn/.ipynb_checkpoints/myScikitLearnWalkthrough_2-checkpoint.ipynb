{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I started this doc on 8/28/17.\n",
    "\n",
    "Here, I am trying out ideas available at https://elitedatascience.com/python-machine-learning-tutorial-scikit-learn which had the following title and date:\n",
    "# Python Machine Learning Tutorial, Scikit-Learn: Wine Snob Edition\n",
    "\n",
    "December 6, 2016\n",
    "\n",
    "Here, I will copy and paste almost everything and I may try out something more but related to what's been asked of the trainee.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this end-to-end Python machine learning tutorial, you’ll learn how to use Scikit-Learn to build and tune a supervised learning model!\n",
    "\n",
    "We’ll be training and tuning a random forest for wine quality (as judged by wine snobs experts) based on traits like acidity, residual sugar, and alcohol concentration.\n",
    "\n",
    "Before we start, we should state that this guide is meant for beginners who are interested in applied machine learning.\n",
    "\n",
    "Our goal is introduce you to one of the most flexible and useful libraries for machine learning in Python. We’ll skip the theory and math in this tutorial, but we’ll still recommend great resources for learning those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before we start...\n",
    "\n",
    "#### Recommended Prerequisites\n",
    "\n",
    "The recommended prerequisites for this guide are:\n",
    "\n",
    "* [At least up to Step 1: Sponge Mode in our Guide to Machine Learning](http://elitedatascience.com/learn-machine-learning)\n",
    "* [Python programming skills](http://elitedatascience.com/learn-python-for-data-science)\n",
    "\n",
    "To move quickly, we'll assume you have this background.\n",
    "\n",
    "#### Why Scikit-Learn for machine learning?\n",
    "\n",
    "Scikit-Learn, also known as sklearn, is Python's premier general-purpose machine learning library. While you'll find other packages that do better at certain tasks, Scikit-Learn's versatility makes it the best starting place for most ML problems.\n",
    "\n",
    "It's also a fantastic library for beginners because it offers a high-level interface for many tasks (e.g. preprocessing data, cross-validation, etc.). This allows you to better practice the entire machine learning workflow and understand the big picture.\n",
    "\n",
    "### WTF is machine learning?\n",
    "\n",
    "* [Ahem... maybe this is a better place to start instead.](https://elitedatascience.com/learn-machine-learning#what)\n",
    "\n",
    "#### What this guide is not:\n",
    "\n",
    "This is not a complete course on machine learning. Machine learning requires the practitioner to make dozens of decisions throughout the entire modeling process, and we won't cover all of those nuances.\n",
    "\n",
    "Instead, this is a tutorial that will take you from zero to your first Python machine learning model with as little headache as possible!\n",
    "\n",
    "If you're interested in mastering the theory behind machine learning, then we recommend our free guide:\n",
    "\n",
    "* [How to Learn Machine Learning, The Self-Starter Way](https://elitedatascience.com/learn-machine-learning)\n",
    "\n",
    "In addition, we also won't be covering exploratory data analysis in much detail, which is a vital part of real-world machine learning. We'll leave that for a separate guide.\n",
    "\n",
    "#### A quick tip before we begin:\n",
    "\n",
    "This tutorial is designed to be streamlined, and it won't cover any one topic in too much detail. It may be helpful to have the [Scikit-Learn documentation](http://scikit-learn.org/stable/user_guide.html) open beside you as a supplemental reference.\n",
    "\n",
    "## Python Machine Learning Tutorial Contents\n",
    "\n",
    "Here are the steps for building your first random forest model using Scikit-Learn:\n",
    "\n",
    "* Set up your environment.\n",
    "* Import libraries and modules.\n",
    "* Load red wine data.\n",
    "* Split data into training and test sets.\n",
    "* Declare data preprocessing steps.\n",
    "* Declare hyperparameters to tune.\n",
    "* Tune model using cross-validation pipeline.\n",
    "* Refit on the entire training set.\n",
    "* Evaluate model pipeline on test data.\n",
    "* Save model for further use.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set up your environment.\n",
    "\n",
    "First, grab a nice glass of wine.\n",
    "Drinking wine makes predicting wine easier (probably).\n",
    "\n",
    "Next, make sure the following are installed on your computer:\n",
    "\n",
    "* Python 2.7+ (Python 3 is fine too, but Python 2.7 is still more popular for data science overall)\n",
    "* NumPy\n",
    "* Pandas\n",
    "* Scikit-Learn (a.k.a. sklearn)\n",
    "\n",
    "We strongly recommend installing Python through [Anaconda](https://www.continuum.io/downloads) ([installation guide](https://docs.continuum.io/anaconda/install)). It comes with all of the above packages already installed.\n",
    "\n",
    "If you need to update any of the packages, it's as easy as typing  $ conda update <package> from your command line program (Terminal in Mac).\n",
    "\n",
    "You can confirm Scikit-Learn was installed properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-3-74a4542abf1f>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-3-74a4542abf1f>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python -c \"import sklearn; print sklearn.__version__\"\u001b[0m\n\u001b[0m                                                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python -c \"import sklearn; print sklearn.__version__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-fb7549e0ef3a>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-fb7549e0ef3a>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    python2 -c \"import sklearn; print sklearn.__version__\"\u001b[0m\n\u001b[0m                                                         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "python2 -c \"import sklearn; print sklearn.__version__\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KP: I was expecting a result as follows:\n",
    "```\n",
    "$ python -c \"import sklearn; print sklearn.__version__\"\n",
    "0.18.1\n",
    "```\n",
    "\n",
    "but as you see, it didn't work on the notebook cell. But, it did work in the terminal with python2 (which was installed with homebrew few weeks ago and 'python' seems to be the factory installation of python2.7.)\n",
    "\n",
    "Great, now let's start a new file and name it sklearn_ml_example.py.\n",
    "\n",
    "## Step 2: Import libraries and modules.\n",
    "\n",
    "To begin, let's import numpy, which provides support for more efficient numerical computation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import Pandas, a convenient library that supports dataframes . Pandas is technically optional because Scikit-Learn can handle numerical matrices directly, but it'll make our lives easier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to start importing functions for machine learning. The first one will be the train_test_split() function from the model_selection module. As its name implies, this module contains many utilities that will help us choose between models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import sampling helper\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll import the entire preprocessing module. This contains utilities for scaling, transforming, and wrangling data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import preprocessing modules\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import the families of models we'll need... wait, did you just say \"families?\"\n",
    "\n",
    "### What's the difference between model \"families\" and actual models?\n",
    "\n",
    "A \"family\" of models are broad types of models, such as random forests, SVM's, linear regression models, etc. Within each family of models, you'll get an actual model after you fit and tune its parameters to the data.\n",
    "\n",
    "* Tip: Don't worry too much about this for now... It will make more sense once we get to Step 7.\n",
    "\n",
    "We can import the random forest family like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import random forest model\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the scope of this tutorial, we'll only focus on training a random forest and tuning its parameters. We'll have another detailed tutorial for how to choose between model families.\n",
    "\n",
    "For now, let's move on to importing the tools to help us perform cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import cross-validation pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's import some metrics we can use to evaluate our model performance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, we'll import a way to persist our model for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import module for saving scikit-learn models\n",
    "from sklearn.externals import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Joblib is an alternative to Python's pickle package, and we'll use it because it's more efficient for storing large numpy arrays.\n",
    "\n",
    "**Phew!** That was a lot. Don't worry, we'll cover each function in detail once we get to it. Let's first take a quick sip of wine and toast to our progress... cheers!\n",
    "\n",
    "## Step 3: Load red wine data.\n",
    "\n",
    "Alright, now we're ready to load our data set. The Pandas library that we imported is loaded with a whole suite of helpful import/output tools.\n",
    "\n",
    "You can read data from CSV, Excel, SQL, SAS, and many other data formats. Here's a [list of all the Pandas IO tools](http://pandas.pydata.org/pandas-docs/stable/io.html).\n",
    "\n",
    "The convenient tool we'll use today is the read_csv() function. Using this function, we can load any CSV file, even from a remote URL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load wine data from remote URL\n",
    "\n",
    "dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "data = pd.read_csv(dataset_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fixed acidity;\"volatile acidity\";\"citric acid\";\"residual sugar\";\"chlorides\";\"free sulfur dioxide\";\"total sulfur dioxide\";\"density\";\"pH\";\"sulphates\";\"alcohol\";\"quality\"\n",
      "0   7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5                                                                                                                     \n",
      "1   7.8;0.88;0;2.6;0.098;25;67;0.9968;3.2;0.68;9.8;5                                                                                                                     \n",
      "2  7.8;0.76;0.04;2.3;0.092;15;54;0.997;3.26;0.65;...                                                                                                                     \n",
      "3  11.2;0.28;0.56;1.9;0.075;17;60;0.998;3.16;0.58...                                                                                                                     \n",
      "4   7.4;0.7;0;1.9;0.076;11;34;0.9978;3.51;0.56;9.4;5                                                                                                                     \n"
     ]
    }
   ],
   "source": [
    "# Output the first 5 rows of data\n",
    "\n",
    "print data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Crap... that looks really messy. Upon further inspection, it looks like the CSV file is actually using semicolons to separate the data. That's annoying, but easy to fix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   fixed acidity  volatile acidity  citric acid  residual sugar  chlorides  \\\n",
      "0            7.4              0.70         0.00             1.9      0.076   \n",
      "1            7.8              0.88         0.00             2.6      0.098   \n",
      "2            7.8              0.76         0.04             2.3      0.092   \n",
      "3           11.2              0.28         0.56             1.9      0.075   \n",
      "4            7.4              0.70         0.00             1.9      0.076   \n",
      "\n",
      "   free sulfur dioxide  total sulfur dioxide  density    pH  sulphates  \\\n",
      "0                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "1                 25.0                  67.0   0.9968  3.20       0.68   \n",
      "2                 15.0                  54.0   0.9970  3.26       0.65   \n",
      "3                 17.0                  60.0   0.9980  3.16       0.58   \n",
      "4                 11.0                  34.0   0.9978  3.51       0.56   \n",
      "\n",
      "   alcohol  quality  \n",
      "0      9.4        5  \n",
      "1      9.8        5  \n",
      "2      9.8        5  \n",
      "3      9.8        6  \n",
      "4      9.4        5  \n"
     ]
    }
   ],
   "source": [
    "# Read CSV with semicolon separator\n",
    "data = pd.read_csv(dataset_url, sep=';')\n",
    "print data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, that's much nicer. Now, let's take a look at the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1599, 12)\n"
     ]
    }
   ],
   "source": [
    "print data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 1,599 samples and 12 features, including our target feature. We can easily print some summary statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       fixed acidity  volatile acidity  citric acid  residual sugar  \\\n",
      "count    1599.000000       1599.000000  1599.000000     1599.000000   \n",
      "mean        8.319637          0.527821     0.270976        2.538806   \n",
      "std         1.741096          0.179060     0.194801        1.409928   \n",
      "min         4.600000          0.120000     0.000000        0.900000   \n",
      "25%         7.100000          0.390000     0.090000        1.900000   \n",
      "50%         7.900000          0.520000     0.260000        2.200000   \n",
      "75%         9.200000          0.640000     0.420000        2.600000   \n",
      "max        15.900000          1.580000     1.000000       15.500000   \n",
      "\n",
      "         chlorides  free sulfur dioxide  total sulfur dioxide      density  \\\n",
      "count  1599.000000          1599.000000           1599.000000  1599.000000   \n",
      "mean      0.087467            15.874922             46.467792     0.996747   \n",
      "std       0.047065            10.460157             32.895324     0.001887   \n",
      "min       0.012000             1.000000              6.000000     0.990070   \n",
      "25%       0.070000             7.000000             22.000000     0.995600   \n",
      "50%       0.079000            14.000000             38.000000     0.996750   \n",
      "75%       0.090000            21.000000             62.000000     0.997835   \n",
      "max       0.611000            72.000000            289.000000     1.003690   \n",
      "\n",
      "                pH    sulphates      alcohol      quality  \n",
      "count  1599.000000  1599.000000  1599.000000  1599.000000  \n",
      "mean      3.311113     0.658149    10.422983     5.636023  \n",
      "std       0.154386     0.169507     1.065668     0.807569  \n",
      "min       2.740000     0.330000     8.400000     3.000000  \n",
      "25%       3.210000     0.550000     9.500000     5.000000  \n",
      "50%       3.310000     0.620000    10.200000     6.000000  \n",
      "75%       3.400000     0.730000    11.100000     6.000000  \n",
      "max       4.010000     2.000000    14.900000     8.000000  \n"
     ]
    }
   ],
   "source": [
    "# Summary statistics\n",
    "\n",
    "print data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the list of all the features:\n",
    "\n",
    "* **quality** (target)\n",
    "* fixed acidity\n",
    "* volatile acidity\n",
    "* citric acid\n",
    "* residual sugar\n",
    "* chlorides\n",
    "* free sulfur dioxide\n",
    "* total sulfur dioxide\n",
    "* density\n",
    "* pH\n",
    "* sulphates\n",
    "* alcohol\n",
    "\n",
    "All of the features are numeric, which is convenient. However, they have some very different scales, so let's make a mental note to **standardize** the data later.\n",
    "\n",
    "As a reminder, for this tutorial, we're cutting out a lot of exploratory data analysis we'd typically recommend.\n",
    "\n",
    "For now, let's move on to splitting the data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Split data into training and test sets.\n",
    "\n",
    "Splitting the data into training and test sets **at the beginning of your modeling workflow** is crucial for getting a realistic estimate of your model's performance.\n",
    "\n",
    "First, let's separate our target (y) features from our input (X) features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Separate target from training features\n",
    "y = data.quality\n",
    "X = data.drop('quality', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This allows us to take advantage of Scikit-Learn's useful **train_test_split** function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we'll set aside 20% of the data as a test set for evaluating our model. We also set **an arbitrary \"random state\" (a.k.a. seed)** so that we can reproduce our results.\n",
    "\n",
    "Finally, it's good practice to **stratify your sample** by the target variable. This will ensure your training set looks similar to your test set, making your evaluation metrics more reliable.\n",
    "\n",
    "## Step 5: Declare data preprocessing steps.\n",
    "\n",
    "Remember, in Step 3, we made the mental note to standardize our features because they were on different scales.\n",
    "\n",
    "**WTF is standardization?**\n",
    "\n",
    "Standardization is the process of subtracting the means from each feature and then dividing by the feature standard deviations.\n",
    "\n",
    "Standardization is a common requirement for machine learning tasks. Many algorithms assume that all features are centered around zero and have approximately the same variance.\n",
    "\n",
    "**First, here's some code that we won't use...**\n",
    "\n",
    "Scikit-Learn makes data preprocessing a breeze. For example, it's pretty easy to simply scale a dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.51358886  2.19680282 -0.164433   ...,  1.08415147 -0.69866131\n",
      "  -0.58608178]\n",
      " [-1.73698885 -0.31792985 -0.82867679 ...,  1.46964764  1.2491516\n",
      "   2.97009781]\n",
      " [-0.35201795  0.46443143 -0.47100705 ..., -0.13658641 -0.35492962\n",
      "  -0.20843439]\n",
      " ..., \n",
      " [-0.98679628  1.10708533 -0.93086814 ...,  0.24890976 -0.98510439\n",
      "   0.35803669]\n",
      " [-0.69826067  0.46443143 -1.28853787 ...,  1.08415147 -0.35492962\n",
      "  -0.68049363]\n",
      " [ 3.1104093  -0.62528606  2.08377675 ..., -1.61432173  0.79084268\n",
      "  -0.39725809]]\n"
     ]
    }
   ],
   "source": [
    "# Lazy way of scaling data\n",
    "X_train_scaled = preprocessing.scale(X_train)\n",
    "print X_train_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can confirm that the scaled dataset is indeed centered at zero, with unit variance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.16664562e-16  -3.05550043e-17  -8.47206937e-17  -2.22218213e-17\n",
      "   2.22218213e-17  -6.38877362e-17  -4.16659149e-18  -2.54439854e-15\n",
      "  -8.70817622e-16  -4.08325966e-16  -1.17220107e-15]\n"
     ]
    }
   ],
   "source": [
    "print X_train_scaled.mean(axis=0)\n",
    "# should return the following\n",
    "# [ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print X_train_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, but why did we say that we won't use this code?\n",
    "\n",
    "The reason is that we won't be able to perform the exact same transformation on the test set.\n",
    "\n",
    "Sure, we can still scale the test set separately, but we won't be using the same means and standard deviations as we used to transform the training set.\n",
    "\n",
    "In other words, that means it wouldn't be a fair representation of how the model pipeline, include the preprocessing steps, would perform on brand new data.\n",
    "\n",
    "**Now, here's the preprocessing code we will use...**\n",
    "\n",
    "So instead of directly invoking the scale function, we'll be using a feature in Scikit-Learn called the **Transformer API**. The Transformer API allows you to \"fit\" a preprocessing step using the training data the same way you'd fit a model...\n",
    "\n",
    "...and then use the same transformation on future data sets!\n",
    "\n",
    "Here's what that process looks like:\n",
    "\n",
    "1. Fit the transformer on the training set (saving the means and standard deviations)\n",
    "2. Apply the transformer to the training set (scaling the training data)\n",
    "3. Apply the transformer to the test set (using the same means and standard deviations)\n",
    "\n",
    "This makes your final estimate of model performance more realistic, and it allows to insert your preprocessing steps into a **cross-validation pipeline** (more on this in Step 7).\n",
    "\n",
    "Here's how you do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Fitting the Transformer API\n",
    "scaler = preprocessing.StandardScaler().fit(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the **scaler** object has the saved means and standard deviations for each feature in the training set.\n",
    "\n",
    "Let's confirm that worked:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.16664562e-16  -3.05550043e-17  -8.47206937e-17  -2.22218213e-17\n",
      "   2.22218213e-17  -6.38877362e-17  -4.16659149e-18  -2.54439854e-15\n",
      "  -8.70817622e-16  -4.08325966e-16  -1.17220107e-15]\n"
     ]
    }
   ],
   "source": [
    "# Applying transformer to training data\n",
    "\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    " \n",
    "print X_train_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print X_train_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we're taking the scaler object and using it to **transform** the training set. Later, we can transform the test set using the exact same means and standard deviations used to transform the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.02776704  0.02592492 -0.03078587 -0.03137977 -0.00471876 -0.04413827\n",
      " -0.02414174 -0.00293273 -0.00467444 -0.10894663  0.01043391]\n"
     ]
    }
   ],
   "source": [
    "# Applying transformer to test data\n",
    "\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    " \n",
    "print X_test_scaled.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.02160495  1.00135689  0.97456598  0.91099054  0.86716698  0.94193125\n",
      "  1.03673213  1.03145119  0.95734849  0.83829505  1.0286218 ]\n"
     ]
    }
   ],
   "source": [
    "print X_test_scaled.std(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how the scaled features in the test set are not perfectly centered at zero with unit variance! This is exactly what we'd expect, as we're transforming the test set using the means from the training set, not from the test set itself.\n",
    "\n",
    "In practice, when we set up the cross-validation pipeline, we won't even need to manually fit the Transformer API. Instead, we'll simply declare the class object, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Pipeline with preprocessing and model\n",
    "pipeline = make_pipeline(preprocessing.StandardScaler(), \n",
    "                         RandomForestRegressor(n_estimators=100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is exactly what it looks like: a **modeling pipeline** that first transforms the data using StandardScaler() and then fits a model using a random forest regressor.\n",
    "\n",
    "## Step 6: Declare hyperparameters to tune.\n",
    "\n",
    "Now it's time to consider the hyperparameters that we'll want to tune for our model.\n",
    "\n",
    "#### WTF are hyperparameters?\n",
    "\n",
    "There are two types of parameters we need to worry about: model parameters and hyperparameters. Models parameters can be learned directly from the data (i.e. regression coefficients), while hyperparameters cannot.\n",
    "\n",
    "Hyperparameters express \"higher-level\" structural information about the model, and they are typically set before training the model.\n",
    "\n",
    "#### Example: random forest hyperparameters.\n",
    "\n",
    "As an example, let's take our random forest for regression:\n",
    "\n",
    "Within each decision tree, the computer can empirically decide where to create branches based on either mean-squared-error (MSE) or mean-absolute-error (MAE). Therefore, the actual branch locations are **model parameters**.\n",
    "\n",
    "However, the algorithm does not know which of the two criteria, MSE or MAE, that it should use. The algorithm also cannot decide how many trees to include in the forest. These are examples of **hyperparameters** that the user must set.\n",
    "\n",
    "We can list the tunable hyperparameters like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomforestregressor__random_state': None, 'randomforestregressor__min_weight_fraction_leaf': 0.0, 'standardscaler__with_mean': True, 'randomforestregressor__n_estimators': 100, 'randomforestregressor__min_samples_leaf': 1, 'standardscaler__copy': True, 'randomforestregressor__warm_start': False, 'randomforestregressor__criterion': 'mse', 'randomforestregressor__n_jobs': 1, 'randomforestregressor__max_leaf_nodes': None, 'randomforestregressor__oob_score': False, 'randomforestregressor__verbose': 0, 'randomforestregressor__min_impurity_split': 1e-07, 'steps': [('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False))], 'randomforestregressor__min_samples_split': 2, 'randomforestregressor': RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
      "           max_features='auto', max_leaf_nodes=None,\n",
      "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
      "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
      "           n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
      "           verbose=0, warm_start=False), 'randomforestregressor__max_depth': None, 'standardscaler__with_std': True, 'randomforestregressor__max_features': 'auto', 'standardscaler': StandardScaler(copy=True, with_mean=True, with_std=True), 'randomforestregressor__bootstrap': True}\n"
     ]
    }
   ],
   "source": [
    "# List tunable hyperparameters\n",
    "\n",
    "print pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also find a list of all the parameters on the [RandomForestRegressor documentation page](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html). Just note that when it's tuned through a pipeline, you'll need to prepend  randomforestregressor__ before the parameter name, like in the code above.\n",
    "\n",
    "Now, let's declare the hyperparameters we want to tune through cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Declare hyperparameters to tune\n",
    "\n",
    "hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                  'randomforestregressor__max_depth': [None, 5, 3, 1]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the format should be a Python dictionary (data structure for key-value pairs) where keys are the hyperparameter names and values are lists of settings to try. The options for parameter values can be found on the documentation page.\n",
    "\n",
    "## Step 7: Tune model using a cross-validation pipeline.\n",
    "\n",
    "Now we're almost ready to dive into fitting our models. But first, we need to spend some time talking about cross-validation.\n",
    "\n",
    "This is one of the most important skills in all of machine learning because it helps you maximize model performance while reducing the chance of overfitting.\n",
    "\n",
    "#### WTF is cross-validation (CV)?\n",
    "\n",
    "Cross-validation is a process for reliably estimating the performance of a method for building a model by training and evaluating your model multiple times using the same method.\n",
    "\n",
    "Practically, that \"method\" is simply a set of hyperparameters in this context.\n",
    "\n",
    "These are the steps for CV:\n",
    "\n",
    "* Split your data into k equal parts, or \"folds\" (typically k=10).\n",
    "* Train your model on k-1 folds (e.g. the first 9 folds).\n",
    "* Evaluate it on the remaining \"hold-out\" fold (e.g. the 10th fold).\n",
    "* Perform steps (2) and (3) k times, each time holding out a different fold.\n",
    "* Aggregate the performance across all k folds. This is your performance metric.\n",
    "\n",
    "![K-Fold Cross-Validation](K-fold_cross_validation_EN.jpg)\n",
    "K-Fold Cross-Validation diagram, courtesy of Wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Why is cross-validation important in machine learning?\n",
    "\n",
    "Let's say you want to train a random forest regressor. One of the hyperparameters you must tune is the maximum depth allowed for each decision tree in your forest.\n",
    "\n",
    "How can you decide?\n",
    "\n",
    "That's where cross-validation comes in. Using only your training set, you can use CV to evaluate different hyperparameters and estimate their effectiveness.\n",
    "\n",
    "This allows you to keep your test set \"untainted\" and save it for a true hold-out evaluation when you're finally ready to select a model.\n",
    "\n",
    "For example, you can use CV to tune a random forest model, a linear regression model, and a k-nearest neighbors model, using only the training set. Then, you still have the untainted test set to make your final selection between the model families!\n",
    "\n",
    "#### So WTF is a cross-validation \"pipeline?\"\n",
    "\n",
    "The best practice when performing CV is to include your data preprocessing steps inside the cross-validation loop. This prevents accidentally tainting your training folds with influential data from your test fold.\n",
    "\n",
    "Here's how the CV pipeline looks after including preprocessing steps:\n",
    "\n",
    "* Split your data into k equal parts, or \"folds\" (typically k=10).\n",
    "* **Preprocess k-1 training folds.**\n",
    "* Train your model on the same k-1 folds.\n",
    "* **Preprocess the hold-out fold using the same transformations from step (2).**\n",
    "* Evaluate your model on the same hold-out fold.\n",
    "* Perform steps **(2) - (5)** k times, each time holding out a different fold.\n",
    "* Aggregate the performance across all k folds. This is your performance metric.\n",
    "\n",
    "Fortunately, Scikit-Learn makes it stupidly simple to set this up:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=10, error_score='raise',\n",
       "       estimator=Pipeline(steps=[('standardscaler', StandardScaler(copy=True, with_mean=True, with_std=True)), ('randomforestregressor', RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_split=1e-07, min_samples_leaf=1,\n",
       "           min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
       "           n_estimators=100, n_jobs=1, oob_score=False, random_state=None,\n",
       "           verbose=0, warm_start=False))]),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'randomforestregressor__max_depth': [None, 5, 3, 1], 'randomforestregressor__max_features': ['auto', 'sqrt', 'log2']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score=True,\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sklearn cross-validation with pipeline\n",
    "\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    " \n",
    "# Fit and tune model\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'randomforestregressor__max_depth': None, 'randomforestregressor__max_features': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "print clf.best_params_\n",
    "# {'randomforestregressor__max_depth': None, 'randomforestregressor__max_features': 'auto'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, it looks like the default parameters win out for this data set.\n",
    "\n",
    "* **Tip:** It turns out that in practice, random forests don't actually require a lot of tuning. They tend to work pretty well out-of-the-box with a reasonable number of trees. Even so, these same steps can be used when building any type of supervised learning model.\n",
    "\n",
    "## Step 8: Refit on the entire training set.\n",
    "\n",
    "After you've tuned your hyperparameters appropriately using cross-validation, you can generally get a small performance improvement by refitting the model on the entire training set.\n",
    "\n",
    "Conveniently, GridSearchCV from sklearn will automatically refit the model with the best set of hyperparameters using the entire training set.\n",
    "\n",
    "This functionality is ON by default, but you can confirm it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Confirm model will be retrained\n",
    "print clf.refit\n",
    "# True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you can simply use the  clf object as your model when applying it to other sets of data. That's what we'll be doing in the next step.\n",
    "\n",
    "## Step 9: Evaluate model pipeline on test data.\n",
    "\n",
    "Alright, we're in the home stretch!\n",
    "\n",
    "This step is really straightforward once you understand that the  clf object you used to tune the hyperparameters can also be used directly like a model object.\n",
    "\n",
    "Here's how to predict a new set of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Predict a new set of data\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the metrics we imported earlier to evaluate our model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.464310914704\n",
      "0.3456659375\n"
     ]
    }
   ],
   "source": [
    "print r2_score(y_test, y_pred)\n",
    "# 0.45044082571584243\n",
    " \n",
    "print mean_squared_error(y_test, y_pred)\n",
    "# 0.35461593750000003"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, so now the question is... **is this performance good enough?**\n",
    "\n",
    "Well, the rule of thumb is that your very first model probably won't be the best possible model. However, we recommend a combination of three strategies to decide if you're satisfied with your model performance.\n",
    "\n",
    "* Start with the goal of the model. If the model is tied to a business problem, have you successfully solved the problem?\n",
    "* Look in academic literature to get a sense of the current performance benchmarks for specific types of data.\n",
    "* Try to find low-hanging fruit in terms of ways to improve your model.\n",
    "\n",
    "There are various ways to improve a model. We'll have more guides that go into detail about how to improve model performance, but here are a few quick things to try:\n",
    "\n",
    "* Try other regression model families (e.g. regularized regression, boosted trees, etc.).\n",
    "* Collect more data if it's cheap to do so.\n",
    "* Engineer smarter features after spending more time on exploratory analysis.\n",
    "* Speak to a domain expert to get more context (...this is a good excuse to go wine tasting!).\n",
    "\n",
    "As a final note, when you try other families of models, we recommend using the same training and test set as you used to fit the random forest model. That's the best way to get a true apples-to-apples comparison between your models.\n",
    "\n",
    "## Step 10: Save model for future use.\n",
    "\n",
    "Great job completing this tutorial!\n",
    "\n",
    "You've done the hard part, and deserve another glass of wine. Maybe this time you can use your shiny new predictive model to select the bottle.\n",
    "\n",
    "But before you go, let's save your hard work so you can use the model in the future. It's really easy to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['rf_regressor.pkl']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save model to a .pkl file\n",
    "joblib.dump(clf, 'rf_regressor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it. When you want to load the model again, simply use this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 6.67,  5.87,  4.95,  5.52,  6.41,  5.63,  4.77,  4.68,  5.  ,\n",
       "        6.07,  5.26,  5.76,  5.88,  5.05,  5.73,  5.68,  6.61,  5.74,\n",
       "        5.75,  6.98,  5.59,  5.73,  4.99,  5.92,  5.86,  5.06,  5.62,\n",
       "        5.17,  5.98,  5.9 ,  5.93,  6.56,  5.97,  5.  ,  4.87,  6.01,\n",
       "        5.  ,  5.75,  5.01,  5.86,  4.85,  6.01,  6.88,  5.1 ,  6.13,\n",
       "        5.42,  5.59,  5.43,  5.09,  6.56,  5.71,  5.32,  5.96,  5.22,\n",
       "        5.67,  5.85,  5.24,  5.4 ,  4.94,  5.37,  5.29,  5.11,  5.11,\n",
       "        5.84,  5.98,  5.19,  6.35,  5.01,  5.12,  6.61,  5.73,  5.39,\n",
       "        5.05,  5.01,  5.27,  5.99,  5.28,  5.09,  5.22,  5.15,  6.6 ,\n",
       "        5.55,  6.34,  6.65,  5.14,  5.73,  6.56,  6.12,  5.55,  5.81,\n",
       "        5.86,  5.27,  6.51,  5.64,  5.68,  5.73,  6.67,  6.71,  5.43,\n",
       "        6.91,  5.09,  5.42,  5.14,  6.66,  5.05,  4.72,  5.71,  4.98,\n",
       "        5.74,  5.98,  5.69,  5.57,  6.09,  5.55,  5.05,  5.19,  5.94,\n",
       "        5.  ,  4.99,  6.03,  5.93,  5.05,  5.79,  6.01,  5.15,  5.31,\n",
       "        5.34,  5.85,  5.4 ,  5.52,  5.81,  6.46,  5.09,  5.27,  5.05,\n",
       "        6.55,  5.03,  5.17,  6.71,  5.45,  5.25,  5.08,  5.84,  6.05,\n",
       "        5.31,  5.49,  5.06,  6.56,  5.67,  5.  ,  5.55,  5.17,  4.76,\n",
       "        4.98,  5.26,  5.99,  5.3 ,  5.73,  5.75,  5.18,  5.48,  5.22,\n",
       "        5.27,  5.9 ,  5.  ,  5.91,  5.17,  5.32,  5.48,  5.25,  5.66,\n",
       "        5.02,  5.59,  5.09,  5.71,  5.51,  5.09,  5.48,  5.58,  5.05,\n",
       "        6.13,  5.63,  5.04,  4.91,  5.21,  6.11,  5.15,  5.58,  5.18,\n",
       "        4.82,  5.45,  6.64,  5.86,  5.94,  5.39,  5.24,  5.45,  5.03,\n",
       "        6.13,  4.68,  6.28,  5.08,  5.22,  5.23,  6.93,  6.02,  5.27,\n",
       "        5.2 ,  5.45,  5.98,  5.95,  5.98,  6.02,  6.45,  5.79,  5.96,\n",
       "        5.19,  5.24,  5.69,  5.19,  5.19,  5.85,  6.11,  5.59,  5.98,\n",
       "        5.8 ,  5.67,  6.18,  5.38,  5.64,  5.46,  5.46,  6.2 ,  5.89,\n",
       "        4.85,  4.12,  6.8 ,  6.65,  6.31,  5.27,  5.39,  5.47,  5.32,\n",
       "        6.27,  5.83,  5.15,  5.11,  5.51,  5.29,  6.61,  5.23,  5.03,\n",
       "        5.21,  5.08,  5.93,  6.54,  5.75,  5.32,  5.44,  6.39,  5.5 ,\n",
       "        5.99,  5.25,  5.05,  5.77,  5.98,  5.74,  5.68,  5.32,  5.07,\n",
       "        5.8 ,  5.39,  6.64,  6.13,  5.62,  4.95,  6.01,  6.73,  5.95,\n",
       "        5.37,  5.7 ,  5.37,  5.38,  6.2 ,  6.92,  5.28,  6.84,  6.07,\n",
       "        5.33,  5.3 ,  5.94,  5.09,  5.18,  6.23,  5.79,  5.92,  6.17,\n",
       "        5.87,  5.49,  5.63,  5.54,  6.24,  5.51,  6.94,  6.97,  6.  ,\n",
       "        6.08,  5.04,  5.33,  6.01,  5.33,  5.34,  5.85,  6.63,  6.81,\n",
       "        5.26,  5.46,  5.63,  5.95,  5.44])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load model from .pkl file\n",
    "clf2 = joblib.load('rf_regressor.pkl')\n",
    " \n",
    "# Predict data set using loaded model\n",
    "clf2.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations, you've reached the end of this tutorial!\n",
    "\n",
    "We've just completed a whirlwind tour of Scikit-Learn's core functionality, but we've only really scratched the surface. Hopefully you've gained some guideposts to further explore all that sklearn has to offer.\n",
    "\n",
    "For continued learning, we recommend studying other [examples in sklearn](http://scikit-learn.org/stable/tutorial/index.html).\n",
    "\n",
    "## The complete code, from start to finish.\n",
    "\n",
    "Here's all the code in one place, in a single script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 2. Import libraries and modules\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    " \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.externals import joblib \n",
    " \n",
    "# 3. Load red wine data.\n",
    "dataset_url = 'http://mlr.cs.umass.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv'\n",
    "data = pd.read_csv(dataset_url, sep=';')\n",
    " \n",
    "# 4. Split data into training and test sets\n",
    "y = data.quality\n",
    "X = data.drop('quality', axis=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=123, \n",
    "                                                    stratify=y)\n",
    " \n",
    "# 5. Declare data preprocessing steps\n",
    "pipeline = make_pipeline(preprocessing.StandardScaler(), \n",
    "                         RandomForestRegressor(n_estimators=100))\n",
    " \n",
    "# 6. Declare hyperparameters to tune\n",
    "hyperparameters = { 'randomforestregressor__max_features' : ['auto', 'sqrt', 'log2'],\n",
    "                  'randomforestregressor__max_depth': [None, 5, 3, 1]}\n",
    " \n",
    "# 7. Tune model using cross-validation pipeline\n",
    "clf = GridSearchCV(pipeline, hyperparameters, cv=10)\n",
    " \n",
    "clf.fit(X_train, y_train)\n",
    " \n",
    "# 8. Refit on the entire training set\n",
    "# No additional code needed if clf.refit == True (default is True)\n",
    " \n",
    "# 9. Evaluate model pipeline on test data\n",
    "pred = clf.predict(X_test)\n",
    "print r2_score(y_test, pred)\n",
    "print mean_squared_error(y_test, pred)\n",
    " \n",
    "# 10. Save model for future use\n",
    "joblib.dump(clf, 'rf_regressor.pkl')\n",
    "# To load: clf2 = joblib.load('rf_regressor.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enjoyed the tutorial?\n",
    "\n",
    "If you enjoy this type of hands-on learning, then you'll love our new Machine Learning Masterclass. It's sorta like this, except more epic, more comprehensive, more detailed, more interactive, and packed with much more information.\n",
    "\n",
    "In fact, it's a complete course on applied machine learning that's 100% project-centric, with strong emphasis on getting real-world results. And yes, you'll walk away with projects that look very impressive in your portfolio. [Click here to learn more](https://elitedatascience.com/machine-learning-masterclass)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "illy\n",
    "May 24, 2017\n",
    "\n",
    "Thanks. Finally a simple end-to-end random forest regressor for the beginner with good explanations of cross validation, hyperparameter and test/train parameters and the need for them.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
